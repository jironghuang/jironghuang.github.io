<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Jirong&#39;s sandbox</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 24 Aug 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/r/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Market Neutral Strategy - SnP500 to Berkshire Hathaway Ratio</title>
      <link>/post/market_neutral/</link>
      <pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/market_neutral/</guid>
      <description>


&lt;div id=&#34;market-neutral-strategy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Market neutral strategy&lt;/h2&gt;
&lt;p&gt;As the negative news pile up (trade wars, slump in economy growths, etc), I sought for market neutral stategies that could perform well in any market environment.&lt;/p&gt;
&lt;p&gt;An idea that struck me recently is to exploit the pair between Berkshire and SnP 500.&lt;/p&gt;
&lt;p&gt;The SnP500/ Berkshire ratio has been falling over the years - insinuating that Berkshire has lost its magic of outperforming the index over a couple of decades.&lt;/p&gt;
&lt;p&gt;That being said, it’s still widely regarded as a safe haven in times of trouble.&lt;/p&gt;
&lt;div id=&#34;strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strategy&lt;/h3&gt;
&lt;p&gt;As the market is facing headwind and I sought for a market neutral strategy.&lt;/p&gt;
&lt;p&gt;Here’s what I did,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I derive the SnP500/ Berkshire ratio over last 5 years&lt;/li&gt;
&lt;li&gt;And I fix a Bollinger Band around the ratio with n = 200 days as the moving average parameter - with 2 SD as the lower (lb) and upper bound (ub) line. Bollinger band ratio accounts for the downward trend in the ratio over time.&lt;/li&gt;
&lt;li&gt;If the ratio is below the lb, I will long SnP500 and short Berkshire Hathaway. And when it mean reverts and touches the middle moving average, I will exit the positions&lt;/li&gt;
&lt;li&gt;Conversely if the ratio is above the ub, I will short SnP500 and long Berkshire Hathaway. Similarly, I will exit the position when it touches the moving average&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;So how did the strategy fare? Fairly impressive I must say.&lt;/p&gt;
&lt;p&gt;Sharpe ratio is around 0.8. Annualized return is around 5.7% with the positions only in the market 30% of the time!&lt;/p&gt;
&lt;p&gt;That being said, I’m cherry picking here because the performance before this period is sub-par; probably because of a change in market regime. You may execute my code to stress-test this simple strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disclosure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Disclosure&lt;/h3&gt;
&lt;p&gt;I may execute this strategy in the near term.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running packages&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;##                  zoo                tidyr                 plyr 
##                 TRUE                 TRUE                 TRUE 
##                dplyr               gtools         googlesheets 
##                 TRUE                 TRUE                 TRUE 
##             quantmod                 urca PerformanceAnalytics 
##                 TRUE                 TRUE                 TRUE 
##             parallel                  TTR 
##                 TRUE                 TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Function using bollinger band
lf_bollinger_pair_trading = function(stock1, stock2, start_date, end_date, prop_res, bband_days){
  
#Start of function
data1 = df_crawl_time_series(stock1, start_date, end_date)
data1 = subset(data1, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Adj.Close&amp;quot;))
names(data1) = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;)
data1$Date = as.Date(data1$Date)

data2 = df_crawl_time_series(stock2, start_date, end_date)
data2 = subset(data2, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Adj.Close&amp;quot;))
names(data2) = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;)
data2$Date = as.Date(data2$Date)

#Training and testing index
data1 = xts(data1[, -1], order.by = data1[, 1])
data2 = xts(data2[, -1], order.by = data2[, 1])

data = merge(data1, data2)
data = as.data.frame(data)
data = subset(data, !is.na(data$Close) &amp;amp; !is.na(data$Close.1))

data$ratio = data$Close/ data$Close.1

# plot(data$ratio)
bb_ratio = data.frame(BBands( data$ratio, n = bband_days))
data = cbind(data, bb_ratio)
data_sub = tail(data, round(nrow(data) * prop_res, 0))

plot(data_sub$ratio)
lines(data_sub$mavg, col = &amp;quot;red&amp;quot;)
lines(data_sub$up, col = &amp;quot;blue&amp;quot;)
lines(data_sub$dn, col = &amp;quot;green&amp;quot;)

#If lower than 
data_sub$longs &amp;lt;- data_sub$ratio &amp;lt;= data_sub$dn # buy spread when its value drops below 2 standard deviations.
data_sub$shorts &amp;lt;- data_sub$ratio &amp;gt;= data_sub$up # short spread when its value rises above 2 standard deviations.

#  exit any spread position when its value is at moving average
data_sub$longExits   &amp;lt;- data_sub$ratio &amp;gt;= data_sub$mavg
data_sub$shortExits &amp;lt;- data_sub$ratio &amp;lt;= data_sub$mavg


# #  define indices for training and test sets
# trainset &amp;lt;- 1:as.integer(nrow(data) * prop_train)
# testset &amp;lt;- (length(trainset)+1):nrow(data)

#Signal
data_sub$posL1 = NA
data_sub$posL2 = NA
data_sub$posS1 = NA
data_sub$posS2 = NA

# initialize to 0
data_sub$posL1[1] &amp;lt;- 0; data_sub$posL2[1] &amp;lt;- 0
data_sub$posS1[1] &amp;lt;- 0; data_sub$posS2[1] &amp;lt;- 0

data_sub$posL1[data_sub$longs] &amp;lt;- 1
data_sub$posL2[data_sub$longs] &amp;lt;- -1

data_sub$posS1[data_sub$shorts] &amp;lt;- -1
data_sub$posS2[data_sub$shorts] &amp;lt;- 1

data_sub$posL1[data_sub$longExits] &amp;lt;- 0
data_sub$posL2[data_sub$longExits] &amp;lt;- 0
data_sub$posS1[data_sub$shortExits] &amp;lt;- 0
data_sub$posS2[data_sub$shortExits] &amp;lt;- 0

#positions
data_sub$posL1 &amp;lt;- zoo::na.locf(data_sub$posL1); data_sub$posL2 &amp;lt;- zoo::na.locf(data_sub$posL2)
data_sub$posS1 &amp;lt;- zoo::na.locf(data_sub$posS1); data_sub$posS2 &amp;lt;- zoo::na.locf(data_sub$posS2)
data_sub$position1 &amp;lt;- data_sub$posL1 + data_sub$posS1
data_sub$position2 &amp;lt;- data_sub$posL2 + data_sub$posS2

#Returns
data_sub$dailyret1 &amp;lt;- ROC(data_sub$Close) #  last row is [385,] -0.0122636689 -0.0140365802
data_sub$dailyret2 &amp;lt;- ROC(data_sub$Close.1) #  last row is [385,] -0.0122636689 -0.0140365802

#Backshifting here. But signal is for following day returns!. So can still use latest Z-score
data_sub$date = as.Date(row.names(data_sub))
data_sub = xts(data_sub[,-which(names(data_sub) == &amp;quot;date&amp;quot;)], order.by = data_sub[, which(names(data_sub) == &amp;quot;date&amp;quot;)])

#Doesn&amp;#39;t account for number of shares!!!!!
data_sub$pnl = lag(data_sub$position1, 1) * data_sub$dailyret1  + lag(data_sub$position2, 1) * data_sub$dailyret2

#Performance analytics
tryCatch({
  # charts_perf = charts.PerformanceSummary(data_sub$pnl)
  charts.PerformanceSummary(data_sub$pnl)
}, error = function(e){})

dd = table.Drawdowns(data_sub$pnl)
ds_risk = table.DownsideRisk(data_sub$pnl)
ret = table.AnnualizedReturns(data_sub$pnl)


df_ret = list(data_sub = data_sub,
              dd = dd,
              ds_risk = ds_risk,
              ret = ret
              )

return(df_ret)

}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Parameters to be includded in function
stock1 = &amp;quot;^GSPC&amp;quot;
stock2 = &amp;quot;BRK-B&amp;quot;

start_date = &amp;quot;2000-07-01&amp;quot;
end_date = &amp;quot;2019-12-30&amp;quot;

prop_res = 0.25     #Proportion of results to show
bband_days = 200   #Bollinger band of ratio

#Storing result to function
res = lf_bollinger_pair_trading(stock1, stock2, start_date, end_date, prop_res, bband_days)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;getSymbols&amp;#39; currently uses auto.assign=TRUE by default, but will
## use auto.assign=FALSE in 0.5-0. You will still be able to use
## &amp;#39;loadSymbols&amp;#39; to automatically load data. getOption(&amp;quot;getSymbols.env&amp;quot;)
## and getOption(&amp;quot;getSymbols.auto.assign&amp;quot;) will still be checked for
## alternate defaults.
## 
## This message is shown once per session and may be disabled by setting 
## options(&amp;quot;getSymbols.warning4.0&amp;quot;=FALSE). See ?getSymbols for details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## WARNING: There have been significant changes to Yahoo Finance data.
## Please see the Warning section of &amp;#39;?getSymbols.yahoo&amp;#39; for details.
## 
## This message is shown once per session and may be disabled by setting
## options(&amp;quot;getSymbols.yahoo.warning&amp;quot;=FALSE).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/market_neutral_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/market_neutral_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;displaying-of-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Displaying of results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Drawdown period&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$dd&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         From     Trough         To   Depth Length To Trough Recovery
## 1 2015-08-10 2015-12-10 2016-05-31 -0.0724    204        87      117
## 2 2018-12-14 2018-12-31 2019-01-08 -0.0627     16        11        5
## 3 2017-01-24 2017-03-09 2017-04-13 -0.0551     57        32       25
## 4 2018-05-31 2018-07-17 2018-08-06 -0.0494     47        33       14
## 5 2018-02-26 2018-03-09 2018-04-17 -0.0349     36        10       26&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Downside risk&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$ds_risk&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   pnl
## Semi Deviation                 0.0031
## Gain Deviation                 0.0049
## Loss Deviation                 0.0041
## Downside Deviation (MAR=210%)  0.0092
## Downside Deviation (Rf=0%)     0.0030
## Downside Deviation (0%)        0.0030
## Maximum Drawdown               0.0724
## Historical VaR (95%)          -0.0071
## Historical ES (95%)           -0.0106
## Modified VaR (95%)            -0.0043
## Modified ES (95%)             -0.0043&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Returns&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$ret&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              pnl
## Annualized Return         0.0574
## Annualized Std Dev        0.0736
## Annualized Sharpe (Rf=0%) 0.7799&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Convert NAs to Obscure Number in Data Frame to Aid in Recoding/ Feature Engineering</title>
      <link>/post/convert_na_num/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/convert_na_num/</guid>
      <description>


&lt;div id=&#34;converting-nas-to-obscure-numbers-to-prevent-the-data-from-messing-up-the-recoding.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Converting NAs to obscure numbers to prevent the data from messing up the recoding.&lt;/h2&gt;
&lt;p&gt;1 issue that I encounter while I data-munge is that NAs in data seem to mess up my recoding. Here’s a neat swiss army knife utility function I developed recently.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(dplyr))

# Converting NA to obscure number to prevent awkward recoding situations that require &amp;amp; !is.na(&amp;lt;variable&amp;gt;)
# Doesn&amp;#39;t work for factors
#&amp;#39; @title Convert NA to obscure number
#&amp;#39; @param dp_dataframe Dataframe in consideration
#&amp;#39; @param np_obscure_num Numeric - Obscure number
#&amp;#39; @param bp_na_to_num Boolean if TRUE, convert NA to num. If FALSE, convert num to NA
#&amp;#39; @return A data frame with converted NAs
#&amp;#39; @export

df_convertNAs_to_obscureNo = function(dp_dataframe, np_obscure_num, bp_na_to_num){

  if(bp_na_to_num == T){

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., is.na(.), as.character(np_obscure_num)))  

  }else if(bp_na_to_num == F){
    
    bf_is_obscure_num = function(np_num){
      return(np_num == np_obscure_num)
    }

    bf_is_obscure_num_string = function(np_num){
      return(as.character(np_num) == as.character(np_obscure_num))
    }

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., bf_is_obscure_num_string(.), NA))
  }
  
  return(dConverted_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To illustrate how we could use the function, first, I load in the car dataset.&lt;/p&gt;
&lt;p&gt;Second, I insert NA values into 1 of the cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2
## 1     4   NA     2
## 2     4   10    10
## 3     7    4     4
## 4     7   22    22
## 5     8   16    16
## 6     9   10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third, I implement a recoding-logic as follows,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2      NA
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice in the first row that just because dist is NA, is_true returns NA. This holds true for even longer recoding rules. A mere NA would render the return value to be NA even if you would expect the result to be 1!&lt;/p&gt;
&lt;p&gt;Here comes the highlight of the post.&lt;/p&gt;
&lt;p&gt;By applying the df_convertNAs_to_obscureNo function I wrote above to the data frame with the following parameters,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data frame name;&lt;/li&gt;
&lt;li&gt;An obscure number that will never be used in recoding (e.g. -123456);&lt;/li&gt;
&lt;li&gt;And a boolean flag (will explain this later),&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You would be able to skirt the issue I highlighted above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA

data = df_convertNAs_to_obscureNo(data, -1234567, T)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2
## 1     4 -1234567     2
## 2     4       10    10
## 3     7        4     4
## 4     7       22    22
## 5     8       16    16
## 6     9       10    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Recoding results
data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2 is_true
## 1     4 -1234567     2       1
## 2     4       10    10       0
## 3     7        4     4       0
## 4     7       22    22       0
## 5     8       16    16       0
## 6     9       10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So instead of obtaining a NA value, it would return 1 instead!&lt;/p&gt;
&lt;p&gt;And you may ask me - how do I change -123457 back into NA?&lt;/p&gt;
&lt;p&gt;You could simply reuse the same function with a FALSE flag. And presto the obscure value is converted back into NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = df_convertNAs_to_obscureNo(data, -1234567, F)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2       1
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warning!!!&lt;/h2&gt;
&lt;p&gt;Developer/ Data Scientist/ Data Analysis would need to be absolutely sure that this is what he/ she wants. If NA value is expected instead of 1 in the above use case, please do not use the function!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R package: Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects</title>
      <link>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</link>
      <pubDate>Thu, 25 Oct 2018 11:39:24 +0800</pubDate>
      
      <guid>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</guid>
      <description>

&lt;h2 id=&#34;decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects&#34;&gt;Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects&lt;/h2&gt;

&lt;p&gt;If you are someone with a stake in foreign positions, this package I wrote here may be a useful tool to help you understand the impact of foreign currency on your positions. For instance,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you are an investor, you may use it to analyze impact of exchange rate on your investment positions.&lt;/li&gt;
&lt;li&gt;If you are in the treasury department, you may wish to analyze the impact of exchange rates on your bonds.&lt;/li&gt;
&lt;li&gt;If you are in the finance department, you could analyze the exchange rate impact on your foreign revenue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To start using this package, you may first install the devtools package and execute the following command. install_github(&amp;ldquo;jironghuang/RemoveExchangeRateEffects&amp;rdquo;). The R documentation is as follows,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/exch_documentation1.png&#34; alt=&#34;/post/img/exch_documentation1.png&#34;&gt;
&lt;img src=&#34;/post/img/exch_documentation2.png&#34; alt=&#34;/post/img/exch_documentation2.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;You may follow the 2 examples to better understand how this package works.&lt;/p&gt;

&lt;h3 id=&#34;example-1&#34;&gt;Example 1&lt;/h3&gt;

&lt;p&gt;In summary, what the example does below is to decompose 1 instrument position in SGD (column value) - from the perspective of someone staying in Singapore - into local static value (i.e if I keep the exchange rate constant at the start of the period) and the residual exchange rate impact.&lt;/p&gt;

&lt;p&gt;If you look at the value at the end of the period (Oct 2018), you would notice that the value in SGD fell from 331 to 261. From the perspective of a Singaporean local - through this package -  we can understand that the appreciation in USD negate the fall in value by 4 SGD.&lt;/p&gt;

&lt;p&gt;If you are an economist, you could have considered the exchange rate elasticities. But let&amp;rsquo;s ignore that for now.&lt;/p&gt;

&lt;h3 id=&#34;quick-example-1-in-r-codes-decomposing-a-single-instrument&#34;&gt;Quick example 1 in R codes (decomposing a single instrument)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;jironghuang/RemoveExchangeRateEffects&amp;quot;)
library(RemoveExchangeRateEffects)

sp_exch_rate_pair = &amp;quot;USDSGD=X&amp;quot;  #exchange rate pair. e.g &amp;quot;USDSGD=X&amp;quot;. &amp;quot;&amp;lt;Foreign_currency&amp;gt;&amp;lt;local_currency&amp;gt;=X&amp;quot;

ap_start_date &amp;lt;- as.Date(&amp;quot;2017-10-01&amp;quot;)  #starting date of portfolio e.g. 2017-10-01
ap_end_date &amp;lt;- as.Date(&amp;quot;2020-10-01&amp;quot;) #ending date of portfolio e.g. 2020-10-01. If you include a date beyond current date, the function will use the current date instead
np_mthly_yearly = &amp;quot;monthly&amp;quot;  #alternatively this could be &amp;quot;yearly&amp;quot;&amp;quot;

data(eg_dat) #example dataset that I included in this package
dp_dates_investment_value = instrument
o_exchRate_effect &amp;lt;- exchange_rate_decomposition(sp_exch_rate_pair, ap_start_date, ap_end_date, np_mthly_yearly, dp_dates_investment_value)
o_exchRate_effect$get_portfolio()

    value_in_sgd exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53   1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85   1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35   1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31   1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06   1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13   1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90   1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73   1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95   1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14   1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66   1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77   1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95   1.38061  189.0107           257.0734        3.8766087

o_exchRate_effect$get_diff_portfolio_value()
[1] 3.8766087
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example 2&lt;/h3&gt;

&lt;p&gt;The second example builds upon the first. I&amp;rsquo;ve expanded the previous function to decompose multiple instruments at once.&lt;/p&gt;

&lt;p&gt;get_full_decomposition() returns a list of data frames with the decompositions.&lt;/p&gt;

&lt;p&gt;But before you implement the function above, you would have to add the information via the mutator functions as shown in the example below (the functions with the prefix set)&lt;/p&gt;

&lt;h3 id=&#34;quick-example-2-in-r-codes-decomposing-multiple-instruments-at-1-time&#34;&gt;Quick example 2 in R codes (Decomposing multiple instruments at 1 time)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;data(eg_dat)

er = c(&amp;quot;USDSGD=X&amp;quot;, &amp;quot;GBPSGD=X&amp;quot;)
start_date = c(&amp;quot;2017-10-01&amp;quot;, &amp;quot;2017-10-01&amp;quot;)
end_date = c(&amp;quot;2020-10-01&amp;quot;, &amp;quot;2020-10-01&amp;quot;)
freq = c(&amp;quot;monthly&amp;quot;, &amp;quot;monthly&amp;quot;)
dat = list(eg_dat, eg_dat)


o_exchRate_effect &amp;lt;- multiple_exchange_rate_decomposition(2)
o_exchRate_effect$set_sa_exch_rate_pair(er) #adding an array of exchange rate pairs
o_exchRate_effect$set_sa_start_date(start_date) #adding an array of starting dates
o_exchRate_effect$set_sa_end_date(end_date) #adding an array of ending dates
o_exchRate_effect$set_sa_mthly_yearly(freq) #adding an array of &amp;quot;monthly&amp;quot; or &amp;quot;yearly&amp;quot; option
o_exchRate_effect$set_dl_dates_investment_value(dat) #adding list of data frames
o_exchRate_effect$get_full_decomposition()  #carry out decomposition and obtain a list of data frames

[[1]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85       1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35       1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31       1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06       1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13       1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90       1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73       1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95       1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14       1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66       1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77       1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95       1.38061  189.0107           257.0734        3.8766087

[[2]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.79703  184.4877           331.5300        0.0000000
Nov 2017 308.85       1.80690  170.9281           307.1629        1.6870605
Dec 2017 311.35       1.79812  173.1531           311.1613        0.1887369
Jan 2018 354.31       1.85680  190.8175           342.9048       11.4051640
Feb 2018 343.06       1.84162  186.2816           334.7537        8.3062984
Mar 2018 266.13       1.83911  144.7059           260.0408        6.0892228
Apr 2018 293.90       1.82588  160.9635           289.2562        4.6437963
May 2018 284.73       1.77878  160.0704           287.6513       -2.9212846
Jun 2018 342.95       1.78907  191.6918           344.4759       -1.5258666
Jul 2018 298.14       1.78638  166.8962           299.9175       -1.7774444
Aug 2018 301.66       1.77860  169.6053           304.7858       -3.1258259
Sep 2018 264.77       1.78285  148.5094           266.8759       -2.1058633
Oct 2018 260.95       1.76900  147.5127           265.0848       -4.1347817

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Shift Share Analysis Package I developed</title>
      <link>/post/shift-share-analysis-package/</link>
      <pubDate>Sat, 22 Sep 2018 12:23:28 +0800</pubDate>
      
      <guid>/post/shift-share-analysis-package/</guid>
      <description>

&lt;h2 id=&#34;shift-share-analysis-package-i-developed&#34;&gt;Shift-share Analysis Package I developed&lt;/h2&gt;

&lt;p&gt;During my career, I often have to deal with compositional &amp;amp; within group effects. For instance, the employment rate fell by 3% across 2 period. How much of it is due to an increase in employment rate within the sub-group and how much of it is due to compositional shift (for example ageing population).&lt;/p&gt;

&lt;p&gt;A formal way to explain these effects is known as shift-share analysis. It allows you to decompose percentage point change or absolute changes (WIP) into within group and across group effects.&lt;/p&gt;

&lt;p&gt;A package currently used in the R community is REAT, but it doesn&amp;rsquo;t allow you to decompose the effects into finer categories. Hence, I hope this package developed here is able to fill up the gap.&lt;/p&gt;

&lt;p&gt;Some examples that you may use this tool are,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Employment rate&lt;/li&gt;
&lt;li&gt;Demographics rate&lt;/li&gt;
&lt;li&gt;Basketball field goal % decomposed into 2 point vs 3 point&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To start using this package, you may first install the devtools package and execute the following command. install_github(&amp;ldquo;jironghuang/shiftshare&amp;rdquo;).&lt;/p&gt;

&lt;p&gt;And if you are interested in the codes used to develop the package, you may visit the following link &lt;a href=&#34;https://github.com/jironghuang/shiftshare&#34; target=&#34;_blank&#34;&gt;https://github.com/jironghuang/shiftshare&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You may follow the example to better understand how this package works. Essentially, what the example does is to decomposte the 29.7% point change into 3 effects. Within effect == 68.8%,  Across_effect == -7.4% and Dynamic Effect == -31.8%&lt;/p&gt;

&lt;h3 id=&#34;quick-example&#34;&gt;Quick example&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;emp1 = c(10, 20, 40, 50)
pop1 = c(40, 50, 60, 70)
emp2 = c(20, 30, 50, 60)
pop2 = c(50, 70, 20, 50)

ss_analysis &amp;lt;- shift_share(ap_grp_labels = c(&amp;quot;grp1&amp;quot;, &amp;quot;grp2&amp;quot;, &amp;quot;grp3&amp;quot;, &amp;quot;grp4&amp;quot;),
                           ap_numerator1 = emp1,
                           ap_numerator2 = emp2,
                           ap_denominator1 = pop1,
                           ap_denominator2 = pop2)
                           
&amp;gt; ss_analysis$get_effects()
  grp_labels     prop1     prop2     rate1     rate2 within_effect across_effect dynamic_effect overall_effect
1       grp1 0.1818182 0.2631579 0.2500000 0.4000000   0.027272727    0.02033493    0.012200957     0.05980861
2       grp2 0.2272727 0.3684211 0.4000000 0.4285714   0.006493506    0.05645933    0.004032809     0.06698565
3       grp3 0.2727273 0.1052632 0.6666667 2.5000000   0.500000000   -0.11164274   -0.307017544     0.08133971
4       grp4 0.3181818 0.2631579 0.7142857 1.2000000   0.154545455   -0.03930280   -0.026725906     0.08851675  

&amp;gt; ss_analysis$get_agg_effects()
         Description within_effect across_effect dynamic_effect overall_effect
1 aggregated_effects     0.6883117   -0.07415129     -0.3175097      0.2966507

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
