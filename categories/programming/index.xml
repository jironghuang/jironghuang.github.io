<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>programming on Jirong&#39;s sandbox</title>
    <link>/categories/programming/</link>
    <description>Recent content in programming on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Mar 2020 11:50:49 +0800</lastBuildDate>
    <atom:link href="/categories/programming/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Integrating volatility targeting into Jarvis, my expert advisor</title>
      <link>/post/volatility-targeting/</link>
      <pubDate>Sun, 15 Mar 2020 11:50:49 +0800</pubDate>
      
      <guid>/post/volatility-targeting/</guid>
      <description>

&lt;h2 id=&#34;volatility-targeting&#34;&gt;Volatility targeting&lt;/h2&gt;

&lt;p&gt;Currently, I&amp;rsquo;ve a suite of toolkits integrated into my Jarvis that advises me on the investing decisions that I&amp;rsquo;ve to make on a daily basis.&lt;/p&gt;

&lt;p&gt;On the latest feature I cobbled together on a Saturday evening, 2 weeks ago, I&amp;rsquo;ve decided to measure the volatility of my portfolio formally.&lt;/p&gt;

&lt;p&gt;Why I&amp;rsquo;m doing this is because managing risks in the form of volatility is easier than targeting returns.&lt;/p&gt;

&lt;p&gt;On any given day, it&amp;rsquo;s easier to predict volatility than returns itself because of its persistent nature.&lt;/p&gt;

&lt;p&gt;Think of it as a coin flips with Binomal distribution: B ~ (n, p).&lt;/p&gt;

&lt;p&gt;P is the probability for which you expect a positive expected payout.&lt;/p&gt;

&lt;p&gt;And variance of this win-lose distribution is n * p * (1-p). This variance is easier to &amp;lsquo;predict&amp;rsquo; based on your win rates.&lt;/p&gt;

&lt;p&gt;But market is not simply a binomial distribution, it also needs to take into consideration the portfolio size exposed to market risks and sequence of returns (autocorrelation here) at any given day.&lt;/p&gt;

&lt;p&gt;Current distribution of my portfolio win rate is around 65 to 70%. But because of some negative skew in returns, my portfolio is languishing at status quo (0%) since start of the year.&lt;/p&gt;

&lt;p&gt;Though my portfolio have outperformed the indexes by a factor of 2 to 3, the steep drawdown during March period of my portfolio could have been prevented if I&amp;rsquo;ve put a hedge early on (if I&amp;rsquo;ve done it through a systematic quant way) to maintain a fixed volatility target.&lt;/p&gt;

&lt;p&gt;Hence, I decided to integrate the volatility targeting feature into Jarvis that advises me daily on the amount of hedge to place to maintain my portfolio at a constant volatility target; not a perfect way to maintain volatility target (since it&amp;rsquo;s adjusted based on market risk beta) but I find it cumbersome to sell off multiple counters on a daily basis. I used a global etf as a proxy to maintain my risk level since my portfolio has a global tilt.&lt;/p&gt;

&lt;p&gt;Here is the volatility of my unlevered portfolio. Notice how it spiked up 5 times in the month of March this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/vol_target.png&#34; alt=&#34;/post/img/vol_target.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;What volatility targeting does is to supposedly to turn the volatility curve into a straight line.&lt;/p&gt;

&lt;p&gt;You may find the code below. I won&amp;rsquo;t delve into the details but this is essentially what it does,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Obtain my current positions from googlesheet&lt;/li&gt;
&lt;li&gt;Compute my portfolio value converted into SGD now and historically based on current positions&lt;/li&gt;
&lt;li&gt;Compute annual standard deviation (normal and exponential version shared by Robert Carver) of my portfolio based on past 36 days daily returns. Annual standard deviation of returns of portfolio = Daily standard deviation of returns of portfolio * sqrt(252)&lt;/li&gt;
&lt;li&gt;Compute beta of my portfolio against VT, global benchmark (at the moment around 0.2)&lt;/li&gt;
&lt;li&gt;Compute &amp;lsquo;imperfect&amp;rsquo; hedge required through beta to achieve volatility target of 15%&lt;/li&gt;
&lt;li&gt;Pushes a notification to me on how much additional/ lesser hedge is required&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#Initialization
sapply(c(&amp;quot;ggplot2&amp;quot;, &amp;quot;plotly&amp;quot;, &amp;quot;quantmod&amp;quot;, &amp;quot;pushoverr&amp;quot;, &amp;quot;zoo&amp;quot;, &amp;quot;dplyr&amp;quot;, &amp;quot;roll&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;pushoverr&amp;quot;, &amp;quot;googlesheets&amp;quot;, &amp;quot;pracma&amp;quot;), require, character.only = T)
source(&#39;util/extract_stock_prices.R&#39;)

rsd_file = Sys.getenv(&amp;quot;GOOGLESHEET&amp;quot;)
gs_auth(token = rsd_file)
suppressMessages(gs_auth(token = rsd_file, verbose = FALSE))

dat = gs_title(&amp;quot;Investment&amp;quot;)
gs_ws_ls(dat)   #tab names
data &amp;lt;- gs_read(ss=dat, ws = &amp;quot;debt_to_equity&amp;quot;, skip=0)
leverage = 1 + data$Ratio[3]
current_hedge_value = data$Ratio[nrow(data)]

num_units = 10
currency = &amp;quot;SGD=X&amp;quot;
last_date = Sys.Date() - 252

##########################Format data function##########################

data_format = function(ticker, num_units, currency, last_date){
  
#Read data
data = df_crawl_time_series(ticker, &amp;quot;1970-07-01&amp;quot;, &amp;quot;2030-12-30&amp;quot;)
  
#Fill NAs
data$Adj.Close = na.locf(data$Adj.Close)

#Subset out data-frame
df = data.frame(Date = data$Date, Price = data$Adj.Close, stringsAsFactors = F)

#Read in num_units
df$num_units = num_units

#Read in currency
currency = df_crawl_time_series(currency, &amp;quot;1970-07-01&amp;quot;, &amp;quot;2030-12-30&amp;quot;)
currency$Adj.Close = na.locf(currency$Adj.Close)
currency = subset(currency, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Adj.Close&amp;quot;))

#Merge in currency
df = df %&amp;gt;%
      left_join(., currency, by = c(&amp;quot;Date&amp;quot;))

#Convert to local currency
df$local_unit_value = df$Price * df$Adj.Close
df$local_value = df$local_unit_value * df$num_units

#Return date, portfolio value     
df$Date = as.Date(df$Date)

#ticker
df$ticker = ticker

df_sub = subset(df, df$Date &amp;gt;= last_date)

return(df_sub)    
}

##########################Format_portfolio##########################
# df_tickers = data.frame(tickers = c(&amp;quot;TLT&amp;quot;, &amp;quot;IEF&amp;quot;, &amp;quot;SPY&amp;quot;, ))
df_tickers &amp;lt;- gs_read(ss=dat, ws = &amp;quot;investment_live&amp;quot;, skip=0)

# df_tickers = read.csv(&amp;quot;positions.csv&amp;quot;, stringsAsFactors = F) 
df_tickers = filter(df_tickers, num_units &amp;gt; 0)
df_tickers = subset(df_tickers, df_tickers$Ticker != &amp;quot;CNYB.AS&amp;quot;)

current_value = sum(df_tickers$value)

df_tickers = df_tickers[, 1:5]
df_tickers[which(df_tickers$exch_rate_type == &amp;quot;SGDHKD=X&amp;quot;), 5] = &amp;quot;HKDSGD=X&amp;quot;


portfolio_format = function(df_tickers, last_date){

i = 1
ticker_agg = data_format(df_tickers$Ticker[i], df_tickers$num_units[i], df_tickers$exch_rate_type[i], last_date)  
  
for(i in 2:nrow(df_tickers)){
  print(i)
  ticker_ind = data_format(df_tickers$Ticker[i], df_tickers$num_units[i], df_tickers$exch_rate_type[i], last_date)  
  ticker_agg = rbind(ticker_agg, ticker_ind)
}
  
# portfolio = mapply(data_format, 
#                ticker = df_tickers[1:2,]$Ticker, 
#                num_units = df_tickers[1:2,]$num_units, 
#                currency = df_tickers[1:2,]$exch_rate_type, 
#                last_date = c(last_date, 2)
#                #,
#                #SIMPLIFY = F
#                )  
  
portfolio = ticker_agg %&amp;gt;%
              group_by(Date) %&amp;gt;%
              summarize(portfolio_value = sum(local_value, na.rm = T),
                        num_counters = n()
                        )

portfolio$portfolio_value_adj = ifelse(portfolio$num_counters &amp;lt; 23, NA, portfolio$portfolio_value)

portfolio$upper_sd = mean(portfolio$portfolio_value_adj, na.rm = T) + 0.4 * sd(portfolio$portfolio_value_adj, na.rm = T)
portfolio$lower_sd = mean(portfolio$portfolio_value_adj, na.rm = T) - 0.4 * sd(portfolio$portfolio_value_adj, na.rm = T)
portfolio$portfolio_value_adj2 = ifelse((portfolio$portfolio_value_adj  &amp;gt; portfolio$upper_sd) | (portfolio$portfolio_value_adj  &amp;lt; portfolio$lower_sd),
                                             NA,
                                             portfolio$portfolio_value_adj)
portfolio$portfolio_value_adj2 = na.locf(portfolio$portfolio_value_adj2)

if(!is.na(current_value)){
  portfolio$portfolio_value_adj2[nrow(portfolio)] = current_value
}

portfolio$returns = ROC(portfolio$portfolio_value_adj2) 
portfolio$returns_100 = portfolio$returns * 100

portfolio$roll_std = roll_sd(portfolio$returns, 30)
portfolio$roll_std_annual = portfolio$roll_std * (252 ^ 0.5)

return(portfolio)

}


df = portfolio_format(df_tickers, last_date = (Sys.Date()-365))
df$roll_std_annnual = as.numeric(df$roll_std_annual)
plot(df$roll_std_annual)

reduce_times = leverage *df$roll_std_annual[nrow(df) - 0]/ 0.15

#Consolidate portfolio and find beta aginst VT and SPY
#Portfolio value / number of times to reduce
benchmark = &amp;quot;VT&amp;quot;

##########################Find beta of portfolio######################################3
find_beta = function(df, benchmark){

price_bench = data_format(benchmark, num_units, currency, last_date)
price_bench$returns = ROC(price_bench$local_value)
price_bench = subset(price_bench, select = c(&amp;quot;Date&amp;quot;, &amp;quot;returns&amp;quot;))    
names(price_bench)[2] = &amp;quot;returns_benchmark&amp;quot;
# price_bench$returns[0] = 0
#price_bench$returns = na.locf(price_bench$returns)

df_agg = df %&amp;gt;%
      left_join(., price_bench, by = &amp;quot;Date&amp;quot;)

reg = lm(df_agg$returns ~ df_agg$returns_benchmark)
return(as.numeric(reg$coefficients[2]))

}

beta = find_beta(df, benchmark)

##########################EMA of volatility######################################3
df$square_returns = df$returns ^ 2
df$square_returns[1] = df$square_returns[2]
df$ema_vol =  movavg(df$square_returns, 36, type = &amp;quot;e&amp;quot;)
df$ema_sd = df$ema_vol ^ 0.5 * (252 ^ 0.5)

reduce_times_exp = leverage * df$ema_sd[nrow(df) - 0]/ 0.15


##########################Find hedge required to target risk######################################3
hedge = (df$portfolio_value_adj2[nrow(df)] - (df$portfolio_value_adj2[nrow(df)]/ reduce_times_exp)) * beta
hedge = round(hedge, 0)

##########################Pushing notifications to inform how much hedge is required######################################
msg = paste0(&amp;quot;You should hedge &amp;quot;, hedge, 
             &amp;quot; Current hedge value is &amp;quot;, current_hedge_value,
             &amp;quot; Additional hedge required &amp;quot;, (hedge - current_hedge_value)
)

print(msg)

pushover(message = msg, 
         user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;))

plot(df$ema_sd)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Mapreduce using Java</title>
      <link>/post/mapreduce-in-java/</link>
      <pubDate>Sun, 15 Mar 2020 11:50:49 +0800</pubDate>
      
      <guid>/post/mapreduce-in-java/</guid>
      <description>

&lt;h2 id=&#34;mapreduce-using-java&#34;&gt;Mapreduce using java&lt;/h2&gt;

&lt;p&gt;I haven&amp;rsquo;t coded in java in eons. The assignment (Mapreduce, Pig and Spark) I worked on over last 3 weeks is a good way to jolt me out from my comfort zone.&lt;/p&gt;

&lt;p&gt;Java is something I need to brush up on before taking the Software Development Process module which requires me to write an android app. Argh!&lt;/p&gt;

&lt;p&gt;Back to Mapreduce. It&amp;rsquo;s a useful framework if you&amp;rsquo;ve to summarise huge datasets (gigabytes, terabytes). Here are some of the common steps in mapreduce,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In Mapper method, each entity of data (e.g. row, word) is tokenized and assigned values&lt;/li&gt;
&lt;li&gt;In Reduce method, data is aggregated (e.g sum, average, etc.)&lt;/li&gt;
&lt;li&gt;In Main function, mapreduce jobs are managed through certain parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Mapreduce data types are different from Java data types! It could be confusing at times. But if you get used to it, it will be easier.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MapReuceJobs {

public static class TokenizerMapper
extends Mapper&amp;lt;LongWritable, Text, Text, Text&amp;gt;
{
&amp;lt;Deliberately left blank&amp;gt;
...Each row of data is tokenize and assign a number
}

//Reducer
public static class IntSumReducer
extends Reducer&amp;lt;Text,Text,Text,Text&amp;gt; {

&amp;lt;Deliberately left blank&amp;gt;
...Aggregated data is collated here
}



//IntSumReducer  
public static void main(String[] args) throws Exception {

&amp;lt;Deliberately left blank&amp;gt;
...Jobs are managed here
}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Embedding D3 interactive charts part 2</title>
      <link>/post/embedding-d3-interactive-charts-part2/</link>
      <pubDate>Sun, 23 Feb 2020 11:50:49 +0800</pubDate>
      
      <guid>/post/embedding-d3-interactive-charts-part2/</guid>
      <description>

&lt;h2 id=&#34;embedding-d3-interactive-charts-part-2-testing-reading-in-of-file-from-directory&#34;&gt;Embedding D3 interactive charts Part 2 - Testing reading in of file from directory&lt;/h2&gt;

&lt;p&gt;Just having fun - testing to see if I could embed D3 charts in my blog.&lt;/p&gt;

&lt;p&gt;Seems like it works too! But I would have to upload the csv under public folder first.&lt;/p&gt;

&lt;p&gt;&lt;meta charset=&#34;utf-8&#34;&gt;&lt;/p&gt;

&lt;!-- Load d3.js --&gt;

&lt;script src=&#34;https://d3js.org/d3.v4.js&#34;&gt;&lt;/script&gt;

&lt;!-- Create a div where the graph will take place --&gt;

&lt;div id=&#34;my_dataviz&#34;&gt;&lt;/div&gt;

&lt;script&gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&#34;#my_dataviz&#34;)
  .append(&#34;svg&#34;)
    .attr(&#34;width&#34;, width + margin.left + margin.right)
    .attr(&#34;height&#34;, height + margin.top + margin.bottom)
  .append(&#34;g&#34;)
    .attr(&#34;transform&#34;,
          &#34;translate(&#34; + margin.left + &#34;,&#34; + margin.top + &#34;)&#34;);

//Read the data
d3.csv(&#34;/post/data/2_TwoNum.csv&#34;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&#34;g&#34;)
    .attr(&#34;transform&#34;, &#34;translate(0,&#34; + height + &#34;)&#34;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&#34;g&#34;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&#34;#my_dataviz&#34;)
    .append(&#34;div&#34;)
    .style(&#34;opacity&#34;, 0)
    .attr(&#34;class&#34;, &#34;tooltip&#34;)
    .style(&#34;background-color&#34;, &#34;white&#34;)
    .style(&#34;border&#34;, &#34;solid&#34;)
    .style(&#34;border-width&#34;, &#34;1px&#34;)
    .style(&#34;border-radius&#34;, &#34;5px&#34;)
    .style(&#34;padding&#34;, &#34;10px&#34;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&#34;opacity&#34;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&#34;The exact value of&lt;br&gt;the Ground Living area is: &#34; + d.GrLivArea)
      .style(&#34;left&#34;, (d3.mouse(this)[0]+90) + &#34;px&#34;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&#34;top&#34;, (d3.mouse(this)[1]) + &#34;px&#34;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&#34;opacity&#34;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&#34;dot&#34;)
    .data(data.filter(function(d,i){return i&lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&#34;circle&#34;)
      .attr(&#34;cx&#34;, function (d) { return x(d.GrLivArea); } )
      .attr(&#34;cy&#34;, function (d) { return y(d.SalePrice); } )
      .attr(&#34;r&#34;, 7)
      .style(&#34;fill&#34;, &#34;#69b3a2&#34;)
      .style(&#34;opacity&#34;, 0.3)
      .style(&#34;stroke&#34;, &#34;white&#34;)
    .on(&#34;mouseover&#34;, mouseover )
    .on(&#34;mousemove&#34;, mousemove )
    .on(&#34;mouseleave&#34;, mouseleave )

})

&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;

&amp;lt;!-- Load d3.js --&amp;gt;
&amp;lt;script src=&amp;quot;https://d3js.org/d3.v4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;!-- Create a div where the graph will take place --&amp;gt;
&amp;lt;div id=&amp;quot;my_dataviz&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;

&amp;lt;script&amp;gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&amp;quot;#my_dataviz&amp;quot;)
  .append(&amp;quot;svg&amp;quot;)
    .attr(&amp;quot;width&amp;quot;, width + margin.left + margin.right)
    .attr(&amp;quot;height&amp;quot;, height + margin.top + margin.bottom)
  .append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;,
          &amp;quot;translate(&amp;quot; + margin.left + &amp;quot;,&amp;quot; + margin.top + &amp;quot;)&amp;quot;);

//Read the data
d3.csv(&amp;quot;/post/data/2_TwoNum.csv&amp;quot;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;, &amp;quot;translate(0,&amp;quot; + height + &amp;quot;)&amp;quot;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&amp;quot;g&amp;quot;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&amp;quot;#my_dataviz&amp;quot;)
    .append(&amp;quot;div&amp;quot;)
    .style(&amp;quot;opacity&amp;quot;, 0)
    .attr(&amp;quot;class&amp;quot;, &amp;quot;tooltip&amp;quot;)
    .style(&amp;quot;background-color&amp;quot;, &amp;quot;white&amp;quot;)
    .style(&amp;quot;border&amp;quot;, &amp;quot;solid&amp;quot;)
    .style(&amp;quot;border-width&amp;quot;, &amp;quot;1px&amp;quot;)
    .style(&amp;quot;border-radius&amp;quot;, &amp;quot;5px&amp;quot;)
    .style(&amp;quot;padding&amp;quot;, &amp;quot;10px&amp;quot;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&amp;quot;opacity&amp;quot;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&amp;quot;The exact value of&amp;lt;br&amp;gt;the Ground Living area is: &amp;quot; + d.GrLivArea)
      .style(&amp;quot;left&amp;quot;, (d3.mouse(this)[0]+90) + &amp;quot;px&amp;quot;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&amp;quot;top&amp;quot;, (d3.mouse(this)[1]) + &amp;quot;px&amp;quot;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&amp;quot;opacity&amp;quot;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&amp;quot;dot&amp;quot;)
    .data(data.filter(function(d,i){return i&amp;lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&amp;quot;circle&amp;quot;)
      .attr(&amp;quot;cx&amp;quot;, function (d) { return x(d.GrLivArea); } )
      .attr(&amp;quot;cy&amp;quot;, function (d) { return y(d.SalePrice); } )
      .attr(&amp;quot;r&amp;quot;, 7)
      .style(&amp;quot;fill&amp;quot;, &amp;quot;#69b3a2&amp;quot;)
      .style(&amp;quot;opacity&amp;quot;, 0.3)
      .style(&amp;quot;stroke&amp;quot;, &amp;quot;white&amp;quot;)
    .on(&amp;quot;mouseover&amp;quot;, mouseover )
    .on(&amp;quot;mousemove&amp;quot;, mousemove )
    .on(&amp;quot;mouseleave&amp;quot;, mouseleave )

})

&amp;lt;/script&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Embedding D3 interactive charts</title>
      <link>/post/embedding-d3-interactive-charts/</link>
      <pubDate>Sun, 23 Feb 2020 11:46:40 +0800</pubDate>
      
      <guid>/post/embedding-d3-interactive-charts/</guid>
      <description>

&lt;h2 id=&#34;embedding-d3-interactive-charts&#34;&gt;Embedding D3 interactive charts&lt;/h2&gt;

&lt;p&gt;Just having fun - testing to see if I could embed D3 charts in my blog.&lt;/p&gt;

&lt;p&gt;Seems like it works!&lt;/p&gt;

&lt;p&gt;&lt;meta charset=&#34;utf-8&#34;&gt;&lt;/p&gt;

&lt;!-- Load d3.js --&gt;

&lt;script src=&#34;https://d3js.org/d3.v4.js&#34;&gt;&lt;/script&gt;

&lt;!-- Create a div where the graph will take place --&gt;

&lt;div id=&#34;my_dataviz&#34;&gt;&lt;/div&gt;

&lt;script&gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&#34;#my_dataviz&#34;)
  .append(&#34;svg&#34;)
    .attr(&#34;width&#34;, width + margin.left + margin.right)
    .attr(&#34;height&#34;, height + margin.top + margin.bottom)
  .append(&#34;g&#34;)
    .attr(&#34;transform&#34;,
          &#34;translate(&#34; + margin.left + &#34;,&#34; + margin.top + &#34;)&#34;);

//Read the data
d3.csv(&#34;https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/2_TwoNum.csv&#34;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&#34;g&#34;)
    .attr(&#34;transform&#34;, &#34;translate(0,&#34; + height + &#34;)&#34;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&#34;g&#34;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&#34;#my_dataviz&#34;)
    .append(&#34;div&#34;)
    .style(&#34;opacity&#34;, 0)
    .attr(&#34;class&#34;, &#34;tooltip&#34;)
    .style(&#34;background-color&#34;, &#34;white&#34;)
    .style(&#34;border&#34;, &#34;solid&#34;)
    .style(&#34;border-width&#34;, &#34;1px&#34;)
    .style(&#34;border-radius&#34;, &#34;5px&#34;)
    .style(&#34;padding&#34;, &#34;10px&#34;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&#34;opacity&#34;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&#34;The exact value of&lt;br&gt;the Ground Living area is: &#34; + d.GrLivArea)
      .style(&#34;left&#34;, (d3.mouse(this)[0]+90) + &#34;px&#34;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&#34;top&#34;, (d3.mouse(this)[1]) + &#34;px&#34;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&#34;opacity&#34;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&#34;dot&#34;)
    .data(data.filter(function(d,i){return i&lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&#34;circle&#34;)
      .attr(&#34;cx&#34;, function (d) { return x(d.GrLivArea); } )
      .attr(&#34;cy&#34;, function (d) { return y(d.SalePrice); } )
      .attr(&#34;r&#34;, 7)
      .style(&#34;fill&#34;, &#34;#69b3a2&#34;)
      .style(&#34;opacity&#34;, 0.3)
      .style(&#34;stroke&#34;, &#34;white&#34;)
    .on(&#34;mouseover&#34;, mouseover )
    .on(&#34;mousemove&#34;, mousemove )
    .on(&#34;mouseleave&#34;, mouseleave )

})

&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Load d3.js --&amp;gt;
&amp;lt;script src=&amp;quot;https://d3js.org/d3.v4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;!-- Create a div where the graph will take place --&amp;gt;
&amp;lt;div id=&amp;quot;my_dataviz&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;

&amp;lt;script&amp;gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&amp;quot;#my_dataviz&amp;quot;)
  .append(&amp;quot;svg&amp;quot;)
    .attr(&amp;quot;width&amp;quot;, width + margin.left + margin.right)
    .attr(&amp;quot;height&amp;quot;, height + margin.top + margin.bottom)
  .append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;,
          &amp;quot;translate(&amp;quot; + margin.left + &amp;quot;,&amp;quot; + margin.top + &amp;quot;)&amp;quot;);

//Read the data
d3.csv(&amp;quot;https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/2_TwoNum.csv&amp;quot;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;, &amp;quot;translate(0,&amp;quot; + height + &amp;quot;)&amp;quot;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&amp;quot;g&amp;quot;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&amp;quot;#my_dataviz&amp;quot;)
    .append(&amp;quot;div&amp;quot;)
    .style(&amp;quot;opacity&amp;quot;, 0)
    .attr(&amp;quot;class&amp;quot;, &amp;quot;tooltip&amp;quot;)
    .style(&amp;quot;background-color&amp;quot;, &amp;quot;white&amp;quot;)
    .style(&amp;quot;border&amp;quot;, &amp;quot;solid&amp;quot;)
    .style(&amp;quot;border-width&amp;quot;, &amp;quot;1px&amp;quot;)
    .style(&amp;quot;border-radius&amp;quot;, &amp;quot;5px&amp;quot;)
    .style(&amp;quot;padding&amp;quot;, &amp;quot;10px&amp;quot;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&amp;quot;opacity&amp;quot;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&amp;quot;The exact value of&amp;lt;br&amp;gt;the Ground Living area is: &amp;quot; + d.GrLivArea)
      .style(&amp;quot;left&amp;quot;, (d3.mouse(this)[0]+90) + &amp;quot;px&amp;quot;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&amp;quot;top&amp;quot;, (d3.mouse(this)[1]) + &amp;quot;px&amp;quot;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&amp;quot;opacity&amp;quot;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&amp;quot;dot&amp;quot;)
    .data(data.filter(function(d,i){return i&amp;lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&amp;quot;circle&amp;quot;)
      .attr(&amp;quot;cx&amp;quot;, function (d) { return x(d.GrLivArea); } )
      .attr(&amp;quot;cy&amp;quot;, function (d) { return y(d.SalePrice); } )
      .attr(&amp;quot;r&amp;quot;, 7)
      .style(&amp;quot;fill&amp;quot;, &amp;quot;#69b3a2&amp;quot;)
      .style(&amp;quot;opacity&amp;quot;, 0.3)
      .style(&amp;quot;stroke&amp;quot;, &amp;quot;white&amp;quot;)
    .on(&amp;quot;mouseover&amp;quot;, mouseover )
    .on(&amp;quot;mousemove&amp;quot;, mousemove )
    .on(&amp;quot;mouseleave&amp;quot;, mouseleave )

})

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Fuzzy matching with many to many matches without loops</title>
      <link>/post/fuzzy_matching_no_loops/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/fuzzy_matching_no_loops/</guid>
      <description>


&lt;div id=&#34;fuzzy-matching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fuzzy matching&lt;/h2&gt;
&lt;p&gt;As a computer scientist graduate, I always strive to reduce my computational complexity through parallelization or vectorization!&lt;/p&gt;
&lt;p&gt;Explicit loops in data science is the root of evil!&lt;/p&gt;
&lt;p&gt;For loops &amp;amp; while loops have their places but definitely not in data science space (fairly broad statement here).&lt;/p&gt;
&lt;p&gt;In this post here, I hope to show a really cool example that avoids the dreaded O(n square) complexity.&lt;/p&gt;
&lt;p&gt;I will be using fuzzy matching to find the closet match of strings in data-frame 2, df2 against data-frame 1, df1.&lt;/p&gt;
&lt;p&gt;Note: I apologise beforehand for lack of documentation because I am simply lazy after a long Friday. Will beef this up with formal R documentation if I choose to wrap it with a R package next time.&lt;/p&gt;
&lt;div id=&#34;creating-the-first-function-to-incorporate-indexing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the first function to incorporate indexing&lt;/h3&gt;
&lt;p&gt;In this function, what I am trying to find is the Levenshtein distance i.e. minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.&lt;/p&gt;
&lt;p&gt;In the test case below, the difference between “abc” and “abcdef” is Levenshtein distance of 3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(sapply(c(&amp;quot;utils&amp;quot;, &amp;quot;dplyr&amp;quot;), require, character.only = T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## utils dplyr 
##  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adist_mod = function(df, index_col, index_row, string){
  dist = adist(df[index_row, index_col], string)
  return(as.numeric(dist))
}

df1 = data.frame(id = 1:7,
                 places = c(&amp;quot;abc&amp;quot;, &amp;quot;tzy&amp;quot;, &amp;quot;abcd&amp;quot;, &amp;quot;wxyz&amp;quot;, &amp;quot;sentosa&amp;quot;, &amp;quot;marina&amp;quot;,&amp;quot;marina2&amp;quot;)
                 )

df1$places = as.character(df1$places)

adist_mod(df1, 2, 1, &amp;quot;abcdef&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-next-function-to-find-the-smallest-levenshtein-distance-in-all-strings-in-df1-against-a-single-row-from-df2.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the next function to find the smallest Levenshtein distance in all strings in df1 against a single row from df2.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create best match function
best_match = function(df, index_col, string){
 
  dist = mapply(adist_mod, 
         index_col = rep(index_col, nrow(df)),
         index_row = 1:nrow(df),
         string = rep(string),
         MoreArgs = list(df)
         )
  
  min_dist = data.frame(
    place2 = rep(string, nrow(df)),
    place1 = df[, index_col],
    dist = dist
  )
  
  min_dist$place2 = as.character(min_dist$place2)
  min_dist$place1 = as.character(min_dist$place1)
  
  
  #return a data frame with df2 and distance
  min_dist = data.frame(dplyr::filter(min_dist,
                                      dist == min(min_dist$dist)
                        ))
  
  return(min_dist)
}

best_match(df1, 2, &amp;quot;abcdef&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   place2 place1 dist
## 1 abcdef   abcd    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;ab&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   place2 place1 dist
## 1     ab    abc    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;sentosa1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     place2  place1 dist
## 1 sentosa1 sentosa    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;marina1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    place2  place1 dist
## 1 marina1  marina    1
## 2 marina1 marina2    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;last-but-not-the-least-find-n-to-m-matches-without-explicit-for-loops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Last but not the least, find n to m matches without explicit for loops&lt;/h3&gt;
&lt;p&gt;The final step with our favorite friend - mapply!&lt;/p&gt;
&lt;p&gt;If you wish to parallelize with more cores, please replace it with mcmapply beforehand. But do load parallel package beforehand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match_all = function(df1, df2, col_index, col_index2){
  
res = mapply(best_match,
       index_col = rep(col_index, nrow(df2)),
       string = df2[, col_index2],
       MoreArgs = list(df = df1),
       SIMPLIFY = F
       )

res_bind = bind_rows(res)

return(res_bind)

}


df2 = data.frame(
  id = 1:3,
  places2 = c(&amp;quot;abcdef&amp;quot;, &amp;quot;sentosa1&amp;quot;, &amp;quot;marina1&amp;quot;)
)
df2$places2 = as.character(df2$places2)

best_match_all(df1, df2, 2, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     place2  place1 dist
## 1   abcdef    abcd    2
## 2 sentosa1 sentosa    1
## 3  marina1  marina    1
## 4  marina1 marina2    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latest lessons learnt from crawling</title>
      <link>/post/crawling-insights/</link>
      <pubDate>Sun, 08 Dec 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/crawling-insights/</guid>
      <description>

&lt;h2 id=&#34;lessons-learnt&#34;&gt;Lessons learnt&lt;/h2&gt;

&lt;p&gt;I just realised that there&amp;rsquo;s a quick way to understand the xpaths&amp;rsquo; patterns.&lt;/p&gt;

&lt;p&gt;In the past, usually what I did is to manually eyeball to infer the patterns from the page source or inspect page.&lt;/p&gt;

&lt;p&gt;Silly me!&lt;/p&gt;

&lt;p&gt;1 quick way to understand the pattern is through the following,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Right click on an element in a web page that you are interested in and click on &amp;lsquo;inspect&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Right click on the node and click &amp;lsquo;copy&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Copy full xpath&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And paste to a notepad.&lt;/p&gt;

&lt;p&gt;Do it for the series of elements that you wish to crawl.&lt;/p&gt;

&lt;p&gt;And presto you are able to observe the patterns!&lt;/p&gt;

&lt;h2 id=&#34;case-in-point&#34;&gt;Case in point&lt;/h2&gt;

&lt;p&gt;With the increment in indexes in the following paths, I could easily do a loop to form the xpaths and feed it into python xpath function and do an &amp;lsquo;extract_first()&amp;lsquo;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[1]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[2]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[3]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[4]/div/div/div/div[1]/div/div[1]/div[3]/a
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Asset allocation notification</title>
      <link>/post/asset_allocation_notification/</link>
      <pubDate>Thu, 12 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/asset_allocation_notification/</guid>
      <description>

&lt;h2 id=&#34;asset-allocation-notification&#34;&gt;Asset allocation notification&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m in the midst of automating/ guiding my life with algorithms (largely inspired by Ray Dalio) - and 1 of the guidelines that I set is on asset allocation,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emerging market and Developed Market should be of the same proportion&lt;/li&gt;
&lt;li&gt;Bonds + Cash proportion should be equivalent to my age. This can deviate in times of crisis when I want to be more opportunistic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it deviates from the portfolio policy statement, it will send me a pushover notification to my phone:)&lt;/p&gt;

&lt;p&gt;Here is a simple example of it works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/asset_notify.jpeg&#34; alt=&#34;/post/img/asset_notify.jpeg&#34;&gt;&lt;/p&gt;

&lt;p&gt;You may find my code below. Briefly this is what it does,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It ssh into my googlesheet&lt;/li&gt;
&lt;li&gt;Pull out the asset allocation and compare against the policy statements&lt;/li&gt;
&lt;li&gt;If it deviates, it will send me a notification&lt;/li&gt;
&lt;li&gt;This is scheduled via a cronjob in linux that runs every midnight&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#Comment on asset allocation
dat = gs_title(&amp;quot;Investment&amp;quot;)
gs_ws_ls(dat)   #tab names
data &amp;lt;- gs_read(ss=dat, ws = &amp;quot;asset_allocation1&amp;quot;, skip=0)
data = as.data.frame(data)

if(data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)] &amp;gt; data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Developed. Shift to Emerging\n&amp;quot;
  msg = paste(msg, &amp;quot;World is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)])
  msg = paste(msg, &amp;quot;Emerging is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)] &amp;gt; data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Emerging. Shift to Developed\n&amp;quot;
  msg = paste(msg, &amp;quot;World is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)])
  msg = paste(msg, &amp;quot;Emerging is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(100 * (data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)] + data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)]) &amp;gt; data$Prop[which(data$Assets == &amp;quot;Age&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Bonds and Cash relative to your age. Shift it out!!!\n&amp;quot;
  msg = paste(msg, &amp;quot;Bonds is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)])
  msg = paste(msg, &amp;quot;Cash is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(100 * (data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)] + data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)]) &amp;lt; data$Prop[which(data$Assets == &amp;quot;Age&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are underweight in Bonds and Cash relative to your age. Move out from equities!!!\n&amp;quot;
  msg = paste(msg, &amp;quot;Bonds is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)])
  msg = paste(msg, &amp;quot;Cash is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ETF watchlist email notification Through Python</title>
      <link>/post/email_notification_python/</link>
      <pubDate>Tue, 10 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/email_notification_python/</guid>
      <description>

&lt;h2 id=&#34;email-notification&#34;&gt;Email notification&lt;/h2&gt;

&lt;p&gt;I finally bit the bullet and updated my previously hideous email notification!&lt;/p&gt;

&lt;p&gt;You may find the updated email notification template here - alongside with the code.&lt;/p&gt;

&lt;p&gt;Feel free to ping me if you are keen to be on the email list too.&lt;/p&gt;

&lt;p&gt;~ Jirong&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/email.png&#34; alt=&#34;/post/img/email.png&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import smtplib, ssl
import datetime
import pandas as pd
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

#Format text
data = pd.read_csv(&#39;/home/jirong/Desktop/github/ETF_watchlist/Output/yahoo_crawled_data.csv&#39;)
data[&#39;Change_fr_52_week_high&#39;] = round(100 * data[&#39;Change_fr_52_week_high&#39;], 1) 
data = data[[&#39;Name&#39;, &#39;Price&#39;, &#39;Change_fr_52_week_high&#39;]].head(20)
data.rename(columns={&#39;Change_fr_52_week_high&#39;:&#39;Change from 52 week high (%)&#39;}, inplace=True)
results = data.to_html()

message = MIMEMultipart(&amp;quot;alternative&amp;quot;)
message[&amp;quot;Subject&amp;quot;] = &amp;quot;ETF Watchlist&amp;quot;
message[&amp;quot;From&amp;quot;] = &amp;quot;jironghuang88@gmail.com&amp;quot;
message[&amp;quot;Bcc&amp;quot;] = &amp;quot;&amp;quot;

# Create the plain-text and HTML version of your message
html = &amp;quot;&amp;quot;&amp;quot;\
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;p&amp;gt;Hey there!       
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;    
       &amp;lt;br&amp;gt;Pls click &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/watch_list/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; for a complete updated ETF watchlist.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;For your convenience, I&#39;ve also appended the top 20 tickers with greatest fall over last 52 week high (potentially cheap but you should triangulate with other sources)&amp;lt;br&amp;gt;
       {0}

       &amp;lt;br&amp;gt;This is part of my daily automated ETF dashboard + Email notification (weekly for email notification) and I thought you may be interested in it.&amp;lt;br&amp;gt; 

       &amp;lt;br&amp;gt;Do also check out my &#39;What is Low - Regression Approach&#39; &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/what-is-low-regression-approach/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; to understand which are the markets that are under or overvalued.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;If this irritates you too much, let me know and I can take you out of this mailing list:)&amp;lt;br&amp;gt;

       &amp;lt;br&amp;gt;Regards,&amp;lt;br&amp;gt;
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
       Jirong
    &amp;lt;/p&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

&amp;quot;&amp;quot;&amp;quot;.format(data.to_html())

# Turn these into plain/html MIMEText objects
#part1 = MIMEText(text, &amp;quot;plain&amp;quot;)
part2 = MIMEText(html, &amp;quot;html&amp;quot;)
#part3 = MIMEText(results, &amp;quot;html&amp;quot;)

# Add HTML/plain-text parts to MIMEMultipart message
# The email client will try to render the last part first
message.attach(part2)

server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587)
server.starttls()
server.login(&amp;quot;jironghuang88@gmail.com&amp;quot;, &amp;quot;&amp;quot;)

recipients = [&amp;quot;jironghuang88@gmail.com&amp;quot;]

server.sendmail(&amp;quot;jironghuang88@gmail.com&amp;quot;, recipients, message.as_string())
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Convert NAs to Obscure Number in Data Frame to Aid in Recoding/ Feature Engineering</title>
      <link>/post/convert_na_num/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/convert_na_num/</guid>
      <description>


&lt;div id=&#34;converting-nas-to-obscure-numbers-to-prevent-the-data-from-messing-up-the-recoding.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Converting NAs to obscure numbers to prevent the data from messing up the recoding.&lt;/h2&gt;
&lt;p&gt;1 issue that I encounter while I data-munge is that NAs in data seem to mess up my recoding. Here’s a neat swiss army knife utility function I developed recently.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(dplyr))

# Converting NA to obscure number to prevent awkward recoding situations that require &amp;amp; !is.na(&amp;lt;variable&amp;gt;)
# Doesn&amp;#39;t work for factors
#&amp;#39; @title Convert NA to obscure number
#&amp;#39; @param dp_dataframe Dataframe in consideration
#&amp;#39; @param np_obscure_num Numeric - Obscure number
#&amp;#39; @param bp_na_to_num Boolean if TRUE, convert NA to num. If FALSE, convert num to NA
#&amp;#39; @return A data frame with converted NAs
#&amp;#39; @export

df_convertNAs_to_obscureNo = function(dp_dataframe, np_obscure_num, bp_na_to_num){

  if(bp_na_to_num == T){

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., is.na(.), as.character(np_obscure_num)))  

  }else if(bp_na_to_num == F){
    
    bf_is_obscure_num = function(np_num){
      return(np_num == np_obscure_num)
    }

    bf_is_obscure_num_string = function(np_num){
      return(as.character(np_num) == as.character(np_obscure_num))
    }

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., bf_is_obscure_num_string(.), NA))
  }
  
  return(dConverted_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To illustrate how we could use the function, first, I load in the car dataset.&lt;/p&gt;
&lt;p&gt;Second, I insert NA values into 1 of the cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2
## 1     4   NA     2
## 2     4   10    10
## 3     7    4     4
## 4     7   22    22
## 5     8   16    16
## 6     9   10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third, I implement a recoding-logic as follows,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2      NA
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice in the first row that just because dist is NA, is_true returns NA. This holds true for even longer recoding rules. A mere NA would render the return value to be NA even if you would expect the result to be 1!&lt;/p&gt;
&lt;p&gt;Here comes the highlight of the post.&lt;/p&gt;
&lt;p&gt;By applying the df_convertNAs_to_obscureNo function I wrote above to the data frame with the following parameters,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data frame name;&lt;/li&gt;
&lt;li&gt;An obscure number that will never be used in recoding (e.g. -123456);&lt;/li&gt;
&lt;li&gt;And a boolean flag (will explain this later),&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You would be able to skirt the issue I highlighted above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA

data = df_convertNAs_to_obscureNo(data, -1234567, T)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2
## 1     4 -1234567     2
## 2     4       10    10
## 3     7        4     4
## 4     7       22    22
## 5     8       16    16
## 6     9       10    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Recoding results
data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2 is_true
## 1     4 -1234567     2       1
## 2     4       10    10       0
## 3     7        4     4       0
## 4     7       22    22       0
## 5     8       16    16       0
## 6     9       10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So instead of obtaining a NA value, it would return 1 instead!&lt;/p&gt;
&lt;p&gt;And you may ask me - how do I change -123457 back into NA?&lt;/p&gt;
&lt;p&gt;You could simply reuse the same function with a FALSE flag. And presto the obscure value is converted back into NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = df_convertNAs_to_obscureNo(data, -1234567, F)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2       1
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warning!!!&lt;/h2&gt;
&lt;p&gt;Developer/ Data Scientist/ Data Analysis would need to be absolutely sure that this is what he/ she wants. If NA value is expected instead of 1 in the above use case, please do not use the function!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Loading excel data with correct variable types</title>
      <link>/post/load_data_with_correct_types/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/load_data_with_correct_types/</guid>
      <description>


&lt;div id=&#34;loading-data-with-data-types&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading data with data types&lt;/h3&gt;
&lt;p&gt;When reading static files into R or Python, most of the times we are lazy as we load the data with no regard to the data types.&lt;/p&gt;
&lt;p&gt;But in mission critical ETL jobs or Data analytics workflow, data types are quintessential and there’s a fine line between life and death. Ok, I’m exaggerating here.&lt;/p&gt;
&lt;p&gt;What I’ve written below is a swiss army knife function to read an excel file: 1st tab is data and 2nd tab is the variable types (e.g. database variable types mapped to R variable types)&lt;/p&gt;
&lt;p&gt;Note: If I read or write to database, I would have to modify my function below. Oh well.&lt;/p&gt;
&lt;p&gt;The steps in the function are pretty simple,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First I read in the data type&lt;/li&gt;
&lt;li&gt;Second I read in the column name of the dataset&lt;/li&gt;
&lt;li&gt;Third I left join the column names to its data type from database (or .sav or .dta or .sas files)&lt;/li&gt;
&lt;li&gt;Fourth I left join the (DB variable types) to (R data types) translation into the column names&lt;/li&gt;
&lt;li&gt;Lastly, I read in the dataset through read_excel functions with col_types as the parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here you go! Hope this is useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Reading excel data together with data types
#&amp;#39; @title Reading data with data types
#&amp;#39; @param spName_file Path of the data file
#&amp;#39; @param spTab_name_data Tab name of data
#&amp;#39; @param spTab_name_dataType Tab name of data type
#&amp;#39; @param sp_dataType_col_name Column name of dataType&amp;#39;s (variables column)
#&amp;#39; @param sp_dataType_datatype_name Column name of dataType&amp;#39;s (data type column)
#&amp;#39; @param dp_r_hana_type data frame of R to hana variable types conversion
#&amp;#39; @param sp_r_hana_type_DBType Column name of r_hana_type (DB_TYPE column)
#&amp;#39; @param sp_r_hana_type_RType Column name of r_hana_type (R_TYPE column)
#&amp;#39; @return A data frame with corresponding data types
#&amp;#39; @export
df_read_data_with_types = function(spName_file, spTab_name_data,
                                   spTab_name_dataType,
                                   sp_dataType_col_name, sp_dataType_datatype_name,
                                   dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType){

  #Read in data types
  data_type = df_read_tab(spName_file, spTab_name_dataType)

  #Read in just first row of dataset
  col_name = read_excel(spName_file, spTab_name_data, n_max = 1)
  col_name = data.frame(col_name = names(col_name)); col_name$col_name = as.character(col_name$col_name)
  col_name$column_order = 1:nrow(col_name)

  #Left join data type to name
  col_name = col_name %&amp;gt;%
    dplyr::left_join(data_type, by = c(&amp;quot;col_name&amp;quot; = sp_dataType_col_name))

  #Split this from above because of errors. weird
  col_name = base::merge(col_name, dp_r_hana_type,
                   by.x = sp_dataType_datatype_name, by.y = sp_r_hana_type_DBType,
                   all.x = T)

  col_name = arrange(col_name, column_order)

  #Read in full dataset with assignment of classes (look at readxl.tidyverse.org)
  data = readxl::read_excel(spName_file, spTab_name_data, col_types = col_name[, sp_r_hana_type_RType])

  return(data)
}

# spName_file = &amp;#39;./data/input/TB_OVSS_FNB_FACT.xlsx&amp;#39;
# spTab_name_data = &amp;#39;data&amp;#39;
# spTab_name_dataType = &amp;#39;data_type&amp;#39;
# sp_dataType_col_name = &amp;#39;COLUMN_NAME&amp;#39; 
# sp_dataType_datatype_name = &amp;#39;DATA_TYPE_NAME&amp;#39;
# dp_r_hana_type = r_hana_type 
# sp_r_hana_type_DBType = &amp;#39;DB_TYPE&amp;#39; 
# sp_r_hana_type_RType = &amp;#39;R_TYPE&amp;#39;
#
# a = df_read_data_with_types (spName_file, spTab_name_data,
#                          spTab_name_dataType,
#                          sp_dataType_col_name, sp_dataType_datatype_name,
#                          dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Function to describe clusters derived from unsupervised learning</title>
      <link>/post/cluster_descriptive_stats/</link>
      <pubDate>Fri, 24 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/cluster_descriptive_stats/</guid>
      <description>

&lt;h2 id=&#34;describing-unsupervised-learning-clusters&#34;&gt;Describing unsupervised learning clusters&lt;/h2&gt;

&lt;p&gt;As a data scientist / analyst, besides doing cool modelling stuff, we&amp;rsquo;re often asked to churn out descriptive statistics. Yes, we know. It&amp;rsquo;s part of the process.&lt;/p&gt;

&lt;p&gt;I chanced upon this really nifty concept at work to describe the clusters derived from unsupervised learnig. Here&amp;rsquo;s how it goes,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Say it&amp;rsquo;s a nominal or ordinal variable. First, I find the proportion of the feature across the X clusters&lt;/li&gt;
&lt;li&gt;Second, I rank this proportion through percentiles across these X values&lt;/li&gt;
&lt;li&gt;The cluster with the highest percentile will earn its right to be represented by the feature&lt;/li&gt;
&lt;li&gt;And if it&amp;rsquo;s a scale variable, you may find the mean of the feature for each cluster and repeat the steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is a nifty function to carry out the above steps. You can compile it into a package and may start using it in your data science work!&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  This function takes in a data-frame
#&#39; @title Finding feature importance associated with each cluster
#&#39; @param dp_data data frame for clustering
#&#39; @param sp_resp_key_name primary key in dataset
#&#39; @param sp_feature Feature name
#&#39; @param sp_cluster_name column name for cluster 
#&#39; @param sp_scale_categorical scale or categorical
#&#39; @param sp_IndivYr_aggregate IndivYr or aggregate
#&#39; @param sp_weight_name Column names for weights
#&#39; @param np_feature_imp_threshold threshold for percentile ranking
#&#39; @param sp_all_filter filter or no filter
#&#39; @return list of proportion, mean or percentile for 6 cluster importance
#&#39; @export
#&#39;

l_feature_importance_cluster = function(dp_data, sp_resp_key_name,
                                        sp_feature, sp_cluster_name, sp_scale_categorical,
                                        sp_IndivYr_aggregate = NULL, sp_weight_name,
                                        np_feature_imp_threshold = 1, sp_all_filter = &amp;quot;all&amp;quot;){

  tryCatch({
  if((sp_scale_categorical == &amp;quot;categorical&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
    #Tabulate

    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_weight_name, sp_cluster_name)) %&amp;gt;%                #select columns
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%   #Count by cluster and feature
        dplyr::summarize(counts = n(),
                         wt_counts = sum(!! rlang:: sym(sp_weight_name))) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%                               #Find proportion of feature in cluster group
        dplyr::mutate(counts_cluster = sum(counts, na.rm = T),
                      prop_feature_within_cluster = counts/counts_cluster,
                      counts_cluster_wt = sum(wt_counts, na.rm = T),
                      prop_feature_within_cluster_wt = wt_counts/counts_cluster_wt) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_feature)) %&amp;gt;%                                    #Find percentile of proportions
        dplyr::mutate(percentile_feature = percent_rank(prop_feature_within_cluster),
                      percentile_feature_wt = percent_rank(prop_feature_within_cluster_wt),
                      counts_feature = sum(counts, na.rm = T),
                      prop_cluster_within_features = counts/counts_feature) %&amp;gt;%            #Find proportion of cluster in feature
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::rename(feature_value = sp_feature)

    #Filter to keep only the &#39;meaningful&#39; features   

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
        dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)   
    }

  } else if((sp_scale_categorical == &amp;quot;scale&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
  
    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_cluster_name, sp_weight_name)) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%
        dplyr::summarize(avg_by_cluster = mean(!! rlang:: sym(sp_feature), na.rm = T),
                         avg_by_cluster_wt = weighted.mean(!! rlang:: sym(sp_feature), !! rlang:: sym(sp_weight_name), na.rm = T))  %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::mutate(percentile_feature = percent_rank(avg_by_cluster),
                      percentile_feature_wt = percent_rank(avg_by_cluster_wt))

     #Filter to keep only the &#39;meaningful&#39; features

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
                            dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)    
    } 
  }
  }, error = function(e){
    print(paste0(&amp;quot;Error with &amp;quot;, sp_feature))
  })
  
  return(dTabulated_data)

}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Playing with Google Place API</title>
      <link>/post/google_place_api/</link>
      <pubDate>Tue, 14 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/google_place_api/</guid>
      <description>

&lt;h2 id=&#34;google-place-api&#34;&gt;Google Place API&lt;/h2&gt;

&lt;p&gt;I was playing around with the API to obtain lat-long for my geo analytics work.&lt;/p&gt;

&lt;p&gt;I entered my credit card info but it seems that I&amp;rsquo;m not charged even with 9000+ API calls. Unsure if it&amp;rsquo;s because I&amp;rsquo;ve a 400+ dollars free cloud credit?&lt;/p&gt;

&lt;p&gt;Anyway, what I did here was to make API calls and storing the data into my local database.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested, you may visit this stackoverflow link (&lt;a href=&#34;https://stackoverflow.com/questions/52565472/get-map-not-passing-the-api-key-http-status-was-403-forbidden/52617929#52617929&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/52565472/get-map-not-passing-the-api-key-http-status-was-403-forbidden/52617929#52617929&lt;/a&gt;) to understand how to set up the credentials.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Load util function
source(&amp;quot;connect_db.R&amp;quot;)
vf_connect_db(&amp;quot;google_place.db&amp;quot;)

#load packages
sapply(c(&amp;quot;ggmap&amp;quot;, &amp;quot;RSQLite&amp;quot;), require, character.only = T)

#Google key information
register_google(key = &amp;quot;&amp;lt;intentionally left blank&amp;gt;&amp;quot;) 

#Load dataframe into the memory
query = paste0(&#39;select TripAdv_Key, address from &amp;quot;trip_advisor_full_dataset_without_lat_long&amp;quot;&#39;)
origAddress = dbGetQuery(con, query)

# Loop through the addresses to get the latitude and longitude of each address and add it to the
# origAddress data frame in new columns lat and lon
for(i in 1:nrow(origAddress))
{
  print(i)
  tryCatch({
    
    #Geocoding based on address
    result &amp;lt;- geocode(origAddress$address[i], output = &amp;quot;latlona&amp;quot;, source = &amp;quot;google&amp;quot;)
    
    #Extracting results values and inserting into database
    TripAdv_Key = origAddress$TripAdv_Key[i] 
    lat = as.numeric(result[2])
    long = as.numeric(result[1])
    add = as.character(result[3])
    values = paste0(&amp;quot;(&amp;quot;,TripAdv_Key, &amp;quot;,&amp;quot;, lat, &amp;quot;,&amp;quot;, long, &amp;quot;,&#39;&amp;quot;, add,&amp;quot;&#39;)&amp;quot;)
    query = paste0(&#39;INSERT INTO lat_long_info VALUES&#39;, values)
    dbSendQuery(con, query)
    
    #Print lat long
    print(c(lat, long))
  }, error=function(e){})
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on Reinforcement Learning - Q Learning</title>
      <link>/post/q_learning/</link>
      <pubDate>Mon, 08 Apr 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/q_learning/</guid>
      <description>

&lt;h2 id=&#34;q-learning&#34;&gt;Q learning&lt;/h2&gt;

&lt;p&gt;I just completed a Reinforcement Learning assignment - in particular on Q-learning. According to Wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Q-learning&#34;&gt;here&lt;/a&gt;, it&amp;rsquo;s a model-free Rl algorithm. The goal for the algo is to learn a policy, which tells an agent what action to take under different circumstances.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s my confession. What I&amp;rsquo;m doing in this post is to summarise what I&amp;rsquo;ve just learnt so that I may come back to this at any point in future. Hence it may or may not make sense to you,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RL is rather different from a conventional ML process. It contains an in-built iterative process to refresh the parameters&lt;/li&gt;
&lt;li&gt;User can train the algorithm till the &amp;lsquo;total reward&amp;rsquo; (in ML lingo that could be error-type measures) converges. I will coin each training process as a simulation&lt;/li&gt;
&lt;li&gt;To set up the learning process, I first initalize a Q learning table of 0s with dimension of number of states * actions (In python lingo, self.q = np.zeros((num_states, num_actions), dtype = np.float64))&lt;/li&gt;
&lt;li&gt;In every step of each simulation, the algo will pick a random float of 0 to 1. If it&amp;rsquo;s lesser than the threshold, it will pick a random action. Else, it will pick the action with the best outcome. According to the literature, it seems that exploration plays a role in improving the results&lt;/li&gt;
&lt;li&gt;From second step onward, the algo will update the Q-table as follows: self.q[self.s, self.a] = (1 - self.alpha) * self.q[self.s, self.a] + self.alpha * (r + self.gamma * np.max(self.q[s_prime,]))&lt;/li&gt;
&lt;li&gt;What it meant in the above formula is that Q-learning computes a weighted score for a particular state and action based on present and future discounted score from best action.&lt;/li&gt;
&lt;li&gt;The updated score will be used in subsequent simulations, and not current one. i.e in future iteration, if the option is non-random, it will pick the highest score option.&lt;/li&gt;
&lt;li&gt;When the current simulation end, the algorithm will return to the starting point and retrain the algorithm with the refreshed Q-table&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dyna-q&#34;&gt;Dyna-Q&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The additional bootstrap component algorithm is deemed to be cheaper because it doesn&amp;rsquo;t require additional interaction with the external environment to refresh the Q-table.&lt;/li&gt;
&lt;li&gt;It will instead pick random states and actions&lt;/li&gt;
&lt;li&gt;And select a new state based on a probability mass function in each loop (each state is assigned a discrete chance. Say there&amp;rsquo;re 100 states where 1 of the states has 2% chance and another has 1% chance. The latter might still be selected albeit with a lower chance)&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s different here is that it will refresh the reward information too&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;insights-from-this-exercise&#34;&gt;Insights from this exercise&lt;/h3&gt;

&lt;p&gt;The strength of this algorithm lies in the fact that it doesn&amp;rsquo;t require a ton of data. I&amp;rsquo;m excited to apply this algorithm if there&amp;rsquo;s a chance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hosting a Flask App on Heroku</title>
      <link>/post/hosting-a-flask-app-on-heroku/</link>
      <pubDate>Thu, 28 Feb 2019 23:34:32 +0800</pubDate>
      
      <guid>/post/hosting-a-flask-app-on-heroku/</guid>
      <description>&lt;p&gt;Following the steps here &amp;ndash;&amp;gt; &lt;a href=&#34;https://realpython.com/flask-by-example-part-1-project-setup/&#34; target=&#34;_blank&#34;&gt;https://realpython.com/flask-by-example-part-1-project-setup/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I managed to deploy my python flask app in Heroku.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)


@app.route(&#39;/&#39;)
def hello():
    return &amp;quot;Hello World!&amp;quot;


@app.route(&#39;/&amp;lt;name&amp;gt;&#39;)
def hello_name(name):
    return &amp;quot;Hello {}!&amp;quot;.format(name)

if __name__ == &#39;__main__&#39;:
    app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may visit the following link &amp;ndash;&amp;gt;&lt;a href=&#34;https://jirong-stage.herokuapp.com/&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/&lt;/a&gt; &amp;amp; add a suffix to it.&lt;/p&gt;

&lt;p&gt;Example &lt;a href=&#34;https://jirong-stage.herokuapp.com/jirong&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/jirong&lt;/a&gt; &amp;amp; this will return Hello jirong!&lt;/p&gt;

&lt;p&gt;Possibilites are immense! I can easily create APIs or host dashboard here. Pretty exciting to me:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling With Replacement Through First Principles</title>
      <link>/post/sampling-with-replacement-through-first-principles/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sampling-with-replacement-through-first-principles/</guid>
      <description>

&lt;h1 id=&#34;sampling-with-replacement&#34;&gt;Sampling with replacement&lt;/h1&gt;

&lt;p&gt;Hello! It&amp;rsquo;s me once again attempting to explain things from first principles - a term popularized by Elon Musk.&lt;/p&gt;

&lt;p&gt;I will use some psudeo code - on sampling with replacement for weights - to aid my explanation.&lt;/p&gt;

&lt;p&gt;Earlier in the week, I attempted to write a simple function from scratch but I gave up after realising that it will take me more than 15 mins! Difficulties lies in the multiple switch statements in defining the intervals. Haven&amp;rsquo;t figured that part out yet.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;re definitely packages out there that does sort of stuff but this forces me to understand the underlying theory from scratch.&lt;/p&gt;

&lt;p&gt;So here is the idea, I&amp;rsquo;ve a dataset with 4 individuals, tagged to respective weights that corresponds to the population. And I wish to do a bootstrap i.e. sampling with replacement to get a sample size of N = 100&lt;/p&gt;

&lt;p&gt;See Wikipedia page on advantages of Bootstrapping &amp;ndash;&amp;gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I will do. I will first line up the individuals and find the Probability Mass Function (PMF) for each individual accounting for its weight. Second, I will add up the PMF to obtain the Cumulative Density Function (cumulative proportion)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df
id weight PMF     CDF
1  2      20%    [0, 20]
2  3      30%    (20, 50]
3  3      30%    (50, 80]
4  2      20%    (80, 100]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, I will do a loop of 100 times. At the start of each loop I will obtain a float of between 0 to 1. If the value lies between a certain range, I will add that individual to the dataset. Given that num is random, it will lie between the various ranges without order, accouting for length of the interval.&lt;/p&gt;

&lt;p&gt;Note that sampling with replacement means there&amp;rsquo;s a chance that an individual may be represented more than once in the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (i in 1:100){

  num = randint(0, 1)
  
  if(num &amp;lt; 0.2){
    add_to_new_dataset(id = 1)
  }else if (num between 0.2 to 0.5){
    add_to_new_dataset(id = 2)
  }else if (num between 0.5 to 0.8){
    add_to_new_dataset(id = 3)
  }else{
    add_to_new_dataset(id = 3)
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course! In R, you should avoid an explicit loop at all costs. The solution is to embed it in a function and use a lapply function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bootstrap_weights = function(i){

num = randint(0, 1)

  if(num &amp;lt; 0.2){
    id = 1
  }else if (num between 0.2 to 0.5){
    id = 2 
  }else if (num between 0.5 to 0.8){
    id = 3
  }else{
    id = 4
  }
  
  data_row = data[id, ]
  return (data_row)
}

bootstrapped_data = rbind.fill(lapply(1: 100, bootstrap_weights))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope this is useful!&lt;/p&gt;

&lt;h2 id=&#34;latest-developments&#34;&gt;Latest developments&lt;/h2&gt;

&lt;p&gt;Courtesy of this post here &amp;ndash;&amp;gt; &lt;a href=&#34;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the simple solution to link a value to an interval,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getValue &amp;lt;- function(x, data) {
  tmp &amp;lt;- data %&amp;gt;%
    filter(CDF1 &amp;lt;= x, x &amp;lt;= CDF2)
  return(tmp$id)
}

# Using rand function to get a list of numbers
rand_numbers &amp;lt;- c(0.1, 0.173, 0.235)
sapply(rand_numbers, getValue, data = df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
