<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>programming on Jirong&#39;s sandbox</title>
    <link>jironghuang.github.io/categories/programming/</link>
    <description>Recent content in programming on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 06 Oct 2020 11:50:49 +0800</lastBuildDate>
    <atom:link href="jironghuang.github.io/categories/programming/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Developing a performance monitoring component in my fully automated algorithmic trading system</title>
      <link>jironghuang.github.io/post/designing-and-deploying-a-fully-automated-algorithmic-trading-system/</link>
      <pubDate>Tue, 06 Oct 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/designing-and-deploying-a-fully-automated-algorithmic-trading-system/</guid>
      <description>

&lt;h2 id=&#34;developing-a-performance-monitoring-system-for-my-algorithmic-trading-system&#34;&gt;Developing a performance monitoring system for my algorithmic trading system&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s one thing to backtest your signals and forecasts on historical data but it&amp;rsquo;s a completely different animal in terms of execution. I have written about this &lt;a href=&#34;https://medium.com/datadriveninvestor/designing-and-building-a-fully-automated-algorithmic-trading-portfolio-management-system-6945c6c87620/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another important component of setting up an algorithmic trading system is performance monitoring. Some important questions I had in mind while developing this component,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How do I measure my slippage? Commissions paid?&lt;/li&gt;
&lt;li&gt;What are my daily returns? And how do I measure performance of my strategy while accounting for inflow and outflow of cash into the account?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;how-do-i-measure-my-slippage&#34;&gt;How do I measure my slippage?&lt;/h3&gt;

&lt;p&gt;On this front, I use &lt;a href=&#34;https://ib-insync.readthedocs.io/_modules/ib_insync/flexreport.html&#34;&gt;FlexReport&lt;/a&gt;
function (using REST API) from ib_insync to query the trades made on that day. The function will query the XML report from Interactive Brokers which I subsequently format for daily reporting purposes. The queryId specification had to be set up via IB web account management system. Unfortunately this expires annually which means you have to find a way to remind yourself to refresh this queryId manually every year.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## Code for querying from IB
report = FlexReport(token, queryId) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some trivial code is then written to format the XML into this format below for my email notification,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/Slippage.png&#34; alt=&#34;/post/img/Slippage.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-are-my-daily-returns-how-do-i-measure-performance-of-my-strategy-while-accounting-for-inflow-and-outflow-of-cash-into-the-account&#34;&gt;What are my daily returns? How do I measure performance of my strategy while accounting for inflow and outflow of cash into the account?&lt;/h3&gt;

&lt;p&gt;In the past, I de-prioritised and placed development of performance monitoring at the bottom of my backlog as I could easily query sophiscated performance figures from IB. You may find some of the figures in my blog &lt;a href=&#34;https://jironghuang.github.io/portfolio/portfolio/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Increasingly, however, I found that it is important to understand the performance drivers of portfolio performance by instruments and strategies. To analyze at this granular level, I would have to capture the daily/ hourly levels and returns of instruments and strategies.&lt;/p&gt;

&lt;p&gt;To do that, the system triggers a request to take an accounting snapshot of the portfolio every hour,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/snapshot.png&#34; alt=&#34;/post/img/snapshot.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;But what if there is inflow and outflow of cash - you may ask? I simply rely on Time Weighted Return Formula.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TWR = [(1+HP1) X&amp;hellip;.(1+HPN)] - 1&lt;/li&gt;
&lt;li&gt;where TWR = Time weighted return&lt;/li&gt;
&lt;li&gt;n = Number of sub-periods&lt;/li&gt;
&lt;li&gt;HP = (End Value of Day- Initial Value of Day + Cash Flow)/ (Initial Value + Cashflow)&lt;/li&gt;
&lt;li&gt;HPN = Return for sub period n&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the portfolio level, the inflow and outflow would the net cash inflow into the account. At the instrument level, the inflow and outflow would be the net buy/sell trades on that day. You would also have to adjust the instrument asset value to &amp;lsquo;the equity value&amp;rsquo; by downsizing through the overall portfolio leverage factor.&lt;/p&gt;

&lt;p&gt;You may find the implementation of measuring time weighted returns at overall portfolio level in the appendix.&lt;/p&gt;

&lt;p&gt;After all that is mentioned and done, what could I do? I could plot some fancy equity curves as shown below,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/strategy1_equity_curve.png&#34; alt=&#34;/post/img/strategy1_equity_curve.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/strategy2_equity_curve.png&#34; alt=&#34;/post/img/strategy2_equity_curve.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;I could also measure realized rolling volatility and do crazy stuff (but unimportant in my opinion) like fama-french 3, 5, or gazillion factors analysis.&lt;/p&gt;

&lt;h3 id=&#34;what-is-in-my-backlog-for-further-enhancement-of-performance-monitoring&#34;&gt;What is in my backlog for further enhancement of performance monitoring?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Understanding performance drivers systematically with data captured i.e. break down overall portfolio returns into weighted performance drivers by strategies, instruments and time-frame.&lt;/li&gt;
&lt;li&gt;Pushing data files into a relational or non-relational database. But as much as possible, I will try to limit interaction between the trading system and databases during trading hours. In the past, I observed concurrency issues while using SQLite and database server &amp;lsquo;hanging&amp;rsquo; while using relational database such as Mysql. Another alternative would be to host the servers in the cloud. But that would mean I have to pay for extra costs and worry about connectivity issues. Sometimes plain old files could just work?&lt;/li&gt;
&lt;li&gt;Impact of exchange rate effect on portfolio performance. I wrote and created a repository on this in the past. You may visit the link &lt;a href=&#34;https://github.com/jironghuang/RemoveExchangeRateEffects&#34;&gt;here&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;appendix&#34;&gt;Appendix&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/accounting.png&#34; alt=&#34;/post/img/accounting.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Crawling data from basketball-reference and espn</title>
      <link>jironghuang.github.io/post/crawling-data-from-basektball-reference-and-espn/</link>
      <pubDate>Tue, 22 Sep 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/crawling-data-from-basektball-reference-and-espn/</guid>
      <description>

&lt;h2 id=&#34;crawling-data-from-basketball-reference-and-espn&#34;&gt;Crawling data from basketball-reference and espn&lt;/h2&gt;

&lt;p&gt;As I required some data for my ML assignment, I thought that predicting NBA player salaries based on previous seasons&amp;rsquo; game stats would be cool.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;re the codes used for crawling the data from basektball-reference and espn.&lt;/p&gt;

&lt;h3 id=&#34;codes&#34;&gt;Codes&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/crawl1.png&#34; alt=&#34;/post/img/crawl1.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/crawl2.png&#34; alt=&#34;/post/img/crawl2.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/crawl3.png&#34; alt=&#34;/post/img/crawl3.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Designing, Building and Deploying a Fully Automated Algorithmic Trading System</title>
      <link>jironghuang.github.io/post/developing-an-accounting-and-performance-monitoring-system-for-my-algorithmic-trading-system/</link>
      <pubDate>Mon, 24 Aug 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/developing-an-accounting-and-performance-monitoring-system-for-my-algorithmic-trading-system/</guid>
      <description>

&lt;h2 id=&#34;designing-building-and-deploying-a-fully-automated-algorithmic-trading-system&#34;&gt;Designing, Building and Deploying a Fully Automated Algorithmic Trading System&lt;/h2&gt;

&lt;p&gt;As I developed several inter-day trading/ portfolio management algorithms, I also embarked on a journey in parallel to develop a fully automated execution framework that could satisfy my requirements.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Previously orders were executed manually after signals are generated automatically.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;A relatively slow trading system triggered by an hourly task scheduler during trading hours. I’m using Linux cron jobs for this.&lt;/li&gt;
&lt;li&gt;A non event-driven framework. I’m relying on ib_sync framework for my algorithms executed in interactive brokers. The alternative is an event-driven framework (e.g. official IB framework based on asyncio) based on my experience is hard to debug and would be an overkill for my low to medium frequency trading/ portfolio management system.&lt;/li&gt;
&lt;li&gt;Automated trading execution and portfolio adjustments in relation to risks, signals and leverage cap.&lt;/li&gt;
&lt;li&gt;Data ingestion from multiple data sources/ APIs for signal processing, trade executions and logs.&lt;/li&gt;
&lt;li&gt;Inclusion of safeguards and circuit breakers in place before firing any trades.&lt;/li&gt;
&lt;li&gt;Notification through email for errors and daily updates @ start, middle and end of trading day.
-Able to integrate the above steps through Python.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;steps-taken&#34;&gt;Steps taken&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Without diving into the details of the code, here is the the general flow of my framework. The ‘main’ programs and execution flowcharts are appended at the end of the article.&lt;/li&gt;
&lt;li&gt;Connect to my brokerage account as a new session.
Cancel any open orders. I adopt this option instead of ‘modifying orders’ as it’s much easier. If I were to modify orders, I would have to keep track of the number of filled orders and unfilled orders. As my algorithm is triggered hourly, any unfilled orders in that hourly slot are deemed to deviate significantly from the current price and the chance of being executed at the limit price is pretty low. That being said, in my next iteration, I would prefer to invest time in writing code to ‘modify orders’ instead of a simplistic ‘cancel orders’.&lt;/li&gt;
&lt;li&gt;Generate forecast i.e. Required leverage and weightage from different tickers. I currently write them as local files instead of committing them to a database or storing them as variables as they are more robust during live trading. I do not have to worry about concurrency issues (else codes have to consider mutex and locks), database downtime.&lt;/li&gt;
&lt;li&gt;Obtain net liquidation value of account.&lt;/li&gt;
&lt;li&gt;Read in ticker weights and leverage from file written out in step 3.&lt;/li&gt;
&lt;li&gt;Create order files for different tickers including actions such as ‘Buy’, ‘Sell’ or ‘Do nothing’, Quantity and Limit price. At the moment, I’m using Adaptive Limit 7. Order from Interactive Brokers — which based on my understanding is a discretionary order with limit price as an upper bound. At the moment, I’m unwilling to invest more time on this to gain any further execution alpha as it could be a non-trivial task to dig into the bid-ask order books.&lt;/li&gt;
&lt;li&gt;Check if there are any errors from step 1 to 4 through an error flag. If there is any error, orders will not be sent. If everything cleared, the program will send the orders to the exchange.&lt;/li&gt;
&lt;li&gt;Trades, if any are recorded into a csv file.&lt;/li&gt;
&lt;li&gt;Save current snapshot which includes current positions, trades, net liquidation value and leverage utilized and margins.&lt;/li&gt;
&lt;li&gt;If errors are encountered, an alert will be sent to my gmail.&lt;/li&gt;
&lt;li&gt;Daily updates/ snapshots are sent to my gmail at the start, middle and end of session.&lt;/li&gt;
&lt;li&gt;If the program is triggered at 5am SGT (after US trading hours), the snapshots and trades information are committed to my database.&lt;/li&gt;
&lt;li&gt;Disconnect from current session.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;technology-stack&#34;&gt;Technology stack&lt;/h3&gt;

&lt;p&gt;If you are keen to know my technology stack, I’m currently using the following,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linux Mint&lt;/li&gt;
&lt;li&gt;Mint Box, 8GB RAM (but any laptop or virtual private server would suffice)&lt;/li&gt;
&lt;li&gt;Interactive brokers brokerage account (but the framework could be adapted to any brokerages with APIs)&lt;/li&gt;
&lt;li&gt;Interactive brokers gateway (IB Trade Worker Station is an alternative but I do not require any graphical user interface for my purpose)&lt;/li&gt;
&lt;li&gt;ib_insync framework&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;word-of-caution&#34;&gt;Word of Caution&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;This framework is not ideal for tick by tick or even bar by bar (seconds or minutes) trading algorithm since it’s not predicated on speed. An event-driven framework would be a better alternative if speed is required.&lt;/li&gt;
&lt;li&gt;I’m relying on brokerage server side to keep track of the current positions, margins, leverage and orders. In the next stage of development beyond this Minimum Viable Product, I’m keen to develop my own accounting system.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;appendix&#34;&gt;Appendix&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/prog1.png&#34; alt=&#34;/post/img/prog1.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/prog2.png&#34; alt=&#34;/post/img/prog2.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/flow1.jpeg&#34; alt=&#34;/post/img/flow1.jpeg&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Could volatility targeting increase risk-adjusted returns for quantitative strategies</title>
      <link>jironghuang.github.io/post/volatility-targeting-could-potentially-increase-risk-adjusted-returns-for-any-quant-strategies/</link>
      <pubDate>Wed, 29 Jul 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/volatility-targeting-could-potentially-increase-risk-adjusted-returns-for-any-quant-strategies/</guid>
      <description>

&lt;h2 id=&#34;volatility-targeting-could-potentially-increase-risk-adjusted-returns-for-quantitative-strategies&#34;&gt;Volatility targeting could potentially increase risk-adjusted returns for quantitative strategies&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Note: Man AHL&amp;rsquo;s framework in this &lt;a href=&#34;https://www.man.com/maninstitute/volatility-is-back-better-to-target-returns-or-target-risk&#34;&gt;paper&lt;/a&gt; is used in the write-up here.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you researched and managed to find a quantitative strategy that suits your risk-reward preferences. How do you further improve your strategy while managing your risk?&lt;/p&gt;

&lt;p&gt;A common way in the trend-following and risk parity space is to target risk - also known as volatility targeting.&lt;/p&gt;

&lt;p&gt;In this article, I will present some simulations to bring you through my thought process on how I apply volatility targeting to my strategies.&lt;/p&gt;

&lt;p&gt;My simulations in this article showed that by applying volatility targeting, I could potentially increase my risk-adjusted returns by 10% (risk adjusted metric sharpe ratio increase from 1.62 to 1.79).&lt;/p&gt;

&lt;h3 id=&#34;what-is-volatility-targeting&#34;&gt;What is volatility targeting?&lt;/h3&gt;

&lt;p&gt;Volatility targeted funds are usually pegged to annualized volatility/ standard deviation. To provide some context, I will provide some intuition on how annualized standard deviation is derived.&lt;/p&gt;

&lt;p&gt;Assuming variance of daily portfolio returns are independent (ignore serial correlation of returns), you could sum up variance of returns across days as follows,&lt;/p&gt;

&lt;p&gt;Variance of portfolio returns across a year (252 trading days) = Var(ret1): Variance of return on day 1 + &amp;hellip;. + Var(ret252): Variance of return on day 252.&lt;/p&gt;

&lt;p&gt;If the variance of daily returns are the same, you could multiply variance of daily returns by 252 to obtain annualized variance of portfolio returns, Annual Variance = 252 * Var(ret)&lt;/p&gt;

&lt;p&gt;To normalize it to annual standard deviation, you&amp;rsquo;ve to take a square root of Annual Variance.&lt;/p&gt;

&lt;p&gt;How do you interpret this metric? Let&amp;rsquo;s say a strategy has an expected returns of 8% with annual standard deviation of 10%. Based on naive normality assumptions, there&amp;rsquo;s a 68% chance that the fund&amp;rsquo;s annualized return will be between -2% to 18%; and 95% chance that the fund&amp;rsquo;s returns will be between -12% to 28%.&lt;/p&gt;

&lt;p&gt;There are couple of ways for fund managers to derive the volatilities,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;One way is to simply find the standard deviation of returns over last X days and annualize it.&lt;/li&gt;
&lt;li&gt;A second way is to find the exponentially weighted standard deviation. See here (&lt;a href=&#34;https://financetrain.com/calculate-historical-volatility-using-ewma/&#34; target=&#34;_blank&#34;&gt;https://financetrain.com/calculate-historical-volatility-using-ewma/&lt;/a&gt;) for further explanation. In my simulations below, I&amp;rsquo;m using this variation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;are-we-leaving-chips-on-the-table-can-we-afford-more-risk-when-should-we-take-chips-off-the-table&#34;&gt;Are we leaving chips on the table? Can we afford more risk? When should we take chips off the table?&lt;/h3&gt;

&lt;p&gt;Below, I simulate a non volatility-targeted risk-parity portfolio rebalanced monthly i.e. portfolio is allocated across different asset classes based on risk rather than dollar value. Within asset classes, tickers are also risk-weighted. See here (&lt;a href=&#34;https://en.wikipedia.org/wiki/Risk_parity&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Risk_parity&lt;/a&gt;) for further explanation.&lt;/p&gt;

&lt;p&gt;Rolling standard deviation of raw portfolio is plotted below to illustrate the portfolio risk across time. From the diagram below, you would notice that there are periods where the portfolio risk is significantly below the 10% volatility target. Are there chips left on the table? Could you afford more risk?&lt;/p&gt;

&lt;p&gt;There are periods where portfolio risk is significantly above the 10% volatility target. Case in point during Covid sell-off? Should you be reducing risk instead?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/volatility curve.png&#34; alt=&#34;/post/img/volatility curve.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Equity curves and portfolio statistics are included below for benchmarking purposes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/raw_portfolio.png&#34; alt=&#34;/post/img/raw_portfolio.png&#34;&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Portfolio&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Annualized returns&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Volatility&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sharpe&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max Drawdown&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Return/Max Drawdown&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Skewness&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Benchmark&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.97%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-7.96%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;what-if-we-target-risk-i-e-undertake-more-risk-during-calm-periods-and-less-risk-during-turbulent-periods&#34;&gt;What if we target risk? i.e. undertake more risk during calm periods and less risk during turbulent periods?&lt;/h3&gt;

&lt;p&gt;So does taking more risk during calm periods and reducing risk during turbulent periods pay off?&lt;/p&gt;

&lt;p&gt;Before, we proceed further, here are some assumptions I included in the simulations,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Portfolio is rebalanced monthly at the start of the trading day.&lt;/li&gt;
&lt;li&gt;On a daily basis, the algorithm checks for the current standard deviation of portfolio based on lookback period of 36 days. If it&amp;rsquo;s lesser than 10%, it will leverage proportionately (to a cap of 1.6 times). If it&amp;rsquo;s more than 10%, leverage will be reduced.&lt;/li&gt;
&lt;li&gt;Leverage financing costs and commission fees are included.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the equity curve and portfolio statistics below, the answer to the question posed earlier in the section is a resounding YES! It&amp;rsquo;s risk-reducing and returns enhancing to target volatility. Risk adjusted returns (Sharpe ratio) increased by 10% (1.62 to 1.79) and Return/ Max Drawdown is considerably higher. You would also notice that the negative skewness of the strategy improved considerably from -1.14 to -0.8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/curve_lev.png&#34; alt=&#34;/post/img/curve_lev.png&#34;&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Portfolio&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Annualized returns&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Volatility&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sharpe&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max Drawdown&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Return/Max Drawdown&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Skewness&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Benchmark&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.97%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-7.96%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Vol target (10%, max leverage of 1.6 times)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;13.92%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.4%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-8.02%&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;risk-reward-preferences&#34;&gt;Risk-reward preferences&lt;/h3&gt;

&lt;p&gt;Above example is all but a single scenario. You could carry out a grid search to find the appropriate volatility target and leverage possibility space.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve included the possibility space with respect to annualized returns, sharpe ratio, max drawdown to understand the risk-reward preferences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/Returns.png&#34; alt=&#34;/post/img/Returns.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/sharpe.png&#34; alt=&#34;/post/img/sharpe.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/drawdown.png&#34; alt=&#34;/post/img/drawdown.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/skewness.png&#34; alt=&#34;/post/img/skewness.png&#34;&gt;
&lt;img src=&#34;jironghuang.github.io/post/img/kurtosis.png&#34; alt=&#34;/post/img/kurtosis.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;caveat&#34;&gt;Caveat&lt;/h3&gt;

&lt;p&gt;Note: Above portfolio is currently in incubation prior to deployment.&lt;/p&gt;

&lt;h3 id=&#34;full-tear-sheet-of-10-volatility-target-and-max-leverage-of-1-6-included-for-reference&#34;&gt;Full tear sheet of 10% volatility target and Max leverage of 1.6 included for reference&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/tear_sheet.png&#34; alt=&#34;/post/img/tear_sheet.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrating volatility targeting into Jarvis, my expert advisor</title>
      <link>jironghuang.github.io/post/volatility-targeting/</link>
      <pubDate>Sun, 15 Mar 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/volatility-targeting/</guid>
      <description>

&lt;h2 id=&#34;volatility-targeting&#34;&gt;Volatility targeting&lt;/h2&gt;

&lt;p&gt;Currently, I&amp;rsquo;ve a suite of toolkits integrated into my Jarvis that advises me on the investing decisions that I&amp;rsquo;ve to make on a daily basis.&lt;/p&gt;

&lt;p&gt;On the latest feature I cobbled together on a Saturday evening, 2 weeks ago, I&amp;rsquo;ve decided to measure the volatility of my portfolio formally.&lt;/p&gt;

&lt;p&gt;Why I&amp;rsquo;m doing this is because managing risks in the form of volatility is easier than targeting returns.&lt;/p&gt;

&lt;p&gt;On any given day, it&amp;rsquo;s easier to predict volatility than returns itself because of its persistent nature.&lt;/p&gt;

&lt;p&gt;Think of it as a coin flips with Binomal distribution: B ~ (n, p).&lt;/p&gt;

&lt;p&gt;P is the probability for which you expect a positive expected payout.&lt;/p&gt;

&lt;p&gt;And variance of this win-lose distribution is n * p * (1-p). This variance is easier to &amp;lsquo;predict&amp;rsquo; based on your win rates.&lt;/p&gt;

&lt;p&gt;But market is not simply a binomial distribution, it also needs to take into consideration the portfolio size exposed to market risks and sequence of returns (autocorrelation here) at any given day.&lt;/p&gt;

&lt;p&gt;Current distribution of my portfolio win rate is around 65 to 70%. But because of some negative skew in returns, my portfolio is languishing at status quo (0%) since start of the year.&lt;/p&gt;

&lt;p&gt;Though my portfolio have outperformed the indexes by a factor of 2 to 3, the steep drawdown during March period of my portfolio could have been prevented if I&amp;rsquo;ve put a hedge early on (if I&amp;rsquo;ve done it through a systematic quant way) to maintain a fixed volatility target.&lt;/p&gt;

&lt;p&gt;Hence, I decided to integrate the volatility targeting feature into Jarvis that advises me daily on the amount of index hedge to place to maintain my portfolio at a constant volatility target; not a perfect way to maintain volatility target (since my portfolio is not perfectly correlated to index) but I find it cumbersome and potentially costly to sell off multiple counters on a daily basis. I used a global etf as a proxy to maintain my risk level since my portfolio has a global tilt.&lt;/p&gt;

&lt;p&gt;Here is the volatility of my levered portfolio. Notice how it spiked up 5 times in the month of March this year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/vol_target.png&#34; alt=&#34;/post/img/vol_target.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;What volatility targeting does is to supposedly to turn the volatility curve into a straight line.&lt;/p&gt;

&lt;p&gt;You may find the code below. I won&amp;rsquo;t delve into the details but this is essentially what it does,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Obtain my current positions from googlesheet&lt;/li&gt;
&lt;li&gt;Compute my portfolio value converted into SGD now and historically based on current positions&lt;/li&gt;
&lt;li&gt;Compute annual standard deviation (normal and exponential version shared by Robert Carver) of my portfolio based on past 36 days daily returns. Annual non exponential standard deviation returns of portfolio = Daily standard deviation of returns of portfolio * sqrt(252)&lt;/li&gt;
&lt;li&gt;Compute benchmark (VT) volatility&lt;/li&gt;
&lt;li&gt;Compute &amp;lsquo;imperfect&amp;rsquo; hedge required to achieve volatility target of 15%&lt;/li&gt;
&lt;li&gt;Pushes a notification to me on how much additional/ lesser hedge is required&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#Note: Volatility targeting

#Initialization
sapply(c(&amp;quot;ggplot2&amp;quot;, &amp;quot;plotly&amp;quot;, &amp;quot;quantmod&amp;quot;, &amp;quot;pushoverr&amp;quot;, &amp;quot;zoo&amp;quot;, &amp;quot;dplyr&amp;quot;, &amp;quot;roll&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;pushoverr&amp;quot;, &amp;quot;googlesheets&amp;quot;, &amp;quot;pracma&amp;quot;, &amp;quot;timeDate&amp;quot;, &amp;quot;riingo&amp;quot;), require, character.only = T)
source(&#39;util/extract_stock_prices.R&#39;)

rsd_file = Sys.getenv(&amp;quot;GOOGLESHEET&amp;quot;)
gs_auth(token = rsd_file)
suppressMessages(gs_auth(token = rsd_file, verbose = FALSE))

dat = gs_title(&amp;quot;Investment&amp;quot;)
gs_ws_ls(dat)   #tab names
data &amp;lt;- gs_read(ss=dat, ws = &amp;quot;debt_to_equity&amp;quot;, skip=0)
leverage = 1 + data$Ratio[3]
# leverage = 1
current_hedge_value = data$Ratio[nrow(data)]

currency = &amp;quot;SGD=X&amp;quot;
last_date = Sys.Date() - 200
benchmark = &amp;quot;VT&amp;quot;
num_counters = 24

##########################Obtain portfolio info##########################
# df_tickers = data.frame(tickers = c(&amp;quot;TLT&amp;quot;, &amp;quot;IEF&amp;quot;, &amp;quot;SPY&amp;quot;, ))
df_tickers &amp;lt;- gs_read(ss=dat, ws = &amp;quot;investment_live&amp;quot;, skip=0)
# df_tickers = subset(df_tickers, df_tickers$Ticker != &amp;quot;CNYB.AS&amp;quot;)
df_tickers = filter(df_tickers, num_units &amp;gt; 0)

current_value = sum(df_tickers$value)

df_tickers = df_tickers[, 1:5]
df_tickers[which(df_tickers$exch_rate_type == &amp;quot;SGDHKD=X&amp;quot;), 5] = &amp;quot;HKDSGD=X&amp;quot;

##########################Format data function##########################
#Create a time series of data, then merge in. Just filter out weekend only
data_format = function(ticker, num_units, currency, last_date){

  #Create date range
  create_date = function(i){
    return(last_date + i)
  }
  
  df = data.frame(Date = sapply(0:(Sys.Date() - last_date), create_date))
  df$Date = as.Date(df$Date, origin = &amp;quot;1970-01-1&amp;quot;)
  df$isWeekend = isWeekend(df$Date)
  df$Date = as.character(df$Date)
  
  #Read data
  data_temp = df_crawl_time_series(ticker, &amp;quot;1970-07-01&amp;quot;, &amp;quot;2030-12-30&amp;quot;)
  data_temp = subset(data_temp, data_temp$Date &amp;gt;= last_date &amp;amp; data_temp$Date &amp;lt;= Sys.Date())
  data = merge(df, data_temp, by = c(&amp;quot;Date&amp;quot;), all.x = T)

  #Fill NAs
  data = subset(data, data$isWeekend == F)
  
  if(is.na(data$Adj.Close[1])){
    data$Adj.Close[1] = data$Adj.Close[2]
  }

  if(is.na(data$Adj.Close[1])){
    data$Adj.Close[1] = data$Adj.Close[3]
  }
  
  if(is.na(data$Adj.Close[1])){
    data$Adj.Close[1] = data$Adj.Close[4]
  }
  
    
  data$Adj.Close = na.locf(data$Adj.Close)
  
  #Subset out data-frame
  df = data.frame(Date = data$Date, Price = data$Adj.Close, stringsAsFactors = F)
  
  #Read in num_units
  df$num_units = num_units
  
  #Subset date range
  df = subset(df, df$Date &amp;gt;= last_date &amp;amp; df$Date &amp;lt;= Sys.Date())
  
  #Read in currency
  currency = df_crawl_time_series(currency, &amp;quot;1970-07-01&amp;quot;, &amp;quot;2030-12-30&amp;quot;)
  currency$Adj.Close = na.locf(currency$Adj.Close)
  currency = subset(currency, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Adj.Close&amp;quot;))
  
  #Merge in currency
  df = df %&amp;gt;%
    left_join(., currency, by = c(&amp;quot;Date&amp;quot;))
  
  #Convert to local currency
  df$local_unit_value = df$Price * df$Adj.Close
  df$local_value = df$local_unit_value * df$num_units
  
  #Return date, portfolio value     
  df$Date = as.Date(df$Date)
  
  #ticker
  df$ticker = ticker
  
  df_sub = subset(df, df$Date &amp;gt;= last_date &amp;amp; df$Date &amp;lt;= Sys.Date())
    
  return(df_sub)    
}

########################Formatting portfolio function####################
portfolio_format = function(df_tickers, last_date){

i = 1
ticker_agg = data_format(df_tickers$Ticker[i], df_tickers$num_units[i], df_tickers$exch_rate_type[i], last_date)  

for(i in 2:nrow(df_tickers)){
  print(i)
  ticker_ind = data_format(df_tickers$Ticker[i], df_tickers$num_units[i], df_tickers$exch_rate_type[i], last_date)  
  ticker_agg = rbind(ticker_agg, ticker_ind)
}

portfolio = ticker_agg %&amp;gt;%
  group_by(Date) %&amp;gt;%
  summarize(portfolio_value = sum(local_value, na.rm = T),
            num_counters = n()
  )

portfolio$portfolio_value_adj = ifelse(portfolio$num_counters &amp;lt; num_counters, NA, portfolio$portfolio_value)

portfolio$upper_sd = mean(portfolio$portfolio_value_adj, na.rm = T) + 0.4 * sd(portfolio$portfolio_value_adj, na.rm = T)
portfolio$lower_sd = mean(portfolio$portfolio_value_adj, na.rm = T) - 0.4 * sd(portfolio$portfolio_value_adj, na.rm = T)
portfolio$portfolio_value_adj2 = ifelse((portfolio$portfolio_value_adj  &amp;gt; portfolio$upper_sd) | (portfolio$portfolio_value_adj  &amp;lt; portfolio$lower_sd),
                                        NA,
                                        portfolio$portfolio_value_adj)
portfolio$portfolio_value_adj2 = na.locf(portfolio$portfolio_value_adj2)

if(!is.na(current_value)){
  portfolio$portfolio_value_adj2[nrow(portfolio)] = current_value
}

portfolio$returns = ROC(portfolio$portfolio_value_adj2) * leverage
portfolio$returns_100 = portfolio$returns * 100

portfolio$roll_std = roll_sd(portfolio$returns, 36)
portfolio$roll_std_annual = portfolio$roll_std * (252 ^ 0.5)

df = portfolio

return(df)

}

##########################Find beta of portfolio######################################
#Find out how much of local_unit_value needed
find_beta = function(df, benchmark){
  
  price_bench = data_format(benchmark, num_units = 10, currency, last_date)
  price_bench$returns = ROC(price_bench$local_value)
  price_bench = subset(price_bench, select = c(&amp;quot;Date&amp;quot;, &amp;quot;returns&amp;quot;, &amp;quot;local_unit_value&amp;quot;))    
  names(price_bench)[2] = &amp;quot;returns_benchmark&amp;quot;

  df_agg = df %&amp;gt;%
    left_join(., price_bench, by = &amp;quot;Date&amp;quot;)
  
  reg = lm(df_agg$returns ~ df_agg$returns_benchmark)
  return(as.numeric(reg$coefficients[2]))
  
}

##########################Pure vol targeting######################################
#Find out how much of local_unit_value needed
#Account for leveraged %

find_vol_units = function(df, benchmark, leverage){
  
  price_bench = data_format(benchmark, num_units=10, currency, last_date)
  price_bench$returns = ROC(price_bench$local_value)
  price_bench = subset(price_bench, select = c(&amp;quot;Date&amp;quot;, &amp;quot;returns&amp;quot;, &amp;quot;local_unit_value&amp;quot;))    
  names(price_bench)[2] = &amp;quot;returns_benchmark&amp;quot;

  df_agg = df %&amp;gt;%
              left_join(., price_bench, by = &amp;quot;Date&amp;quot;)
  
  #Compute EMA of VT price. Loop number of units. And iteratively compute new portfolio value and EMA of SD. Find out closest number of VT units required.
  df_agg$square_returns_target = df_agg$returns_benchmark ^ 2
  df_agg$square_returns_target[1] = df_agg$square_returns_target[2]
  df_agg$ema_vol_target =  movavg(df_agg$square_returns_target, 36, type = &amp;quot;e&amp;quot;)
  df_agg$ema_sd_target = df_agg$ema_vol_target ^ 0.5 * (252 ^ 0.5)
  
  df_agg$square_returns = (df_agg$returns) ^ 2
  df_agg$square_returns[1] = df_agg$square_returns[2]
  df_agg$ema_vol =  movavg(df_agg$square_returns, 36, type = &amp;quot;e&amp;quot;)
  df_agg$ema_sd = df_agg$ema_vol ^ 0.5 * (252 ^ 0.5)
  
  
  return(df_agg)
}

##########################Generate df######################################
df = portfolio_format(df_tickers, last_date)
beta = find_beta(df, benchmark)
df = find_vol_units(df, benchmark, leverage)
reduce_times_exp = df$ema_sd[nrow(df) - 0]/ 0.15

##########################Find hedge required to target risk######################################
hedge = (df$portfolio_value_adj2[nrow(df)] - (df$portfolio_value_adj2[nrow(df)]/ reduce_times_exp)) * (df$ema_sd[nrow(df)] / df$ema_sd_target[nrow(df)])
hedge = round(hedge, 0)

##########################Pushing notifications to inform how much hedge is required######################################
msg = paste0(&amp;quot;You should hedge &amp;quot;, hedge, 
             &amp;quot; Current hedge value is &amp;quot;, current_hedge_value,
             &amp;quot; Additional hedge required &amp;quot;, (hedge - current_hedge_value),
             &amp;quot; Portfolio vol is &amp;quot;, round(df$ema_sd[nrow(df)], 3),
             &amp;quot; Benchmark vol is &amp;quot;, round(df$ema_sd_target[nrow(df)], 3)
)

print(msg)
plot(df$ema_sd)

if(abs(hedge - current_hedge_value) &amp;gt; 3000){
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;))
  
}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Mapreduce using Java</title>
      <link>jironghuang.github.io/post/mapreduce-in-java/</link>
      <pubDate>Sun, 15 Mar 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/mapreduce-in-java/</guid>
      <description>

&lt;h2 id=&#34;mapreduce-using-java&#34;&gt;Mapreduce using java&lt;/h2&gt;

&lt;p&gt;I haven&amp;rsquo;t coded in java in eons. The assignment (Mapreduce, Pig and Spark) I worked on over last 3 weeks is a good way to jolt me out from my comfort zone.&lt;/p&gt;

&lt;p&gt;Java is something I need to brush up on before taking the Software Development Process module which requires me to write an android app. Argh!&lt;/p&gt;

&lt;p&gt;Back to Mapreduce. It&amp;rsquo;s a useful framework if you&amp;rsquo;ve to summarise huge datasets (gigabytes, terabytes). Here are some of the common steps in mapreduce,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In Mapper method, each entity of data (e.g. row, word) is tokenized and assigned values&lt;/li&gt;
&lt;li&gt;In Reduce method, data is aggregated (e.g sum, average, etc.)&lt;/li&gt;
&lt;li&gt;In Main function, mapreduce jobs are managed through certain parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Mapreduce data types are different from Java data types! It could be confusing at times. But if you get used to it, it will be easier.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MapReuceJobs {

public static class TokenizerMapper
extends Mapper&amp;lt;LongWritable, Text, Text, Text&amp;gt;
{
&amp;lt;Deliberately left blank&amp;gt;
...Each row of data is tokenize and assign a number
}

//Reducer
public static class IntSumReducer
extends Reducer&amp;lt;Text,Text,Text,Text&amp;gt; {

&amp;lt;Deliberately left blank&amp;gt;
...Aggregated data is collated here
}



//IntSumReducer  
public static void main(String[] args) throws Exception {

&amp;lt;Deliberately left blank&amp;gt;
...Jobs are managed here
}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Embedding D3 interactive charts part 2</title>
      <link>jironghuang.github.io/post/embedding-d3-interactive-charts-part2/</link>
      <pubDate>Sun, 23 Feb 2020 11:50:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/embedding-d3-interactive-charts-part2/</guid>
      <description>

&lt;h2 id=&#34;embedding-d3-interactive-charts-part-2-testing-reading-in-of-file-from-directory&#34;&gt;Embedding D3 interactive charts Part 2 - Testing reading in of file from directory&lt;/h2&gt;

&lt;p&gt;Just having fun - testing to see if I could embed D3 charts in my blog.&lt;/p&gt;

&lt;p&gt;Seems like it works too! But I would have to upload the csv under public folder first.&lt;/p&gt;

&lt;p&gt;&lt;meta charset=&#34;utf-8&#34;&gt;&lt;/p&gt;

&lt;!-- Load d3.js --&gt;

&lt;script src=&#34;https://d3js.org/d3.v4.js&#34;&gt;&lt;/script&gt;

&lt;!-- Create a div where the graph will take place --&gt;

&lt;div id=&#34;my_dataviz&#34;&gt;&lt;/div&gt;

&lt;script&gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&#34;#my_dataviz&#34;)
  .append(&#34;svg&#34;)
    .attr(&#34;width&#34;, width + margin.left + margin.right)
    .attr(&#34;height&#34;, height + margin.top + margin.bottom)
  .append(&#34;g&#34;)
    .attr(&#34;transform&#34;,
          &#34;translate(&#34; + margin.left + &#34;,&#34; + margin.top + &#34;)&#34;);

//Read the data
d3.csv(&#34;/post/data/2_TwoNum.csv&#34;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&#34;g&#34;)
    .attr(&#34;transform&#34;, &#34;translate(0,&#34; + height + &#34;)&#34;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&#34;g&#34;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&#34;#my_dataviz&#34;)
    .append(&#34;div&#34;)
    .style(&#34;opacity&#34;, 0)
    .attr(&#34;class&#34;, &#34;tooltip&#34;)
    .style(&#34;background-color&#34;, &#34;white&#34;)
    .style(&#34;border&#34;, &#34;solid&#34;)
    .style(&#34;border-width&#34;, &#34;1px&#34;)
    .style(&#34;border-radius&#34;, &#34;5px&#34;)
    .style(&#34;padding&#34;, &#34;10px&#34;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&#34;opacity&#34;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&#34;The exact value of&lt;br&gt;the Ground Living area is: &#34; + d.GrLivArea)
      .style(&#34;left&#34;, (d3.mouse(this)[0]+90) + &#34;px&#34;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&#34;top&#34;, (d3.mouse(this)[1]) + &#34;px&#34;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&#34;opacity&#34;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&#34;dot&#34;)
    .data(data.filter(function(d,i){return i&lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&#34;circle&#34;)
      .attr(&#34;cx&#34;, function (d) { return x(d.GrLivArea); } )
      .attr(&#34;cy&#34;, function (d) { return y(d.SalePrice); } )
      .attr(&#34;r&#34;, 7)
      .style(&#34;fill&#34;, &#34;#69b3a2&#34;)
      .style(&#34;opacity&#34;, 0.3)
      .style(&#34;stroke&#34;, &#34;white&#34;)
    .on(&#34;mouseover&#34;, mouseover )
    .on(&#34;mousemove&#34;, mousemove )
    .on(&#34;mouseleave&#34;, mouseleave )

})

&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;

&amp;lt;!-- Load d3.js --&amp;gt;
&amp;lt;script src=&amp;quot;https://d3js.org/d3.v4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;!-- Create a div where the graph will take place --&amp;gt;
&amp;lt;div id=&amp;quot;my_dataviz&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;

&amp;lt;script&amp;gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&amp;quot;#my_dataviz&amp;quot;)
  .append(&amp;quot;svg&amp;quot;)
    .attr(&amp;quot;width&amp;quot;, width + margin.left + margin.right)
    .attr(&amp;quot;height&amp;quot;, height + margin.top + margin.bottom)
  .append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;,
          &amp;quot;translate(&amp;quot; + margin.left + &amp;quot;,&amp;quot; + margin.top + &amp;quot;)&amp;quot;);

//Read the data
d3.csv(&amp;quot;/post/data/2_TwoNum.csv&amp;quot;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;, &amp;quot;translate(0,&amp;quot; + height + &amp;quot;)&amp;quot;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&amp;quot;g&amp;quot;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&amp;quot;#my_dataviz&amp;quot;)
    .append(&amp;quot;div&amp;quot;)
    .style(&amp;quot;opacity&amp;quot;, 0)
    .attr(&amp;quot;class&amp;quot;, &amp;quot;tooltip&amp;quot;)
    .style(&amp;quot;background-color&amp;quot;, &amp;quot;white&amp;quot;)
    .style(&amp;quot;border&amp;quot;, &amp;quot;solid&amp;quot;)
    .style(&amp;quot;border-width&amp;quot;, &amp;quot;1px&amp;quot;)
    .style(&amp;quot;border-radius&amp;quot;, &amp;quot;5px&amp;quot;)
    .style(&amp;quot;padding&amp;quot;, &amp;quot;10px&amp;quot;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&amp;quot;opacity&amp;quot;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&amp;quot;The exact value of&amp;lt;br&amp;gt;the Ground Living area is: &amp;quot; + d.GrLivArea)
      .style(&amp;quot;left&amp;quot;, (d3.mouse(this)[0]+90) + &amp;quot;px&amp;quot;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&amp;quot;top&amp;quot;, (d3.mouse(this)[1]) + &amp;quot;px&amp;quot;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&amp;quot;opacity&amp;quot;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&amp;quot;dot&amp;quot;)
    .data(data.filter(function(d,i){return i&amp;lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&amp;quot;circle&amp;quot;)
      .attr(&amp;quot;cx&amp;quot;, function (d) { return x(d.GrLivArea); } )
      .attr(&amp;quot;cy&amp;quot;, function (d) { return y(d.SalePrice); } )
      .attr(&amp;quot;r&amp;quot;, 7)
      .style(&amp;quot;fill&amp;quot;, &amp;quot;#69b3a2&amp;quot;)
      .style(&amp;quot;opacity&amp;quot;, 0.3)
      .style(&amp;quot;stroke&amp;quot;, &amp;quot;white&amp;quot;)
    .on(&amp;quot;mouseover&amp;quot;, mouseover )
    .on(&amp;quot;mousemove&amp;quot;, mousemove )
    .on(&amp;quot;mouseleave&amp;quot;, mouseleave )

})

&amp;lt;/script&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Embedding D3 interactive charts</title>
      <link>jironghuang.github.io/post/embedding-d3-interactive-charts/</link>
      <pubDate>Sun, 23 Feb 2020 11:46:40 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/embedding-d3-interactive-charts/</guid>
      <description>

&lt;h2 id=&#34;embedding-d3-interactive-charts&#34;&gt;Embedding D3 interactive charts&lt;/h2&gt;

&lt;p&gt;Just having fun - testing to see if I could embed D3 charts in my blog.&lt;/p&gt;

&lt;p&gt;Seems like it works!&lt;/p&gt;

&lt;p&gt;&lt;meta charset=&#34;utf-8&#34;&gt;&lt;/p&gt;

&lt;!-- Load d3.js --&gt;

&lt;script src=&#34;https://d3js.org/d3.v4.js&#34;&gt;&lt;/script&gt;

&lt;!-- Create a div where the graph will take place --&gt;

&lt;div id=&#34;my_dataviz&#34;&gt;&lt;/div&gt;

&lt;script&gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&#34;#my_dataviz&#34;)
  .append(&#34;svg&#34;)
    .attr(&#34;width&#34;, width + margin.left + margin.right)
    .attr(&#34;height&#34;, height + margin.top + margin.bottom)
  .append(&#34;g&#34;)
    .attr(&#34;transform&#34;,
          &#34;translate(&#34; + margin.left + &#34;,&#34; + margin.top + &#34;)&#34;);

//Read the data
d3.csv(&#34;https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/2_TwoNum.csv&#34;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&#34;g&#34;)
    .attr(&#34;transform&#34;, &#34;translate(0,&#34; + height + &#34;)&#34;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&#34;g&#34;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&#34;#my_dataviz&#34;)
    .append(&#34;div&#34;)
    .style(&#34;opacity&#34;, 0)
    .attr(&#34;class&#34;, &#34;tooltip&#34;)
    .style(&#34;background-color&#34;, &#34;white&#34;)
    .style(&#34;border&#34;, &#34;solid&#34;)
    .style(&#34;border-width&#34;, &#34;1px&#34;)
    .style(&#34;border-radius&#34;, &#34;5px&#34;)
    .style(&#34;padding&#34;, &#34;10px&#34;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&#34;opacity&#34;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&#34;The exact value of&lt;br&gt;the Ground Living area is: &#34; + d.GrLivArea)
      .style(&#34;left&#34;, (d3.mouse(this)[0]+90) + &#34;px&#34;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&#34;top&#34;, (d3.mouse(this)[1]) + &#34;px&#34;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&#34;opacity&#34;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&#34;dot&#34;)
    .data(data.filter(function(d,i){return i&lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&#34;circle&#34;)
      .attr(&#34;cx&#34;, function (d) { return x(d.GrLivArea); } )
      .attr(&#34;cy&#34;, function (d) { return y(d.SalePrice); } )
      .attr(&#34;r&#34;, 7)
      .style(&#34;fill&#34;, &#34;#69b3a2&#34;)
      .style(&#34;opacity&#34;, 0.3)
      .style(&#34;stroke&#34;, &#34;white&#34;)
    .on(&#34;mouseover&#34;, mouseover )
    .on(&#34;mousemove&#34;, mousemove )
    .on(&#34;mouseleave&#34;, mouseleave )

})

&lt;/script&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Load d3.js --&amp;gt;
&amp;lt;script src=&amp;quot;https://d3js.org/d3.v4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;!-- Create a div where the graph will take place --&amp;gt;
&amp;lt;div id=&amp;quot;my_dataviz&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;

&amp;lt;script&amp;gt;

// set the dimensions and margins of the graph
var margin = {top: 10, right: 30, bottom: 30, left: 60},
    width = 460 - margin.left - margin.right,
    height = 450 - margin.top - margin.bottom;

// append the svg object to the body of the page
var svg = d3.select(&amp;quot;#my_dataviz&amp;quot;)
  .append(&amp;quot;svg&amp;quot;)
    .attr(&amp;quot;width&amp;quot;, width + margin.left + margin.right)
    .attr(&amp;quot;height&amp;quot;, height + margin.top + margin.bottom)
  .append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;,
          &amp;quot;translate(&amp;quot; + margin.left + &amp;quot;,&amp;quot; + margin.top + &amp;quot;)&amp;quot;);

//Read the data
d3.csv(&amp;quot;https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/2_TwoNum.csv&amp;quot;, function(data) {

  // Add X axis
  var x = d3.scaleLinear()
    .domain([0, 3000])
    .range([ 0, width ]);
  svg.append(&amp;quot;g&amp;quot;)
    .attr(&amp;quot;transform&amp;quot;, &amp;quot;translate(0,&amp;quot; + height + &amp;quot;)&amp;quot;)
    .call(d3.axisBottom(x));

  // Add Y axis
  var y = d3.scaleLinear()
    .domain([0, 400000])
    .range([ height, 0]);
  svg.append(&amp;quot;g&amp;quot;)
    .call(d3.axisLeft(y));

  // Add a tooltip div. Here I define the general feature of the tooltip: stuff that do not depend on the data point.
  // Its opacity is set to 0: we don&#39;t see it by default.
  var tooltip = d3.select(&amp;quot;#my_dataviz&amp;quot;)
    .append(&amp;quot;div&amp;quot;)
    .style(&amp;quot;opacity&amp;quot;, 0)
    .attr(&amp;quot;class&amp;quot;, &amp;quot;tooltip&amp;quot;)
    .style(&amp;quot;background-color&amp;quot;, &amp;quot;white&amp;quot;)
    .style(&amp;quot;border&amp;quot;, &amp;quot;solid&amp;quot;)
    .style(&amp;quot;border-width&amp;quot;, &amp;quot;1px&amp;quot;)
    .style(&amp;quot;border-radius&amp;quot;, &amp;quot;5px&amp;quot;)
    .style(&amp;quot;padding&amp;quot;, &amp;quot;10px&amp;quot;)



  // A function that change this tooltip when the user hover a point.
  // Its opacity is set to 1: we can now see it. Plus it set the text and position of tooltip depending on the datapoint (d)
  var mouseover = function(d) {
    tooltip
      .style(&amp;quot;opacity&amp;quot;, 1)
  }

  var mousemove = function(d) {
    tooltip
      .html(&amp;quot;The exact value of&amp;lt;br&amp;gt;the Ground Living area is: &amp;quot; + d.GrLivArea)
      .style(&amp;quot;left&amp;quot;, (d3.mouse(this)[0]+90) + &amp;quot;px&amp;quot;) // It is important to put the +90: other wise the tooltip is exactly where the point is an it creates a weird effect
      .style(&amp;quot;top&amp;quot;, (d3.mouse(this)[1]) + &amp;quot;px&amp;quot;)
  }

  // A function that change this tooltip when the leaves a point: just need to set opacity to 0 again
  var mouseleave = function(d) {
    tooltip
      .transition()
      .duration(200)
      .style(&amp;quot;opacity&amp;quot;, 0)
  }

  // Add dots
  svg.append(&#39;g&#39;)
    .selectAll(&amp;quot;dot&amp;quot;)
    .data(data.filter(function(d,i){return i&amp;lt;50})) // the .filter part is just to keep a few dots on the chart, not all of them
    .enter()
    .append(&amp;quot;circle&amp;quot;)
      .attr(&amp;quot;cx&amp;quot;, function (d) { return x(d.GrLivArea); } )
      .attr(&amp;quot;cy&amp;quot;, function (d) { return y(d.SalePrice); } )
      .attr(&amp;quot;r&amp;quot;, 7)
      .style(&amp;quot;fill&amp;quot;, &amp;quot;#69b3a2&amp;quot;)
      .style(&amp;quot;opacity&amp;quot;, 0.3)
      .style(&amp;quot;stroke&amp;quot;, &amp;quot;white&amp;quot;)
    .on(&amp;quot;mouseover&amp;quot;, mouseover )
    .on(&amp;quot;mousemove&amp;quot;, mousemove )
    .on(&amp;quot;mouseleave&amp;quot;, mouseleave )

})

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Fuzzy matching with many to many matches without loops</title>
      <link>jironghuang.github.io/post/fuzzy_matching_no_loops/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>jironghuang.github.io/post/fuzzy_matching_no_loops/</guid>
      <description>


&lt;div id=&#34;fuzzy-matching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fuzzy matching&lt;/h2&gt;
&lt;p&gt;As a computer scientist graduate, I always strive to reduce my computational complexity through parallelization or vectorization!&lt;/p&gt;
&lt;p&gt;Explicit loops in data science is the root of evil!&lt;/p&gt;
&lt;p&gt;For loops &amp;amp; while loops have their places but definitely not in data science space (fairly broad statement here).&lt;/p&gt;
&lt;p&gt;In this post here, I hope to show a really cool example that avoids the dreaded O(n square) complexity.&lt;/p&gt;
&lt;p&gt;I will be using fuzzy matching to find the closet match of strings in data-frame 2, df2 against data-frame 1, df1.&lt;/p&gt;
&lt;p&gt;Note: I apologise beforehand for lack of documentation because I am simply lazy after a long Friday. Will beef this up with formal R documentation if I choose to wrap it with a R package next time.&lt;/p&gt;
&lt;div id=&#34;creating-the-first-function-to-incorporate-indexing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the first function to incorporate indexing&lt;/h3&gt;
&lt;p&gt;In this function, what I am trying to find is the Levenshtein distance i.e. minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.&lt;/p&gt;
&lt;p&gt;In the test case below, the difference between “abc” and “abcdef” is Levenshtein distance of 3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(sapply(c(&amp;quot;utils&amp;quot;, &amp;quot;dplyr&amp;quot;), require, character.only = T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## utils dplyr 
##  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adist_mod = function(df, index_col, index_row, string){
  dist = adist(df[index_row, index_col], string)
  return(as.numeric(dist))
}

df1 = data.frame(id = 1:7,
                 places = c(&amp;quot;abc&amp;quot;, &amp;quot;tzy&amp;quot;, &amp;quot;abcd&amp;quot;, &amp;quot;wxyz&amp;quot;, &amp;quot;sentosa&amp;quot;, &amp;quot;marina&amp;quot;,&amp;quot;marina2&amp;quot;)
                 )

df1$places = as.character(df1$places)

adist_mod(df1, 2, 1, &amp;quot;abcdef&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-next-function-to-find-the-smallest-levenshtein-distance-in-all-strings-in-df1-against-a-single-row-from-df2.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the next function to find the smallest Levenshtein distance in all strings in df1 against a single row from df2.&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#create best match function
best_match = function(df, index_col, string){
 
  dist = mapply(adist_mod, 
         index_col = rep(index_col, nrow(df)),
         index_row = 1:nrow(df),
         string = rep(string),
         MoreArgs = list(df)
         )
  
  min_dist = data.frame(
    place2 = rep(string, nrow(df)),
    place1 = df[, index_col],
    dist = dist
  )
  
  min_dist$place2 = as.character(min_dist$place2)
  min_dist$place1 = as.character(min_dist$place1)
  
  
  #return a data frame with df2 and distance
  min_dist = data.frame(dplyr::filter(min_dist,
                                      dist == min(min_dist$dist)
                        ))
  
  return(min_dist)
}

best_match(df1, 2, &amp;quot;abcdef&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   place2 place1 dist
## 1 abcdef   abcd    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;ab&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   place2 place1 dist
## 1     ab    abc    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;sentosa1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     place2  place1 dist
## 1 sentosa1 sentosa    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match(df1, 2, &amp;quot;marina1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    place2  place1 dist
## 1 marina1  marina    1
## 2 marina1 marina2    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;last-but-not-the-least-find-n-to-m-matches-without-explicit-for-loops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Last but not the least, find n to m matches without explicit for loops&lt;/h3&gt;
&lt;p&gt;The final step with our favorite friend - mapply!&lt;/p&gt;
&lt;p&gt;If you wish to parallelize with more cores, please replace it with mcmapply beforehand. But do load parallel package beforehand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_match_all = function(df1, df2, col_index, col_index2){
  
res = mapply(best_match,
       index_col = rep(col_index, nrow(df2)),
       string = df2[, col_index2],
       MoreArgs = list(df = df1),
       SIMPLIFY = F
       )

res_bind = bind_rows(res)

return(res_bind)

}


df2 = data.frame(
  id = 1:3,
  places2 = c(&amp;quot;abcdef&amp;quot;, &amp;quot;sentosa1&amp;quot;, &amp;quot;marina1&amp;quot;)
)
df2$places2 = as.character(df2$places2)

best_match_all(df1, df2, 2, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     place2  place1 dist
## 1   abcdef    abcd    2
## 2 sentosa1 sentosa    1
## 3  marina1  marina    1
## 4  marina1 marina2    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latest lessons learnt from crawling</title>
      <link>jironghuang.github.io/post/crawling-insights/</link>
      <pubDate>Sun, 08 Dec 2019 11:46:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/crawling-insights/</guid>
      <description>

&lt;h2 id=&#34;lessons-learnt&#34;&gt;Lessons learnt&lt;/h2&gt;

&lt;p&gt;I just realised that there&amp;rsquo;s a quick way to understand the xpaths&amp;rsquo; patterns.&lt;/p&gt;

&lt;p&gt;In the past, usually what I did is to manually eyeball to infer the patterns from the page source or inspect page.&lt;/p&gt;

&lt;p&gt;Silly me!&lt;/p&gt;

&lt;p&gt;1 quick way to understand the pattern is through the following,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Right click on an element in a web page that you are interested in and click on &amp;lsquo;inspect&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Right click on the node and click &amp;lsquo;copy&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Copy full xpath&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And paste to a notepad.&lt;/p&gt;

&lt;p&gt;Do it for the series of elements that you wish to crawl.&lt;/p&gt;

&lt;p&gt;And presto you are able to observe the patterns!&lt;/p&gt;

&lt;h2 id=&#34;case-in-point&#34;&gt;Case in point&lt;/h2&gt;

&lt;p&gt;With the increment in indexes in the following paths, I could easily do a loop to form the xpaths and feed it into python xpath function and do an &amp;lsquo;extract_first()&amp;lsquo;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[1]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[2]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[3]/div/div/div/div[1]/div/div[1]/div[3]/a
/html/body/div[4]/div[2]/div[1]/div[1]/div/div[2]/div[11]/div[4]/div/div/div/div[1]/div/div[1]/div[3]/a
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Asset allocation notification</title>
      <link>jironghuang.github.io/post/asset_allocation_notification/</link>
      <pubDate>Thu, 12 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/asset_allocation_notification/</guid>
      <description>

&lt;h2 id=&#34;asset-allocation-notification&#34;&gt;Asset allocation notification&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m in the midst of automating/ guiding my life with algorithms (largely inspired by Ray Dalio) - and 1 of the guidelines that I set is on asset allocation,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emerging market and Developed Market should be of the same proportion&lt;/li&gt;
&lt;li&gt;Bonds + Cash proportion should be equivalent to my age. This can deviate in times of crisis when I want to be more opportunistic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it deviates from the portfolio policy statement, it will send me a pushover notification to my phone:)&lt;/p&gt;

&lt;p&gt;Here is a simple example of it works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/asset_notify.jpeg&#34; alt=&#34;/post/img/asset_notify.jpeg&#34;&gt;&lt;/p&gt;

&lt;p&gt;You may find my code below. Briefly this is what it does,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It ssh into my googlesheet&lt;/li&gt;
&lt;li&gt;Pull out the asset allocation and compare against the policy statements&lt;/li&gt;
&lt;li&gt;If it deviates, it will send me a notification&lt;/li&gt;
&lt;li&gt;This is scheduled via a cronjob in linux that runs every midnight&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;#Comment on asset allocation
dat = gs_title(&amp;quot;Investment&amp;quot;)
gs_ws_ls(dat)   #tab names
data &amp;lt;- gs_read(ss=dat, ws = &amp;quot;asset_allocation1&amp;quot;, skip=0)
data = as.data.frame(data)

if(data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)] &amp;gt; data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Developed. Shift to Emerging\n&amp;quot;
  msg = paste(msg, &amp;quot;World is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)])
  msg = paste(msg, &amp;quot;Emerging is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)] &amp;gt; data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Emerging. Shift to Developed\n&amp;quot;
  msg = paste(msg, &amp;quot;World is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;World + Developed (ex real estate)&amp;quot;)])
  msg = paste(msg, &amp;quot;Emerging is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Emerging&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(100 * (data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)] + data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)]) &amp;gt; data$Prop[which(data$Assets == &amp;quot;Age&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are overweight in Bonds and Cash relative to your age. Shift it out!!!\n&amp;quot;
  msg = paste(msg, &amp;quot;Bonds is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)])
  msg = paste(msg, &amp;quot;Cash is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}

if(100 * (data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)] + data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)]) &amp;lt; data$Prop[which(data$Assets == &amp;quot;Age&amp;quot;)]){
  
  msg = &amp;quot;Bro, you are underweight in Bonds and Cash relative to your age. Move out from equities!!!\n&amp;quot;
  msg = paste(msg, &amp;quot;Bonds is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Bonds&amp;quot;)])
  msg = paste(msg, &amp;quot;Cash is %&amp;quot;, 100 * data$Prop[which(data$Assets == &amp;quot;Cash&amp;quot;)])
  
  pushover(message = msg, 
           user = Sys.getenv(&amp;quot;pushover_user&amp;quot;), app = Sys.getenv(&amp;quot;pushover_app&amp;quot;)
  )
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ETF watchlist email notification Through Python</title>
      <link>jironghuang.github.io/post/email_notification_python/</link>
      <pubDate>Tue, 10 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/email_notification_python/</guid>
      <description>

&lt;h2 id=&#34;email-notification&#34;&gt;Email notification&lt;/h2&gt;

&lt;p&gt;I finally bit the bullet and updated my previously hideous email notification!&lt;/p&gt;

&lt;p&gt;You may find the updated email notification template here - alongside with the code.&lt;/p&gt;

&lt;p&gt;Feel free to ping me if you are keen to be on the email list too.&lt;/p&gt;

&lt;p&gt;~ Jirong&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;jironghuang.github.io/post/img/email.png&#34; alt=&#34;/post/img/email.png&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import smtplib, ssl
import datetime
import pandas as pd
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

#Format text
data = pd.read_csv(&#39;/home/jirong/Desktop/github/ETF_watchlist/Output/yahoo_crawled_data.csv&#39;)
data[&#39;Change_fr_52_week_high&#39;] = round(100 * data[&#39;Change_fr_52_week_high&#39;], 1) 
data = data[[&#39;Name&#39;, &#39;Price&#39;, &#39;Change_fr_52_week_high&#39;]].head(20)
data.rename(columns={&#39;Change_fr_52_week_high&#39;:&#39;Change from 52 week high (%)&#39;}, inplace=True)
results = data.to_html()

message = MIMEMultipart(&amp;quot;alternative&amp;quot;)
message[&amp;quot;Subject&amp;quot;] = &amp;quot;ETF Watchlist&amp;quot;
message[&amp;quot;From&amp;quot;] = &amp;quot;jironghuang88@gmail.com&amp;quot;
message[&amp;quot;Bcc&amp;quot;] = &amp;quot;&amp;quot;

# Create the plain-text and HTML version of your message
html = &amp;quot;&amp;quot;&amp;quot;\
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;p&amp;gt;Hey there!       
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;    
       &amp;lt;br&amp;gt;Pls click &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/watch_list/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; for a complete updated ETF watchlist.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;For your convenience, I&#39;ve also appended the top 20 tickers with greatest fall over last 52 week high (potentially cheap but you should triangulate with other sources)&amp;lt;br&amp;gt;
       {0}

       &amp;lt;br&amp;gt;This is part of my daily automated ETF dashboard + Email notification (weekly for email notification) and I thought you may be interested in it.&amp;lt;br&amp;gt; 

       &amp;lt;br&amp;gt;Do also check out my &#39;What is Low - Regression Approach&#39; &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/what-is-low-regression-approach/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; to understand which are the markets that are under or overvalued.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;If this irritates you too much, let me know and I can take you out of this mailing list:)&amp;lt;br&amp;gt;

       &amp;lt;br&amp;gt;Regards,&amp;lt;br&amp;gt;
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
       Jirong
    &amp;lt;/p&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

&amp;quot;&amp;quot;&amp;quot;.format(data.to_html())

# Turn these into plain/html MIMEText objects
#part1 = MIMEText(text, &amp;quot;plain&amp;quot;)
part2 = MIMEText(html, &amp;quot;html&amp;quot;)
#part3 = MIMEText(results, &amp;quot;html&amp;quot;)

# Add HTML/plain-text parts to MIMEMultipart message
# The email client will try to render the last part first
message.attach(part2)

server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587)
server.starttls()
server.login(&amp;quot;jironghuang88@gmail.com&amp;quot;, &amp;quot;&amp;quot;)

recipients = [&amp;quot;jironghuang88@gmail.com&amp;quot;]

server.sendmail(&amp;quot;jironghuang88@gmail.com&amp;quot;, recipients, message.as_string())
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Convert NAs to Obscure Number in Data Frame to Aid in Recoding/ Feature Engineering</title>
      <link>jironghuang.github.io/post/convert_na_num/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>jironghuang.github.io/post/convert_na_num/</guid>
      <description>


&lt;div id=&#34;converting-nas-to-obscure-numbers-to-prevent-the-data-from-messing-up-the-recoding.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Converting NAs to obscure numbers to prevent the data from messing up the recoding.&lt;/h2&gt;
&lt;p&gt;1 issue that I encounter while I data-munge is that NAs in data seem to mess up my recoding. Here’s a neat swiss army knife utility function I developed recently.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(dplyr))

# Converting NA to obscure number to prevent awkward recoding situations that require &amp;amp; !is.na(&amp;lt;variable&amp;gt;)
# Doesn&amp;#39;t work for factors
#&amp;#39; @title Convert NA to obscure number
#&amp;#39; @param dp_dataframe Dataframe in consideration
#&amp;#39; @param np_obscure_num Numeric - Obscure number
#&amp;#39; @param bp_na_to_num Boolean if TRUE, convert NA to num. If FALSE, convert num to NA
#&amp;#39; @return A data frame with converted NAs
#&amp;#39; @export

df_convertNAs_to_obscureNo = function(dp_dataframe, np_obscure_num, bp_na_to_num){

  if(bp_na_to_num == T){

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., is.na(.), np_obscure_num)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., is.na(.), as.character(np_obscure_num)))  

  }else if(bp_na_to_num == F){
    
    bf_is_obscure_num = function(np_num){
      return(np_num == np_obscure_num)
    }

    bf_is_obscure_num_string = function(np_num){
      return(as.character(np_num) == as.character(np_obscure_num))
    }

    dConverted_data = dp_dataframe %&amp;gt;%
      mutate_if(is.integer, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.numeric, ~ replace(., bf_is_obscure_num(.), NA)) %&amp;gt;%
      mutate_if(is.character, ~ replace(., bf_is_obscure_num_string(.), NA))
  }
  
  return(dConverted_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To illustrate how we could use the function, first, I load in the car dataset.&lt;/p&gt;
&lt;p&gt;Second, I insert NA values into 1 of the cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2
## 1     4   NA     2
## 2     4   10    10
## 3     7    4     4
## 4     7   22    22
## 5     8   16    16
## 6     9   10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third, I implement a recoding-logic as follows,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2      NA
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice in the first row that just because dist is NA, is_true returns NA. This holds true for even longer recoding rules. A mere NA would render the return value to be NA even if you would expect the result to be 1!&lt;/p&gt;
&lt;p&gt;Here comes the highlight of the post.&lt;/p&gt;
&lt;p&gt;By applying the df_convertNAs_to_obscureNo function I wrote above to the data frame with the following parameters,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data frame name;&lt;/li&gt;
&lt;li&gt;An obscure number that will never be used in recoding (e.g. -123456);&lt;/li&gt;
&lt;li&gt;And a boolean flag (will explain this later),&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You would be able to skirt the issue I highlighted above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
data = head(cars)
data$dist2 = data$dist
str(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  3 variables:
##  $ speed: num  4 4 7 7 8 9
##  $ dist : num  2 10 4 22 16 10
##  $ dist2: num  2 10 4 22 16 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$dist[1] = NA

data = df_convertNAs_to_obscureNo(data, -1234567, T)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2
## 1     4 -1234567     2
## 2     4       10    10
## 3     7        4     4
## 4     7       22    22
## 5     8       16    16
## 6     9       10    10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Recoding results
data$is_true = ifelse(data$speed == 4 &amp;amp; data$dist != 0 &amp;amp; data$dist2 == 2, 1, 0)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed     dist dist2 is_true
## 1     4 -1234567     2       1
## 2     4       10    10       0
## 3     7        4     4       0
## 4     7       22    22       0
## 5     8       16    16       0
## 6     9       10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So instead of obtaining a NA value, it would return 1 instead!&lt;/p&gt;
&lt;p&gt;And you may ask me - how do I change -123457 back into NA?&lt;/p&gt;
&lt;p&gt;You could simply reuse the same function with a FALSE flag. And presto the obscure value is converted back into NA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = df_convertNAs_to_obscureNo(data, -1234567, F)
print(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist dist2 is_true
## 1     4   NA     2       1
## 2     4   10    10       0
## 3     7    4     4       0
## 4     7   22    22       0
## 5     8   16    16       0
## 6     9   10    10       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warning!!!&lt;/h2&gt;
&lt;p&gt;Developer/ Data Scientist/ Data Analysis would need to be absolutely sure that this is what he/ she wants. If NA value is expected instead of 1 in the above use case, please do not use the function!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Loading excel data with correct variable types</title>
      <link>jironghuang.github.io/post/load_data_with_correct_types/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>jironghuang.github.io/post/load_data_with_correct_types/</guid>
      <description>


&lt;div id=&#34;loading-data-with-data-types&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading data with data types&lt;/h3&gt;
&lt;p&gt;When reading static files into R or Python, most of the times we are lazy as we load the data with no regard to the data types.&lt;/p&gt;
&lt;p&gt;But in mission critical ETL jobs or Data analytics workflow, data types are quintessential and there’s a fine line between life and death. Ok, I’m exaggerating here.&lt;/p&gt;
&lt;p&gt;What I’ve written below is a swiss army knife function to read an excel file: 1st tab is data and 2nd tab is the variable types (e.g. database variable types mapped to R variable types)&lt;/p&gt;
&lt;p&gt;Note: If I read or write to database, I would have to modify my function below. Oh well.&lt;/p&gt;
&lt;p&gt;The steps in the function are pretty simple,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First I read in the data type&lt;/li&gt;
&lt;li&gt;Second I read in the column name of the dataset&lt;/li&gt;
&lt;li&gt;Third I left join the column names to its data type from database (or .sav or .dta or .sas files)&lt;/li&gt;
&lt;li&gt;Fourth I left join the (DB variable types) to (R data types) translation into the column names&lt;/li&gt;
&lt;li&gt;Lastly, I read in the dataset through read_excel functions with col_types as the parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here you go! Hope this is useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Reading excel data together with data types
#&amp;#39; @title Reading data with data types
#&amp;#39; @param spName_file Path of the data file
#&amp;#39; @param spTab_name_data Tab name of data
#&amp;#39; @param spTab_name_dataType Tab name of data type
#&amp;#39; @param sp_dataType_col_name Column name of dataType&amp;#39;s (variables column)
#&amp;#39; @param sp_dataType_datatype_name Column name of dataType&amp;#39;s (data type column)
#&amp;#39; @param dp_r_hana_type data frame of R to hana variable types conversion
#&amp;#39; @param sp_r_hana_type_DBType Column name of r_hana_type (DB_TYPE column)
#&amp;#39; @param sp_r_hana_type_RType Column name of r_hana_type (R_TYPE column)
#&amp;#39; @return A data frame with corresponding data types
#&amp;#39; @export
df_read_data_with_types = function(spName_file, spTab_name_data,
                                   spTab_name_dataType,
                                   sp_dataType_col_name, sp_dataType_datatype_name,
                                   dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType){

  #Read in data types
  data_type = df_read_tab(spName_file, spTab_name_dataType)

  #Read in just first row of dataset
  col_name = read_excel(spName_file, spTab_name_data, n_max = 1)
  col_name = data.frame(col_name = names(col_name)); col_name$col_name = as.character(col_name$col_name)
  col_name$column_order = 1:nrow(col_name)

  #Left join data type to name
  col_name = col_name %&amp;gt;%
    dplyr::left_join(data_type, by = c(&amp;quot;col_name&amp;quot; = sp_dataType_col_name))

  #Split this from above because of errors. weird
  col_name = base::merge(col_name, dp_r_hana_type,
                   by.x = sp_dataType_datatype_name, by.y = sp_r_hana_type_DBType,
                   all.x = T)

  col_name = arrange(col_name, column_order)

  #Read in full dataset with assignment of classes (look at readxl.tidyverse.org)
  data = readxl::read_excel(spName_file, spTab_name_data, col_types = col_name[, sp_r_hana_type_RType])

  return(data)
}

# spName_file = &amp;#39;./data/input/TB_OVSS_FNB_FACT.xlsx&amp;#39;
# spTab_name_data = &amp;#39;data&amp;#39;
# spTab_name_dataType = &amp;#39;data_type&amp;#39;
# sp_dataType_col_name = &amp;#39;COLUMN_NAME&amp;#39; 
# sp_dataType_datatype_name = &amp;#39;DATA_TYPE_NAME&amp;#39;
# dp_r_hana_type = r_hana_type 
# sp_r_hana_type_DBType = &amp;#39;DB_TYPE&amp;#39; 
# sp_r_hana_type_RType = &amp;#39;R_TYPE&amp;#39;
#
# a = df_read_data_with_types (spName_file, spTab_name_data,
#                          spTab_name_dataType,
#                          sp_dataType_col_name, sp_dataType_datatype_name,
#                          dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Function to describe clusters derived from unsupervised learning</title>
      <link>jironghuang.github.io/post/cluster_descriptive_stats/</link>
      <pubDate>Fri, 24 May 2019 11:46:49 +0800</pubDate>
      
      <guid>jironghuang.github.io/post/cluster_descriptive_stats/</guid>
      <description>

&lt;h2 id=&#34;describing-unsupervised-learning-clusters&#34;&gt;Describing unsupervised learning clusters&lt;/h2&gt;

&lt;p&gt;As a data scientist / analyst, besides doing cool modelling stuff, we&amp;rsquo;re often asked to churn out descriptive statistics. Yes, we know. It&amp;rsquo;s part of the process.&lt;/p&gt;

&lt;p&gt;I chanced upon this really nifty concept at work to describe the clusters derived from unsupervised learnig. Here&amp;rsquo;s how it goes,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Say it&amp;rsquo;s a nominal or ordinal variable. First, I find the proportion of the feature across the X clusters&lt;/li&gt;
&lt;li&gt;Second, I rank this proportion through percentiles across these X values&lt;/li&gt;
&lt;li&gt;The cluster with the highest percentile will earn its right to be represented by the feature&lt;/li&gt;
&lt;li&gt;And if it&amp;rsquo;s a scale variable, you may find the mean of the feature for each cluster and repeat the steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is a nifty function to carry out the above steps. You can compile it into a package and may start using it in your data science work!&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  This function takes in a data-frame
#&#39; @title Finding feature importance associated with each cluster
#&#39; @param dp_data data frame for clustering
#&#39; @param sp_resp_key_name primary key in dataset
#&#39; @param sp_feature Feature name
#&#39; @param sp_cluster_name column name for cluster 
#&#39; @param sp_scale_categorical scale or categorical
#&#39; @param sp_IndivYr_aggregate IndivYr or aggregate
#&#39; @param sp_weight_name Column names for weights
#&#39; @param np_feature_imp_threshold threshold for percentile ranking
#&#39; @param sp_all_filter filter or no filter
#&#39; @return list of proportion, mean or percentile for 6 cluster importance
#&#39; @export
#&#39;

l_feature_importance_cluster = function(dp_data, sp_resp_key_name,
                                        sp_feature, sp_cluster_name, sp_scale_categorical,
                                        sp_IndivYr_aggregate = NULL, sp_weight_name,
                                        np_feature_imp_threshold = 1, sp_all_filter = &amp;quot;all&amp;quot;){

  tryCatch({
  if((sp_scale_categorical == &amp;quot;categorical&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
    #Tabulate

    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_weight_name, sp_cluster_name)) %&amp;gt;%                #select columns
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%   #Count by cluster and feature
        dplyr::summarize(counts = n(),
                         wt_counts = sum(!! rlang:: sym(sp_weight_name))) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%                               #Find proportion of feature in cluster group
        dplyr::mutate(counts_cluster = sum(counts, na.rm = T),
                      prop_feature_within_cluster = counts/counts_cluster,
                      counts_cluster_wt = sum(wt_counts, na.rm = T),
                      prop_feature_within_cluster_wt = wt_counts/counts_cluster_wt) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_feature)) %&amp;gt;%                                    #Find percentile of proportions
        dplyr::mutate(percentile_feature = percent_rank(prop_feature_within_cluster),
                      percentile_feature_wt = percent_rank(prop_feature_within_cluster_wt),
                      counts_feature = sum(counts, na.rm = T),
                      prop_cluster_within_features = counts/counts_feature) %&amp;gt;%            #Find proportion of cluster in feature
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::rename(feature_value = sp_feature)

    #Filter to keep only the &#39;meaningful&#39; features   

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
        dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)   
    }

  } else if((sp_scale_categorical == &amp;quot;scale&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
  
    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_cluster_name, sp_weight_name)) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%
        dplyr::summarize(avg_by_cluster = mean(!! rlang:: sym(sp_feature), na.rm = T),
                         avg_by_cluster_wt = weighted.mean(!! rlang:: sym(sp_feature), !! rlang:: sym(sp_weight_name), na.rm = T))  %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::mutate(percentile_feature = percent_rank(avg_by_cluster),
                      percentile_feature_wt = percent_rank(avg_by_cluster_wt))

     #Filter to keep only the &#39;meaningful&#39; features

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
                            dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)    
    } 
  }
  }, error = function(e){
    print(paste0(&amp;quot;Error with &amp;quot;, sp_feature))
  })
  
  return(dTabulated_data)

}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
