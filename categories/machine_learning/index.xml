<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning on Jirong&#39;s sandbox</title>
    <link>/categories/machine_learning/</link>
    <description>Recent content in machine_learning on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 10 Sep 2019 11:46:49 +0800</lastBuildDate>
    <atom:link href="/categories/machine_learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ETF watchlist email notification Through Python</title>
      <link>/post/email_notification_python/</link>
      <pubDate>Tue, 10 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/email_notification_python/</guid>
      <description>

&lt;h2 id=&#34;email-notification&#34;&gt;Email notification&lt;/h2&gt;

&lt;p&gt;I finally bit the bullet and updated my previously hideous email notification!&lt;/p&gt;

&lt;p&gt;You may find the updated email notification template here - alongside with the code.&lt;/p&gt;

&lt;p&gt;Feel free to ping me if you are keen to be on the watchlist too.&lt;/p&gt;

&lt;p&gt;~ Jirong&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/email.png&#34; alt=&#34;/post/img/email.png&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import smtplib, ssl
import datetime
import pandas as pd
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

#Format text
data = pd.read_csv(&#39;/home/jirong/Desktop/github/ETF_watchlist/Output/yahoo_crawled_data.csv&#39;)
data[&#39;Change_fr_52_week_high&#39;] = round(100 * data[&#39;Change_fr_52_week_high&#39;], 1) 
data = data[[&#39;Name&#39;, &#39;Price&#39;, &#39;Change_fr_52_week_high&#39;]].head(20)
data.rename(columns={&#39;Change_fr_52_week_high&#39;:&#39;Change from 52 week high (%)&#39;}, inplace=True)
results = data.to_html()

message = MIMEMultipart(&amp;quot;alternative&amp;quot;)
message[&amp;quot;Subject&amp;quot;] = &amp;quot;ETF Watchlist&amp;quot;
message[&amp;quot;From&amp;quot;] = &amp;quot;jironghuang88@gmail.com&amp;quot;
message[&amp;quot;Bcc&amp;quot;] = &amp;quot;&amp;quot;

# Create the plain-text and HTML version of your message
html = &amp;quot;&amp;quot;&amp;quot;\
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;p&amp;gt;Hey there!       
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;    
       &amp;lt;br&amp;gt;Pls click &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/watch_list/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; for a complete updated ETF watchlist.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;For your convenience, I&#39;ve also appended the top 20 tickers with greatest fall over last 52 week high (potentially cheap but you should triangulate with other sources)&amp;lt;br&amp;gt;
       {0}

       &amp;lt;br&amp;gt;This is part of my daily automated ETF dashboard + Email notification (weekly for email notification) and I thought you may be interested in it.&amp;lt;br&amp;gt; 

       &amp;lt;br&amp;gt;Do also check out my &#39;What is Low - Regression Approach&#39; &amp;lt;a href=&amp;quot;https://jironghuang.github.io/project/what-is-low-regression-approach/&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; to understand which are the markets that are under or overvalued.&amp;lt;br&amp;gt;
       
       &amp;lt;br&amp;gt;If this irritates you too much, let me know and I can take you out of this mailing list:)&amp;lt;br&amp;gt;

       &amp;lt;br&amp;gt;Regards,&amp;lt;br&amp;gt;
       &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
       Jirong
    &amp;lt;/p&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

&amp;quot;&amp;quot;&amp;quot;.format(data.to_html())

# Turn these into plain/html MIMEText objects
#part1 = MIMEText(text, &amp;quot;plain&amp;quot;)
part2 = MIMEText(html, &amp;quot;html&amp;quot;)
#part3 = MIMEText(results, &amp;quot;html&amp;quot;)

# Add HTML/plain-text parts to MIMEMultipart message
# The email client will try to render the last part first
message.attach(part2)

server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587)
server.starttls()
server.login(&amp;quot;jironghuang88@gmail.com&amp;quot;, &amp;quot;&amp;quot;)

recipients = [&amp;quot;jironghuang88@gmail.com&amp;quot;]

server.sendmail(&amp;quot;jironghuang88@gmail.com&amp;quot;, recipients, message.as_string())
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on Reinforcement Learning - Q Learning</title>
      <link>/post/q_learning/</link>
      <pubDate>Mon, 08 Apr 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/q_learning/</guid>
      <description>

&lt;h2 id=&#34;q-learning&#34;&gt;Q learning&lt;/h2&gt;

&lt;p&gt;I just completed a Reinforcement Learning assignment - in particular on Q-learning. According to Wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Q-learning&#34;&gt;here&lt;/a&gt;, it&amp;rsquo;s a model-free Rl algorithm. The goal for the algo is to learn a policy, which tells an agent what action to take under different circumstances.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s my confession. What I&amp;rsquo;m doing in this post is to summarise what I&amp;rsquo;ve just learnt so that I may come back to this at any point in future. Hence it may or may not make sense to you,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RL is rather different from a conventional ML process. It contains an in-built iterative process to refresh the parameters&lt;/li&gt;
&lt;li&gt;User can train the algorithm till the &amp;lsquo;total reward&amp;rsquo; (in ML lingo that could be error-type measures) converges. I will coin each training process as a simulation&lt;/li&gt;
&lt;li&gt;To set up the learning process, I first initalize a Q learning table of 0s with dimension of number of states * actions (In python lingo, self.q = np.zeros((num_states, num_actions), dtype = np.float64))&lt;/li&gt;
&lt;li&gt;In every step of each simulation, the algo will pick a random float of 0 to 1. If it&amp;rsquo;s lesser than the threshold, it will pick a random action. Else, it will pick the action with the best outcome. According to the literature, it seems that exploration plays a role in improving the results&lt;/li&gt;
&lt;li&gt;From second step onward, the algo will update the Q-table as follows: self.q[self.s, self.a] = (1 - self.alpha) * self.q[self.s, self.a] + self.alpha * (r + self.gamma * np.max(self.q[s_prime,]))&lt;/li&gt;
&lt;li&gt;What it meant in the above formula is that Q-learning computes a weighted score for a particular state and action based on present and future discounted score from best action.&lt;/li&gt;
&lt;li&gt;The updated score will be used in subsequent simulations, and not current one. i.e in future iteration, if the option is non-random, it will pick the highest score option.&lt;/li&gt;
&lt;li&gt;When the current simulation end, the algorithm will return to the starting point and retrain the algorithm with the refreshed Q-table&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dyna-q&#34;&gt;Dyna-Q&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The additional bootstrap component algorithm is deemed to be cheaper because it doesn&amp;rsquo;t require additional interaction with the external environment to refresh the Q-table.&lt;/li&gt;
&lt;li&gt;It will instead pick random states and actions&lt;/li&gt;
&lt;li&gt;And select a new state based on a probability mass function in each loop (each state is assigned a discrete chance. Say there&amp;rsquo;re 100 states where 1 of the states has 2% chance and another has 1% chance. The latter might still be selected albeit with a lower chance)&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s different here is that it will refresh the reward information too&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;insights-from-this-exercise&#34;&gt;Insights from this exercise&lt;/h3&gt;

&lt;p&gt;The strength of this algorithm lies in the fact that it doesn&amp;rsquo;t require a ton of data. I&amp;rsquo;m excited to apply this algorithm if there&amp;rsquo;s a chance.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
