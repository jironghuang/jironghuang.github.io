<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>algo on Jirong&#39;s sandbox</title>
    <link>/categories/algo/</link>
    <description>Recent content in algo on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 24 Aug 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/algo/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Market Neutral Strategy - SnP500 to Berkshire Hathaway Ratio</title>
      <link>/post/market_neutral/</link>
      <pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/market_neutral/</guid>
      <description>


&lt;div id=&#34;market-neutral-strategy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Market neutral strategy&lt;/h2&gt;
&lt;p&gt;As the negative news pile up (trade wars, slump in economy growths, etc), I sought for market neutral stategies that could perform well in any market environment.&lt;/p&gt;
&lt;p&gt;An idea that struck me recently is to exploit the pair between Berkshire and SnP 500.&lt;/p&gt;
&lt;p&gt;The SnP500/ Berkshire ratio has been falling over the years - insinuating that Berkshire still outperforms the index in the last couple of years.&lt;/p&gt;
&lt;p&gt;And what’s more impressive, it’s still widely regarded as a safe haven in times of trouble.&lt;/p&gt;
&lt;div id=&#34;strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strategy&lt;/h3&gt;
&lt;p&gt;As the market is facing headwind and I sought for a market neutral strategy.&lt;/p&gt;
&lt;p&gt;Here’s what I did,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I derive the SnP500/ Berkshire ratio over last 5 years&lt;/li&gt;
&lt;li&gt;And I fix a Bollinger Band around the ratio with n = 200 days as the moving average parameter - with 2 SD as the lower (lb) and upper bound (ub) line. Bollinger band ratio accounts for the downward trend in the ratio over time.&lt;/li&gt;
&lt;li&gt;If the ratio is below the lb, I will long SnP500 and short Berkshire Hathaway. And when it mean reverts and touches the middle moving average, I will exit the positions&lt;/li&gt;
&lt;li&gt;Conversely if the ratio is above the ub, I will short SnP500 and long Berkshire Hathaway. Similarly, I will exit the position when it touches the moving average&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;So how did the strategy fare? Fairly impressive I must say.&lt;/p&gt;
&lt;p&gt;Sharpe ratio is around 0.8. Annualized return is around 5.7% with the positions only in the market 30% of the time!&lt;/p&gt;
&lt;p&gt;That being said, I’m cherry picking here because the performance before this period is sub-par; probably because of a change in market regime. You may execute my code to stress-test this simple strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disclosure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Disclosure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I executed a long (BRK-B) short (SnP500 index CFD) strategy on 26-August with approximately $8000 on both positions. As I’m using margin, the cash outlay is only 1600 dollars.&lt;/li&gt;
&lt;li&gt;I’m using an email notification system to inform me to close my positions when the ratio dips below moving average.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running packages&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;##                  zoo                tidyr                 plyr 
##                 TRUE                 TRUE                 TRUE 
##                dplyr               gtools         googlesheets 
##                 TRUE                 TRUE                 TRUE 
##             quantmod                 urca PerformanceAnalytics 
##                 TRUE                 TRUE                 TRUE 
##             parallel                  TTR 
##                 TRUE                 TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Function using bollinger band
lf_bollinger_pair_trading = function(stock1, stock2, start_date, end_date, prop_res, bband_days){
  
#Start of function
data1 = df_crawl_time_series(stock1, start_date, end_date)
data1 = base::subset(data1, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Adj.Close&amp;quot;))
names(data1) = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;)
data1$Date = as.Date(data1$Date)

data2 = df_crawl_time_series(stock2, start_date, end_date)
data2 = base::subset(data2, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Adj.Close&amp;quot;))
names(data2) = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;)
data2$Date = as.Date(data2$Date)

#Training and testing index
data1 = xts(data1[, -1], order.by = data1[, 1])
data2 = xts(data2[, -1], order.by = data2[, 1])

data = merge(data1, data2)
data = as.data.frame(data)
data = subset(data, !is.na(data$Close) &amp;amp; !is.na(data$Close.1))

data$ratio = data$Close/ data$Close.1

# plot(data$ratio)
bb_ratio = data.frame(BBands( data$ratio, n = bband_days))
data = cbind(data, bb_ratio)
data_sub = tail(data, round(nrow(data) * prop_res, 0))

plot(data_sub$ratio)
lines(data_sub$mavg, col = &amp;quot;red&amp;quot;)
lines(data_sub$up, col = &amp;quot;blue&amp;quot;)
lines(data_sub$dn, col = &amp;quot;green&amp;quot;)

#If lower than 
data_sub$longs &amp;lt;- data_sub$ratio &amp;lt;= data_sub$dn # buy spread when its value drops below 2 standard deviations.
data_sub$shorts &amp;lt;- data_sub$ratio &amp;gt;= data_sub$up # short spread when its value rises above 2 standard deviations.

#  exit any spread position when its value is at moving average
data_sub$longExits   &amp;lt;- data_sub$ratio &amp;gt;= data_sub$mavg
data_sub$shortExits &amp;lt;- data_sub$ratio &amp;lt;= data_sub$mavg


# #  define indices for training and test sets
# trainset &amp;lt;- 1:as.integer(nrow(data) * prop_train)
# testset &amp;lt;- (length(trainset)+1):nrow(data)

#Signal
data_sub$posL1 = NA
data_sub$posL2 = NA
data_sub$posS1 = NA
data_sub$posS2 = NA

# initialize to 0
data_sub$posL1[1] &amp;lt;- 0; data_sub$posL2[1] &amp;lt;- 0
data_sub$posS1[1] &amp;lt;- 0; data_sub$posS2[1] &amp;lt;- 0

data_sub$posL1[data_sub$longs] &amp;lt;- 1
data_sub$posL2[data_sub$longs] &amp;lt;- -1

data_sub$posS1[data_sub$shorts] &amp;lt;- -1
data_sub$posS2[data_sub$shorts] &amp;lt;- 1

data_sub$posL1[data_sub$longExits] &amp;lt;- 0
data_sub$posL2[data_sub$longExits] &amp;lt;- 0
data_sub$posS1[data_sub$shortExits] &amp;lt;- 0
data_sub$posS2[data_sub$shortExits] &amp;lt;- 0

#positions
data_sub$posL1 &amp;lt;- zoo::na.locf(data_sub$posL1); data_sub$posL2 &amp;lt;- zoo::na.locf(data_sub$posL2)
data_sub$posS1 &amp;lt;- zoo::na.locf(data_sub$posS1); data_sub$posS2 &amp;lt;- zoo::na.locf(data_sub$posS2)
data_sub$position1 &amp;lt;- data_sub$posL1 + data_sub$posS1
data_sub$position2 &amp;lt;- data_sub$posL2 + data_sub$posS2

#Returns
data_sub$dailyret1 &amp;lt;- ROC(data_sub$Close) #  last row is [385,] -0.0122636689 -0.0140365802
data_sub$dailyret2 &amp;lt;- ROC(data_sub$Close.1) #  last row is [385,] -0.0122636689 -0.0140365802

#Backshifting here. But signal is for following day returns!. So can still use latest Z-score
data_sub$date = as.Date(row.names(data_sub))
data_sub = xts(data_sub[,-which(names(data_sub) == &amp;quot;date&amp;quot;)], order.by = data_sub[, which(names(data_sub) == &amp;quot;date&amp;quot;)])

#Doesn&amp;#39;t account for number of shares!!!!!
data_sub$pnl = lag(data_sub$position1, 1) * data_sub$dailyret1  + lag(data_sub$position2, 1) * data_sub$dailyret2

#Performance analytics
tryCatch({
  # charts_perf = charts.PerformanceSummary(data_sub$pnl)
  charts.PerformanceSummary(data_sub$pnl)
}, error = function(e){})

dd = table.Drawdowns(data_sub$pnl)
ds_risk = table.DownsideRisk(data_sub$pnl)
ret = table.AnnualizedReturns(data_sub$pnl)


df_ret = list(data_sub = data_sub,
              dd = dd,
              ds_risk = ds_risk,
              ret = ret
              )

return(df_ret)

}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Parameters to be includded in function
stock1 = &amp;quot;^GSPC&amp;quot;
stock2 = &amp;quot;BRK-B&amp;quot;

start_date = &amp;quot;2000-07-01&amp;quot;
end_date = &amp;quot;2019-12-30&amp;quot;

prop_res = 0.25     #Proportion of results to show
bband_days = 200   #Bollinger band of ratio

#Storing result to function
res = lf_bollinger_pair_trading(stock1, stock2, start_date, end_date, prop_res, bband_days)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;getSymbols&amp;#39; currently uses auto.assign=TRUE by default, but will
## use auto.assign=FALSE in 0.5-0. You will still be able to use
## &amp;#39;loadSymbols&amp;#39; to automatically load data. getOption(&amp;quot;getSymbols.env&amp;quot;)
## and getOption(&amp;quot;getSymbols.auto.assign&amp;quot;) will still be checked for
## alternate defaults.
## 
## This message is shown once per session and may be disabled by setting 
## options(&amp;quot;getSymbols.warning4.0&amp;quot;=FALSE). See ?getSymbols for details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## WARNING: There have been significant changes to Yahoo Finance data.
## Please see the Warning section of &amp;#39;?getSymbols.yahoo&amp;#39; for details.
## 
## This message is shown once per session and may be disabled by setting
## options(&amp;quot;getSymbols.yahoo.warning&amp;quot;=FALSE).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/market_neutral_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/market_neutral_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;displaying-of-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Displaying of results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Drawdown period&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$dd&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         From     Trough         To   Depth Length To Trough Recovery
## 1 2015-08-10 2015-12-10 2016-05-31 -0.0724    204        87      117
## 2 2018-12-14 2018-12-31 2019-01-08 -0.0627     16        11        5
## 3 2017-01-24 2017-03-09 2017-04-13 -0.0551     57        32       25
## 4 2018-05-31 2018-07-17 2018-08-06 -0.0494     47        33       14
## 5 2018-02-26 2018-03-09 2018-04-17 -0.0349     36        10       26&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Downside risk&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$ds_risk&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   pnl
## Semi Deviation                 0.0031
## Gain Deviation                 0.0049
## Loss Deviation                 0.0041
## Downside Deviation (MAR=210%)  0.0092
## Downside Deviation (Rf=0%)     0.0030
## Downside Deviation (0%)        0.0030
## Maximum Drawdown               0.0724
## Historical VaR (95%)          -0.0071
## Historical ES (95%)           -0.0106
## Modified VaR (95%)            -0.0043
## Modified ES (95%)             -0.0043&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Returns&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$ret&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              pnl
## Annualized Return         0.0584
## Annualized Std Dev        0.0737
## Annualized Sharpe (Rf=0%) 0.7928&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R</title>
      <link>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</link>
      <pubDate>Tue, 01 Jan 2019 00:15:53 +0800</pubDate>
      
      <guid>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</guid>
      <description>

&lt;h2 id=&#34;translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r&#34;&gt;Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m really intrigued by Ernest Chan&amp;rsquo;s approach in Quant Trading.&lt;/p&gt;

&lt;p&gt;Often in the retail trading space, what &amp;lsquo;gurus&amp;rsquo; preach often sounds really dubious. But Ernest Chan is different. He&amp;rsquo;s sincere, down-to-earth and earnest (meant to be a pun here).&lt;/p&gt;

&lt;p&gt;In my first month of deploying algo trading strategies, I focus mainly on mean-reversion strategies - paricularly amongst pairs. What I learnt - with real capital - is that the hedge ratio is dynamic and will vary over time. In the early days, I fixed it through linear regression. But boy this doesn&amp;rsquo;t work! It&amp;rsquo;s not really market neutral because of the imbalance in values between pairs across time.&lt;/p&gt;

&lt;p&gt;Then I chanced upon Kalman filter - something I learnt during my AI module in my Computer Science Degree days. I&amp;rsquo;ll spare the Math here. It&amp;rsquo;s a variant of the markov model, that uses a series of measurements over time (in this case, one of the pairs price), containing noise and produces estimates of unknown (here it&amp;rsquo;s the hedge ratio and intercept). Hedge ratio is updated in each time step.&lt;/p&gt;

&lt;p&gt;I saw the Python code online for EWA-EWC pair strategy that returns a sharpe ratio of 2.4. I tried to search for a R version but to no avail!&lt;/p&gt;

&lt;p&gt;Hence I decided to spend a day translating the python code into R code (for deployment purposes. Currently my algo trading stack is built around R). Thankfully I&amp;rsquo;m not translating the Matlab version because I do not have prior experience in that. And it would definitely take me more than a day for the translation.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve since,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Wrapped the code with a function&lt;/li&gt;
&lt;li&gt;Loop through a 40 choose 2 combinations of country pairs&lt;/li&gt;
&lt;li&gt;Triangulated with distance metrics like Correlation, Euclidean Distance and Manhattan Distance&lt;/li&gt;
&lt;li&gt;Filtered out long half life (i.e. # of days before reverting to the mean)&lt;/li&gt;
&lt;li&gt;Filtered by sharpe ratios, drawdown and average profits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so far, this market-neutral strategy is really promising!&lt;/p&gt;

&lt;h3 id=&#34;translated-r-code-for-ewa-ewc-strategy&#34;&gt;Translated R code for EWA - EWC strategy&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#Note: Try to put everything in a data-frame
lapply(c(&amp;quot;zoo&amp;quot;, &amp;quot;tidyr&amp;quot;, &amp;quot;plyr&amp;quot;, &amp;quot;dplyr&amp;quot;,
         &amp;quot;gtools&amp;quot;,&amp;quot;googlesheets&amp;quot;, &amp;quot;quantmod&amp;quot;, 
         &amp;quot;urca&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;parallel&amp;quot;), require, character.only = T)

source(&#39;util/calculateReturns.R&#39;)
source(&#39;util/calculateMaxDD.R&#39;)
source(&#39;util/backshift.R&#39;)
source(&#39;util/extract_stock_prices.R&#39;)
source(&#39;util/cointegration_pair.R&#39;)


#Reading in the data
df = read.csv(&#39;kalman_filter/inputData_EWA_EWC.csv&#39;)
df = subset(df, select = c(&amp;quot;Date&amp;quot;, &amp;quot;EWC&amp;quot;, &amp;quot;EWA&amp;quot;))

# Augment x with ones to  accomodate possible offset in the regression between y vs x.
# df$EWA_ones = 1

# delta=1 gives fastest change in beta, delta=0.000....1 allows no change (like traditional linear regression).
delta = 0.0001 

#yhat=np.full(y.shape[0], np.nan) # measurement prediction
df$yhat = NA

#Initialize matrix
df$e = df$yhat # e = yhat.copy(), residuals
df$Q = df$yhat # Q = yhat.copy(), measurement variance

# For clarity, we denote R(t|t) by P(t). Initialize R, P and beta.
R = matrix(dat = rep(0, 4), nrow = 2, ncol = 2) #R = np.zeros((2,2))
P = R   #P = R.copy()

#Store beta in df and separately for computation
beta = matrix(dat = rep(NA, nrow(df) * 2), nrow = 2, ncol = nrow(df))
df$beta1 = NA; df$beta2 = NA  #beta = np.full((2, x.shape[0]), np.nan)

Vw = delta/(1-delta) * diag(2) #Vw=delta/(1-delta)*np.eye(2)
Ve = 0.001

# Initialize beta(:, 1) to zero
beta[, 1] = 0 #beta[:, 0]=0
df$beta1 = beta[1, 1]; df$beta2 = beta[2, 1]

#for t in range(len(y)):
for (t in 1:nrow(df)){
 
  if(t &amp;gt; 1){
    #Update matrix
    beta[, t] = beta[, t-1]
    R = P + Vw
    
    #Update df
    df$beta1[t] = beta[1, t]
    df$beta2[t] = beta[2, t]
  }
  
  # yhat[t, ] = as.matrix(x[t, ]) %*% as.matrix(beta[, t])    #yhat[t]=np.dot(x[t, :], beta[:, t])
  df$yhat[t] = as.matrix(data.frame(df$EWA[t], 1)) %*% as.matrix(beta[, t]) 
  
  # Q[t, ] = (as.matrix(x[t, ]) %*% R) %*% t(as.matrix(x[t, ])) + Ve  #Q[t] = np.dot(np.dot(x[t, :], R), x[t, :].T)+Ve
  df$Q[t] = (as.matrix(data.frame(df$EWA[t], 1)) %*% R) %*% t(as.matrix(data.frame(df$EWA[t], 1))) + Ve
  
  # e[t, ] = y[t, ] - yhat[t, ] #e[t]=y[t]-yhat[t] # measurement prediction error
  df$e[t] = df$EWC[t] - df$yhat[t]
  
  K = R %*% t(as.matrix(data.frame(df$EWA[t], 1))) / df$Q[t]  #K = np.dot(R, x[t, :].T)/Q[t] #  Kalman gain
  beta[, t] = beta[, t] + K %*% as.matrix(df$e[t]) #beta[:, t]=beta[:, t]+np.dot(K, e[t]) #  State update. Equation 3.11
  
  #Update df
  df$beta1[t] = beta[1, t]
  df$beta2[t] = beta[2, t]
  
  # State covariance update. Euqation 3.12
  P = R - ((K %*% as.matrix(data.frame(df$EWA[t], 1))) %*% R) #P = R-np.dot(np.dot(K, x[t, :]), R) 
}

#Generated signals
df$Q_root = df$Q ^ 0.5
df$longs &amp;lt;- df$e &amp;lt;= -df$Q ^ 0.5 # buy spread when its value drops below 2 standard deviations.
df$shorts &amp;lt;- df$e &amp;gt;= df$Q ^ 0.5  # short spread when its value rises above 2 standard deviations.  Short EWC

df$longExits  &amp;lt;- df$e &amp;gt; 0 
df$shortExits &amp;lt;- df$e &amp;lt; 0

# initialize to 0
df$numUnitsLong = NA
df$numUnitsShort = NA
df$numUnitsLong[0]=0.
df$numUnitsShort[0]=0.

df$numUnitsLong[df$longs]=1.
df$numUnitsLong[df$longsExit]=0
df$numUnitsLong = ifelse(is.na(df$numUnitsLong), 0, df$numUnitsLong)

df$numUnitsShort[df$shorts]=-1.
df$numUnitsShort[df$shortsExit]=0
df$numUnitsShort = ifelse(is.na(df$numUnitsShort), 0, df$numUnitsShort)

df$numUnits = df$numUnitsLong + df$numUnitsShort 

df$position1 = 0; df$position2 = 0 

df$position1 = ifelse(df$numUnits == -1, -1, df$position1)   #short EWC, Long EWA --&amp;gt; df$e[t] = df$EWC[t] - df$yhat[t]
df$position2 = ifelse(df$numUnits == -1, 1, df$position2)  #short EWC, Long EWA 

df$position1 = ifelse(df$numUnits == 1, 1, df$position1)   #long EWC, short EWA 
df$position2 = ifelse(df$numUnits == 1, -1, df$position2)  #long EWC, short EWA 

# df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(-df$beta1, 1)) * data.frame(df$EWA, df$EWC)   #Adjusted price
df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(1, -df$beta1)) * data.frame(df$EWC, df$EWA)   #Adjusted price

#Returns
df$dailyret1 &amp;lt;- c(NA, (df$EWC[2: nrow(df)] - df$EWC[1: (nrow(df) - 1)])/df$EWC[1: (nrow(df) - 1)])
df$dailyret2 &amp;lt;- c(NA, (df$EWA[2: nrow(df)] - df$EWA[1: (nrow(df) - 1)])/df$EWA[1: (nrow(df) - 1)])

#Daily returns
# lag(df$position1, 1)
# lag(df$position2, 1) * df$beta1
df$pnl = lag(df$positions$df.numUnits, 1) * df$dailyret1  + lag(df$positions$df.numUnits.1, 1) * df$dailyret2

df$ret = (df$pnl)/lag((df$positions$df.numUnits + df$positions$df.numUnits.1), 1)
df$ret = ifelse(is.na(df$ret), 0, df$ret)
df$ret[2] = 0

#Sharpe ratio
sqrt(252)*mean(df$pnl, na.rm = TRUE)/sd(df$pnl, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I Find Country Pairs for Mean Reversion Strategy</title>
      <link>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</link>
      <pubDate>Wed, 26 Dec 2018 12:30:03 +0800</pubDate>
      
      <guid>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</guid>
      <description>

&lt;h2 id=&#34;how-i-find-country-pairs-for-mean-reversion-strategy&#34;&gt;How I Find Country Pairs for Mean Reversion Strategy&lt;/h2&gt;

&lt;p&gt;As mentioned in my previous post &lt;a href=&#34;https://jirong-huang.netlify.com/post/research-to-production-pipeline-for-mean-reversion/&#34;&gt;here&lt;/a&gt;, the first step for a mean reversion strategy is to conduct some background quantitative research.&lt;/p&gt;

&lt;h3 id=&#34;step-1&#34;&gt;Step 1&lt;/h3&gt;

&lt;p&gt;First, I use a pair trading function to loop across 800+ country pairs (created from combination function),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pair_trading = function(stock1, stock2, trade_amount, finance_rates, start_date, end_date, 
                        prop_train, enter_z_score, exit_z_score){

## More codes here
   
## Return this
key_info = list(
  ticker = c(stock1, stock2),
  start_date = start_date,
  trade_table = data_trade,
  sharpe = c(sharpeRatioTrainset, sharpeRatioTestset),
  half_life = half_life,
  profits = data_trade_stats,
  max_drawdown = c(table.DownsideRisk(data$pnl[trainset])[1]$pnl[7], table.DownsideRisk(data$pnl[testset])[1]$pnl[7]),
  returns = cbind(table.AnnualizedReturns(data$pnl[trainset]), table.AnnualizedReturns(data$pnl[testset])),
  hedgeRatio_mean_sd = c(as.numeric(hedgeRatio), as.numeric(data_trade$spreadMean[nrow(data_trade)]), as.numeric(data_trade$spreadStd[nrow(data_trade)])),   #critical --&amp;gt;to be used in real-time trading
  close_z_score = as.numeric(data_trade$zscore[nrow(data_trade)]),
  hist_spread = data_trade$spread[(nrow(data_trade) - round(half_life) + 2):nrow(data_trade)],
  prop_days_mkt = c(prop_days_mkt_train, prop_days_mkt_test),
  close_price = c(data_trade$Close[nrow(data_trade)], data_trade$Close.1[nrow(data_trade)]),
  win_rate = c(perc_win_train, perc_win_test)
  # ,
  # chart_train = charts.PerformanceSummary(data$pnl[trainset]),
  # chart_test = charts.PerformanceSummary(data$pnl[testset])
)

return(key_info)
                        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2&#34;&gt;Step 2&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Next, I select pairs with sharpe ratio &amp;gt;1 in both training and testing periods.&lt;/li&gt;
&lt;li&gt;And also select pairs with shorter half-life i.e. shorter duration before it reverts to its mean path - more than 5 and lesser than 25&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;step-3&#34;&gt;Step 3&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then I went on to IShares website to get the respective tickers&amp;rsquo; industries&amp;rsquo; composition.
&lt;img src=&#34;/post/img/country_composition.png&#34; alt=&#34;/post/img/country_composition.png&#34;&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With this new piece of information, I went on to compute the manhattan distance, euclidean distance and correlation between these country pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next I applied percentile ranks to these distance measures and find an average percentile rank&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anything that is above 50th percentile is selected.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;distance_metrics = function(stock1, stock2){
  
  dist = c(NA, NA, NA)
  
  tryCatch({
  
  ctry_pair_composition_sub = subset(ctry_pair_composition, ctry_pair_composition$ticker == stock1 | ctry_pair_composition$ticker == stock2)
  manhattan = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;manhattan&amp;quot;))
  euclidean = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;euclidean&amp;quot;))
  correlation = cor(as.numeric(ctry_pair_composition_sub[1, -1]), as.numeric(ctry_pair_composition_sub[2, -1]))
  
  dist = c(manhattan, euclidean, correlation)
  }, error=function(e){})
  
  return(dist)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And ta-dah! This is the final selected country pairs that I will be using for my mean reversion strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/final_selection.png&#34; alt=&#34;/post/img/final_selection.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-improvement&#34;&gt;Further improvement&lt;/h3&gt;

&lt;p&gt;Note: I could have applied co-integration test. Will do it pretty soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research to Production Pipeline for Mean Reversion</title>
      <link>/post/research-to-production-pipeline-for-mean-reversion/</link>
      <pubDate>Tue, 25 Dec 2018 18:07:19 +0800</pubDate>
      
      <guid>/post/research-to-production-pipeline-for-mean-reversion/</guid>
      <description>

&lt;h2 id=&#34;research-to-production-pipeline-for-mean-reversion&#34;&gt;Research to Production Pipeline for Mean Reversion&lt;/h2&gt;

&lt;p&gt;Here is a high level overview of something that I&amp;rsquo;m working on.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been grappling with the finite state automata Event Driven Computing transitions and I kinda sorted it out for production use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/research_to_production.png&#34; alt=&#34;/post/img/research_to_production.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
