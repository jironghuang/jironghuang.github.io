<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jirong&#39;s sandbox on Jirong&#39;s sandbox</title>
    <link>/</link>
    <description>Recent content in Jirong&#39;s sandbox on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What&#39;re the returns (XIRR) for my CPFIS Portfolio</title>
      <link>/post/xirr_cpfis/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/xirr_cpfis/</guid>
      <description>


&lt;div id=&#34;whatre-the-returns-xirr-for-my-cpfis-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’re the returns (XIRR) for my CPFIS Portfolio?&lt;/h2&gt;
&lt;p&gt;Every employee in Singapore is bounded by the same set of CPF rules.&lt;/p&gt;
&lt;p&gt;As an ex-economist/ data geek who doesn’t shy away from having skin in the game. I asked myself this question back in 2015 when I was still a starry-eyed young man 2 years into the workforce - how do I set out to optimize my returns in my CPF OA with these given set of constraints,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You cannot withdraw your CPF OA till you are 55 years old&lt;/li&gt;
&lt;li&gt;You can only invest CPF OA beyond 20k&lt;/li&gt;
&lt;li&gt;And of this delta, you can only invest 35% in non ETFs instruments OR 100% of it in ETFs&lt;/li&gt;
&lt;li&gt;CPF OA returns of 2.5% per annum&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last point is something that I wish to highlight. If I choose to invest, I would have to overcome the 2.5% hurdle rate from CPF OA.&lt;/p&gt;
&lt;p&gt;Subscribing to Vanguard’s philosophy, a pioneeer &amp;amp; leader in the space of index funds - (&lt;a href=&#34;https://personal.vanguard.com/pdf/s315.pdf&#34; class=&#34;uri&#34;&gt;https://personal.vanguard.com/pdf/s315.pdf&lt;/a&gt;) at least for my CPFIS portion, they advocate that lump sum investing proves to be superior than spacing out your investments (dollar cost averaging).&lt;/p&gt;
&lt;p&gt;Since then - whenever I have 5-6k in my CPF OA beyond the 20k, I promptly allocated it to the market. In some periods, I invested with smaller amounts when POEMS brokerage offered some promotions on commission fees.&lt;/p&gt;
&lt;div id=&#34;how-did-this-strategy-fare-thus-far&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did this strategy fare thus far?&lt;/h3&gt;
&lt;p&gt;XIRR- a metric used in assessing rate of returns with a given set of cashflows - came up to be around 5.7%! Note: This figure here accounts for the dividends received over the years.&lt;/p&gt;
&lt;p&gt;Hurray! It’s more than twice the 2.5% hurdle rate in CPF OA. And even the 4% rate from SA. One could voluntarily transfer OA to SA, but you will lose the flexibility of using the OA for serving mortage in future&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My thoughts&lt;/h3&gt;
&lt;p&gt;This is still an ongoing experiment in my ‘lab’. Apparently it seems to be working well! Going forward, I will continue this strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;if-you-are-keen-in-the-technicalities-of-computing-xirr-applied-the-function-developed-by-someone-else-here-httpsgithub.comsunilveeravallixirr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;If you are keen in the technicalities of computing XIRR (applied the function developed by someone else here –&amp;gt;#&lt;a href=&#34;https://github.com/SunilVeeravalli/xirr&#34; class=&#34;uri&#34;&gt;https://github.com/SunilVeeravalli/xirr&lt;/a&gt;)…&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(source(&amp;quot;./CPFOA/F_xirr.R&amp;quot;))
suppressMessages(source(&amp;quot;./CPFOA/another_irr_eg.R&amp;quot;))
suppressPackageStartupMessages(library(tvm))

#Reading my dataset
cpf_contrib &amp;lt;- read_csv(file = &amp;quot;./CPFOA/jr_cpfis_contrib.csv&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Date = col_character(),
##   num_stocks_cpf_port = col_double(),
##   sti_px = col_double(),
##   cpf_oa_port_val = col_double(),
##   Dividends_per_share = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Formattting date
cpf_contrib$Date = as.Date(cpf_contrib$Date, format = &amp;quot;%m/%d/%y&amp;quot;)
names(cpf_contrib)[which(names(cpf_contrib) == &amp;quot;Date&amp;quot;)] = &amp;quot;dates&amp;quot; 

#including ss
cpf_contrib$net_stocks_purchase = lead(cpf_contrib$num_stocks_cpf_port - lag(cpf_contrib$num_stocks_cpf_port))

#Contributions
cpf_contrib$stock_amt_purchase = cpf_contrib$net_stocks_purchase * cpf_contrib$sti_px

#Dividends amount
cpf_contrib$dividends = cpf_contrib$num_stocks_cpf_port * cpf_contrib$Dividends_per_share 
cpf_contrib$dividends = ifelse(is.na(cpf_contrib$dividends), 0, cpf_contrib$dividends)

#Net cashflow
cpf_contrib = cpf_contrib %&amp;gt;%
                mutate(net_cash_flow = dividends - stock_amt_purchase)

#Setting final cashflow  
cpf_contrib$net_cash_flow[nrow(cpf_contrib)] = cpf_contrib$cpf_oa_port_val[nrow(cpf_contrib)]

#Computing the xirr
xirr(cpf_contrib[, c(&amp;quot;dates&amp;quot;,&amp;quot;net_cash_flow&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;XIRR is 5.775%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(xirr2(cpf_contrib$net_cash_flow, cpf_contrib$dates))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05777714&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;final-dataset-on-dividends-purchases-cashflows-portfolio-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final dataset on dividends, purchases, cashflows, portfolio values&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))

kable(cpf_contrib, caption = &amp;quot;Final data-frame of dividends, purchase, cashflows, portfolio values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Final data-frame of dividends, purchase, cashflows, portfolio values
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
dates
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num_stocks_cpf_port
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sti_px
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cpf_oa_port_val
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Dividends_per_share
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
net_stocks_purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
stock_amt_purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dividends
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
net_cash_flow
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-05-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.350
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6030.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6030.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.240
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3240.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3240.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-07-15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8372
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2970.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2832.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-08-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.850
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10564
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5700.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5700.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-09-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15486
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-10-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.920
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-11-11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15892
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2950.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2950.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-12-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.630
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18768
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-01-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.690
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16728
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1883.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1883.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19275
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.107
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1144.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
802.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-341.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.870
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21567
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1148.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1148.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-04-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.840
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22742
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2556.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2556.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-05-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24932
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1728.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1728.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-06-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1152.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1152.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-07-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1144.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1144.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-08-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.910
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29362
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2910.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
890.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2019.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-09-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32712
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2002.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2002.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-10-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-11-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35178
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-12-09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-01-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37023
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4960.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4960.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-02-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1914.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
736.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1177.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-03-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2560.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2560.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-04-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-05-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.270
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49113
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-06-22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49266
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2656.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2656.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-07-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52647
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
772.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
772.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-08-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53452
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1300.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1300.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-09-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.420
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53625
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3762.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3762.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-10-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.480
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-11-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.580
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.520
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64416
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-02-22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61952
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
932.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
932.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-03-23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.637
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60368
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-05-17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.321
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63008
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-06-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.388
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-07-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.251
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58432
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-08-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.299
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5608.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1056.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4552.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-09-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60795
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-10-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60216
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4102.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4102.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-11-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64272
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-12-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.247
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-01-11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.218
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67362
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-02-19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66332
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6382.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1153.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5228.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-03-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.210
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72388
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72388.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hosting a Flask App on Heroku</title>
      <link>/post/hosting-a-flask-app-on-heroku/</link>
      <pubDate>Thu, 28 Feb 2019 23:34:32 +0800</pubDate>
      
      <guid>/post/hosting-a-flask-app-on-heroku/</guid>
      <description>&lt;p&gt;Following the steps here &amp;ndash;&amp;gt; &lt;a href=&#34;https://realpython.com/flask-by-example-part-1-project-setup/&#34; target=&#34;_blank&#34;&gt;https://realpython.com/flask-by-example-part-1-project-setup/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I managed to deploy my python flask app in Heroku.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)


@app.route(&#39;/&#39;)
def hello():
    return &amp;quot;Hello World!&amp;quot;


@app.route(&#39;/&amp;lt;name&amp;gt;&#39;)
def hello_name(name):
    return &amp;quot;Hello {}!&amp;quot;.format(name)

if __name__ == &#39;__main__&#39;:
    app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may visit the following link &amp;ndash;&amp;gt;&lt;a href=&#34;https://jirong-stage.herokuapp.com/&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/&lt;/a&gt; &amp;amp; add a suffix to it.&lt;/p&gt;

&lt;p&gt;Example &lt;a href=&#34;https://jirong-stage.herokuapp.com/jirong&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/jirong&lt;/a&gt; &amp;amp; this will return Hello jirong!&lt;/p&gt;

&lt;p&gt;Possibilites are immense! I can easily create APIs or host dashboard here. Pretty exciting to me:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling With Replacement Through First Principles</title>
      <link>/post/sampling-with-replacement-through-first-principles/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sampling-with-replacement-through-first-principles/</guid>
      <description>

&lt;h1 id=&#34;sampling-with-replacement&#34;&gt;Sampling with replacement&lt;/h1&gt;

&lt;p&gt;Hello! It&amp;rsquo;s me once again attempting to explain things from first principles - a term popularized by Elon Musk.&lt;/p&gt;

&lt;p&gt;I will use some psudeo code - on sampling with replacement for weights - to aid my explanation.&lt;/p&gt;

&lt;p&gt;Earlier in the week, I attempted to write a simple function from scratch but I gave up after realising that it will take me more than 15 mins! Difficulties lies in the multiple switch statements in defining the intervals. Haven&amp;rsquo;t figured that part out yet.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;re definitely packages out there that does sort of stuff but this forces me to understand the underlying theory from scratch.&lt;/p&gt;

&lt;p&gt;So here is the idea, I&amp;rsquo;ve a dataset with 4 individuals, tagged to respective weights that corresponds to the population. And I wish to do a bootstrap i.e. sampling with replacement to get a sample size of N = 100&lt;/p&gt;

&lt;p&gt;See Wikipedia page on advantages of Bootstrapping &amp;ndash;&amp;gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I will do. I will first line up the individuals and find the Probability Mass Function (PMF) for each individual accounting for its weight. Second, I will add up the PMF to obtain the Cumulative Density Function (cumulative proportion)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df
id weight PMF     CDF
1  2      20%    [0, 20]
2  3      30%    (20, 50]
3  3      30%    (50, 80]
4  2      20%    (80, 100]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, I will do a loop of 100 times. At the start of each loop I will obtain a float of between 0 to 1. If the value lies between a certain range, I will add that individual to the dataset. Given that num is random, it will lie between the various ranges without order, accouting for length of the interval.&lt;/p&gt;

&lt;p&gt;Note that sampling with replacement means there&amp;rsquo;s a chance that an individual may be represented more than once in the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (i in 1:100){

  num = randint(0, 1)
  
  if(num &amp;lt; 0.2){
    add_to_new_dataset(id = 1)
  }else if (num between 0.2 to 0.5){
    add_to_new_dataset(id = 2)
  }else if (num between 0.5 to 0.8){
    add_to_new_dataset(id = 3)
  }else{
    add_to_new_dataset(id = 3)
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course! In R, you should avoid an explicit loop at all costs. The solution is to embed it in a function and use a lapply function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bootstrap_weights = function(i){

num = randint(0, 1)

  if(num &amp;lt; 0.2){
    id = 1
  }else if (num between 0.2 to 0.5){
    id = 2 
  }else if (num between 0.5 to 0.8){
    id = 3
  }else{
    id = 4
  }
  
  data_row = data[id, ]
  return (data_row)
}

bootstrapped_data = rbind.fill(lapply(1: 100, bootstrap_weights))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope this is useful!&lt;/p&gt;

&lt;h2 id=&#34;latest-developments&#34;&gt;Latest developments&lt;/h2&gt;

&lt;p&gt;Courtesy of this post here &amp;ndash;&amp;gt; &lt;a href=&#34;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the simple solution to link a value to an interval,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getValue &amp;lt;- function(x, data) {
  tmp &amp;lt;- data %&amp;gt;%
    filter(CDF1 &amp;lt;= x, x &amp;lt;= CDF2)
  return(tmp$id)
}

# Using rand function to get a list of numbers
rand_numbers &amp;lt;- c(0.1, 0.173, 0.235)
sapply(rand_numbers, getValue, data = df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a decision tree algorithm from scratch</title>
      <link>/post/building-adecision-tree-from-scratch/</link>
      <pubDate>Fri, 15 Feb 2019 13:09:44 +0800</pubDate>
      
      <guid>/post/building-adecision-tree-from-scratch/</guid>
      <description>

&lt;h2 id=&#34;building-a-decision-tree-from-scratch&#34;&gt;Building a decision tree from scratch&lt;/h2&gt;

&lt;p&gt;Sometimes to truly understand and internalise an algorithm, it&amp;rsquo;s always useful to build from scratch. Rather than relying on a module or library written by someone else.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fortunate to be given the chance to do it in 1 of my assignments for decision trees.&lt;/p&gt;

&lt;p&gt;From this exercise, I had to rely on my knowledge on recursion, binary trees (in-order traversal) and object oriented programming.&lt;/p&gt;

&lt;p&gt;Below is a snippet of method in a class. The algorithm works as follows,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First you define a leaf size i.e. the maximum number of data you allow to be left in the leaf node.&lt;/li&gt;
&lt;li&gt;If number of data in leaf node is still more than the allowable size, check if all data is the same. If it&amp;rsquo;s the same, return just 1 value&lt;/li&gt;
&lt;li&gt;Next I find the feature based on best correlation (gini coefficient and information gain works too) with the dependent variable values. Note that as you traverse down the tree, this dataset gets smaller&lt;/li&gt;
&lt;li&gt;With the best feature found in each split, I proceed to contruct the left tree and right tree together with a root node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is that if you are left with a node that&amp;rsquo;s smaller or equals to allowable leaf size, it will stop traversing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/Building_decision_tree.png&#34; alt=&#34;/post/img/Building_decision_tree.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Martingale Strategy - Double Down</title>
      <link>/post/martingale-strategy/</link>
      <pubDate>Sat, 26 Jan 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/martingale-strategy/</guid>
      <description>

&lt;h2 id=&#34;martingale-strategy&#34;&gt;Martingale Strategy&lt;/h2&gt;

&lt;p&gt;In this post, I will simulate a martingale strategy in Roulette&amp;rsquo;s context to highlight the potential risks associated with this strategy.&lt;/p&gt;

&lt;p&gt;Double down! That&amp;rsquo;s essentially the essence of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a simple explanation of the strategy,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The croupier spins the ball. If it&amp;rsquo;s red you win the amount you bet, black you lose the same amount&lt;/li&gt;
&lt;li&gt;If you win, you continue to bet the same amount (same as your 1st bet amount)&lt;/li&gt;
&lt;li&gt;If you lose, you double your bet amount&lt;/li&gt;
&lt;li&gt;And if your accumulated winnings hits a certain amount, you stop and leave the casino&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So how would the strategy fare? To explain, I will use Monte Carlo with a Bernoulli distribution for each roulette spin (X ~ B(1, 0.48)).&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-10-runs-times&#34;&gt;Simulate strategy 10 runs/ times&lt;/h3&gt;

&lt;p&gt;Here, I simulate the strategy of 10 times. Think of it in this way, there&amp;rsquo;re 10 alternate universes which you exist and you play the same game 10 times. Or you can just simply treat this as going back to the casino on 10 separate days.&lt;/p&gt;

&lt;p&gt;In the chart below, you will notice that for some runs; because of a sequence of bad luck, the losses quick spiralled!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/10_simulations.png&#34; alt=&#34;/post/img/10_simulations.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-1000-runs-times-and-compute-the-mean&#34;&gt;Simulate strategy 1000 runs/ times and compute the mean&lt;/h3&gt;

&lt;p&gt;Next, for a more robust interpretation, I went on to simulate this strategy 1000 times and computed the mean and standard deviation. You will notice that the strategy eventually converges to a desired terminal value. In this case, it&amp;rsquo;s $80. So essentially, out of 1000 simulations, all of them reaches my profit target!&lt;/p&gt;

&lt;p&gt;However the the risk is enormous. Near the average 120th run, the standard deviation sky-rocketed to 120,000. I&amp;rsquo;m unsure if anyone could stomach this loss at any one point of time. The journey matters!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/1000_simulations.png&#34; alt=&#34;/post/img/1000_simulations.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/simulation_sd.png&#34; alt=&#34;/post/img/simulation_sd.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;insights-from-this-strategy&#34;&gt;Insights from this strategy&lt;/h3&gt;

&lt;p&gt;Martingale strategy - to put it simply - is a win small, lose potentially huge strategy.&lt;/p&gt;

&lt;p&gt;In this strategy, you will win 100% of the time.&lt;/p&gt;

&lt;p&gt;But the question is, do you have the money (or infinite bankroll) to tide through tough times?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Create a Python Environment in Ubuntu or any Debian-based system</title>
      <link>/post/how-to-create-a-python-environment-in-ubuntu/</link>
      <pubDate>Wed, 09 Jan 2019 23:39:22 +0800</pubDate>
      
      <guid>/post/how-to-create-a-python-environment-in-ubuntu/</guid>
      <description>&lt;p&gt;Often, certain projects or classes involving python require a set of modules/packages for the code to work.&lt;/p&gt;

&lt;p&gt;1 solution is to create a Python Environment dedicated to that project.&lt;/p&gt;

&lt;p&gt;First set up a folder, and include a .yml file with the specific modules and environment that you wish to install. Here is an example (env.yml),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: env
channels: !!python/tuple
- !!python/unicode &#39;defaults&#39;
dependencies:
- nb_conda=2.2.0=py27_0
- python=2.7.13=0
- cycler=0.10.0
- functools32=3.2.3.2
- matplotlib=2.0.2
- numpy=1.13.1
- pandas=0.20.3
- py=1.4.34
- pyparsing=2.2.0
- pytest=3.2.1
- python-dateutil=2.6.1
- pytz=2017.2
- scipy=0.19.1
- six=1.10.0
- subprocess32=3.2.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have anaconda installed, navigate to the folder with .yml; right click and select open in terminal. Then, type the following into bash&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda env create -f env.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once installed, type the following into bash to bring up the environment,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source activate env
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you wish to install a specific program in this environment - say spyder - you can install it directly. Example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install spyder

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To open spyder program, simply type spyder into terminal. And there you go!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R</title>
      <link>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</link>
      <pubDate>Tue, 01 Jan 2019 00:15:53 +0800</pubDate>
      
      <guid>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</guid>
      <description>

&lt;h2 id=&#34;translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r&#34;&gt;Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m really intrigued by Ernest Chan&amp;rsquo;s approach in Quant Trading.&lt;/p&gt;

&lt;p&gt;Often in the retail trading space, what &amp;lsquo;gurus&amp;rsquo; preach often sounds really dubious. But Ernest Chan is different. He&amp;rsquo;s sincere, down-to-earth and earnest (meant to be a pun here).&lt;/p&gt;

&lt;p&gt;In my first month of deploying algo trading strategies, I focus mainly on mean-reversion strategies - paricularly amongst pairs. What I learnt - with real capital - is that the hedge ratio is dynamic and will vary over time. In the early days, I fixed it through linear regression. But boy this doesn&amp;rsquo;t work! It&amp;rsquo;s not really market neutral because of the imbalance in values between pairs across time.&lt;/p&gt;

&lt;p&gt;Then I chanced upon Kalman filter - something I learnt during my AI module in my Computer Science Degree days. I&amp;rsquo;ll spare the Math here. It&amp;rsquo;s a variant of the markov model, that uses a series of measurements over time (in this case, one of the pairs price), containing noise and produces estimates of unknown (here it&amp;rsquo;s the hedge ratio and intercept). Hedge ratio is updated in each time step.&lt;/p&gt;

&lt;p&gt;I saw the Python code online for EWA-EWC pair strategy that returns a sharpe ratio of 2.4. I tried to search for a R version but to no avail!&lt;/p&gt;

&lt;p&gt;Hence I decided to spend a day translating the python code into R code (for deployment purposes. Currently my algo trading stack is built around R). Thankfully I&amp;rsquo;m not translating the Matlab version because I do not have prior experience in that. And it would definitely take me more than a day for the translation.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve since,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Wrapped the code with a function&lt;/li&gt;
&lt;li&gt;Loop through a 40 choose 2 combinations of country pairs&lt;/li&gt;
&lt;li&gt;Triangulated with distance metrics like Correlation, Euclidean Distance and Manhattan Distance&lt;/li&gt;
&lt;li&gt;Filtered out long half life (i.e. # of days before reverting to the mean)&lt;/li&gt;
&lt;li&gt;Filtered by sharpe ratios, drawdown and average profits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so far, this market-neutral strategy is really promising!&lt;/p&gt;

&lt;h3 id=&#34;translated-r-code-for-ewa-ewc-strategy&#34;&gt;Translated R code for EWA - EWC strategy&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#Note: Try to put everything in a data-frame
lapply(c(&amp;quot;zoo&amp;quot;, &amp;quot;tidyr&amp;quot;, &amp;quot;plyr&amp;quot;, &amp;quot;dplyr&amp;quot;,
         &amp;quot;gtools&amp;quot;,&amp;quot;googlesheets&amp;quot;, &amp;quot;quantmod&amp;quot;, 
         &amp;quot;urca&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;parallel&amp;quot;), require, character.only = T)

source(&#39;util/calculateReturns.R&#39;)
source(&#39;util/calculateMaxDD.R&#39;)
source(&#39;util/backshift.R&#39;)
source(&#39;util/extract_stock_prices.R&#39;)
source(&#39;util/cointegration_pair.R&#39;)


#Reading in the data
df = read.csv(&#39;kalman_filter/inputData_EWA_EWC.csv&#39;)
df = subset(df, select = c(&amp;quot;Date&amp;quot;, &amp;quot;EWC&amp;quot;, &amp;quot;EWA&amp;quot;))

# Augment x with ones to  accomodate possible offset in the regression between y vs x.
# df$EWA_ones = 1

# delta=1 gives fastest change in beta, delta=0.000....1 allows no change (like traditional linear regression).
delta = 0.0001 

#yhat=np.full(y.shape[0], np.nan) # measurement prediction
df$yhat = NA

#Initialize matrix
df$e = df$yhat # e = yhat.copy(), residuals
df$Q = df$yhat # Q = yhat.copy(), measurement variance

# For clarity, we denote R(t|t) by P(t). Initialize R, P and beta.
R = matrix(dat = rep(0, 4), nrow = 2, ncol = 2) #R = np.zeros((2,2))
P = R   #P = R.copy()

#Store beta in df and separately for computation
beta = matrix(dat = rep(NA, nrow(df) * 2), nrow = 2, ncol = nrow(df))
df$beta1 = NA; df$beta2 = NA  #beta = np.full((2, x.shape[0]), np.nan)

Vw = delta/(1-delta) * diag(2) #Vw=delta/(1-delta)*np.eye(2)
Ve = 0.001

# Initialize beta(:, 1) to zero
beta[, 1] = 0 #beta[:, 0]=0
df$beta1 = beta[1, 1]; df$beta2 = beta[2, 1]

#for t in range(len(y)):
for (t in 1:nrow(df)){
 
  if(t &amp;gt; 1){
    #Update matrix
    beta[, t] = beta[, t-1]
    R = P + Vw
    
    #Update df
    df$beta1[t] = beta[1, t]
    df$beta2[t] = beta[2, t]
  }
  
  # yhat[t, ] = as.matrix(x[t, ]) %*% as.matrix(beta[, t])    #yhat[t]=np.dot(x[t, :], beta[:, t])
  df$yhat[t] = as.matrix(data.frame(df$EWA[t], 1)) %*% as.matrix(beta[, t]) 
  
  # Q[t, ] = (as.matrix(x[t, ]) %*% R) %*% t(as.matrix(x[t, ])) + Ve  #Q[t] = np.dot(np.dot(x[t, :], R), x[t, :].T)+Ve
  df$Q[t] = (as.matrix(data.frame(df$EWA[t], 1)) %*% R) %*% t(as.matrix(data.frame(df$EWA[t], 1))) + Ve
  
  # e[t, ] = y[t, ] - yhat[t, ] #e[t]=y[t]-yhat[t] # measurement prediction error
  df$e[t] = df$EWC[t] - df$yhat[t]
  
  K = R %*% t(as.matrix(data.frame(df$EWA[t], 1))) / df$Q[t]  #K = np.dot(R, x[t, :].T)/Q[t] #  Kalman gain
  beta[, t] = beta[, t] + K %*% as.matrix(df$e[t]) #beta[:, t]=beta[:, t]+np.dot(K, e[t]) #  State update. Equation 3.11
  
  #Update df
  df$beta1[t] = beta[1, t]
  df$beta2[t] = beta[2, t]
  
  # State covariance update. Euqation 3.12
  P = R - ((K %*% as.matrix(data.frame(df$EWA[t], 1))) %*% R) #P = R-np.dot(np.dot(K, x[t, :]), R) 
}

#Generated signals
df$Q_root = df$Q ^ 0.5
df$longs &amp;lt;- df$e &amp;lt;= -df$Q ^ 0.5 # buy spread when its value drops below 2 standard deviations.
df$shorts &amp;lt;- df$e &amp;gt;= df$Q ^ 0.5  # short spread when its value rises above 2 standard deviations.  Short EWC

df$longExits  &amp;lt;- df$e &amp;gt; 0 
df$shortExits &amp;lt;- df$e &amp;lt; 0

# initialize to 0
df$numUnitsLong = NA
df$numUnitsShort = NA
df$numUnitsLong[0]=0.
df$numUnitsShort[0]=0.

df$numUnitsLong[df$longs]=1.
df$numUnitsLong[df$longsExit]=0
df$numUnitsLong = ifelse(is.na(df$numUnitsLong), 0, df$numUnitsLong)

df$numUnitsShort[df$shorts]=-1.
df$numUnitsShort[df$shortsExit]=0
df$numUnitsShort = ifelse(is.na(df$numUnitsShort), 0, df$numUnitsShort)

df$numUnits = df$numUnitsLong + df$numUnitsShort 

df$position1 = 0; df$position2 = 0 

df$position1 = ifelse(df$numUnits == -1, -1, df$position1)   #short EWC, Long EWA --&amp;gt; df$e[t] = df$EWC[t] - df$yhat[t]
df$position2 = ifelse(df$numUnits == -1, 1, df$position2)  #short EWC, Long EWA 

df$position1 = ifelse(df$numUnits == 1, 1, df$position1)   #long EWC, short EWA 
df$position2 = ifelse(df$numUnits == 1, -1, df$position2)  #long EWC, short EWA 

# df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(-df$beta1, 1)) * data.frame(df$EWA, df$EWC)   #Adjusted price
df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(1, -df$beta1)) * data.frame(df$EWC, df$EWA)   #Adjusted price

#Returns
df$dailyret1 &amp;lt;- c(NA, (df$EWC[2: nrow(df)] - df$EWC[1: (nrow(df) - 1)])/df$EWC[1: (nrow(df) - 1)])
df$dailyret2 &amp;lt;- c(NA, (df$EWA[2: nrow(df)] - df$EWA[1: (nrow(df) - 1)])/df$EWA[1: (nrow(df) - 1)])

#Daily returns
# lag(df$position1, 1)
# lag(df$position2, 1) * df$beta1
df$pnl = lag(df$positions$df.numUnits, 1) * df$dailyret1  + lag(df$positions$df.numUnits.1, 1) * df$dailyret2

df$ret = (df$pnl)/lag((df$positions$df.numUnits + df$positions$df.numUnits.1), 1)
df$ret = ifelse(is.na(df$ret), 0, df$ret)
df$ret[2] = 0

#Sharpe ratio
sqrt(252)*mean(df$pnl, na.rm = TRUE)/sd(df$pnl, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I Find Country Pairs for Mean Reversion Strategy</title>
      <link>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</link>
      <pubDate>Wed, 26 Dec 2018 12:30:03 +0800</pubDate>
      
      <guid>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</guid>
      <description>

&lt;h2 id=&#34;how-i-find-country-pairs-for-mean-reversion-strategy&#34;&gt;How I Find Country Pairs for Mean Reversion Strategy&lt;/h2&gt;

&lt;p&gt;As mentioned in my previous post &lt;a href=&#34;https://jirong-huang.netlify.com/post/research-to-production-pipeline-for-mean-reversion/&#34;&gt;here&lt;/a&gt;, the first step for a mean reversion strategy is to conduct some background quantitative research.&lt;/p&gt;

&lt;h3 id=&#34;step-1&#34;&gt;Step 1&lt;/h3&gt;

&lt;p&gt;First, I use a pair trading function to loop across 800+ country pairs (created from combination function),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pair_trading = function(stock1, stock2, trade_amount, finance_rates, start_date, end_date, 
                        prop_train, enter_z_score, exit_z_score){

## More codes here
   
## Return this
key_info = list(
  ticker = c(stock1, stock2),
  start_date = start_date,
  trade_table = data_trade,
  sharpe = c(sharpeRatioTrainset, sharpeRatioTestset),
  half_life = half_life,
  profits = data_trade_stats,
  max_drawdown = c(table.DownsideRisk(data$pnl[trainset])[1]$pnl[7], table.DownsideRisk(data$pnl[testset])[1]$pnl[7]),
  returns = cbind(table.AnnualizedReturns(data$pnl[trainset]), table.AnnualizedReturns(data$pnl[testset])),
  hedgeRatio_mean_sd = c(as.numeric(hedgeRatio), as.numeric(data_trade$spreadMean[nrow(data_trade)]), as.numeric(data_trade$spreadStd[nrow(data_trade)])),   #critical --&amp;gt;to be used in real-time trading
  close_z_score = as.numeric(data_trade$zscore[nrow(data_trade)]),
  hist_spread = data_trade$spread[(nrow(data_trade) - round(half_life) + 2):nrow(data_trade)],
  prop_days_mkt = c(prop_days_mkt_train, prop_days_mkt_test),
  close_price = c(data_trade$Close[nrow(data_trade)], data_trade$Close.1[nrow(data_trade)]),
  win_rate = c(perc_win_train, perc_win_test)
  # ,
  # chart_train = charts.PerformanceSummary(data$pnl[trainset]),
  # chart_test = charts.PerformanceSummary(data$pnl[testset])
)

return(key_info)
                        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2&#34;&gt;Step 2&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Next, I select pairs with sharpe ratio &amp;gt;1 in both training and testing periods.&lt;/li&gt;
&lt;li&gt;And also select pairs with shorter half-life i.e. shorter duration before it reverts to its mean path - more than 5 and lesser than 25&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;step-3&#34;&gt;Step 3&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then I went on to IShares website to get the respective tickers&amp;rsquo; industries&amp;rsquo; composition.
&lt;img src=&#34;/post/img/country_composition.png&#34; alt=&#34;/post/img/country_composition.png&#34;&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With this new piece of information, I went on to compute the manhattan distance, euclidean distance and correlation between these country pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next I applied percentile ranks to these distance measures and find an average percentile rank&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anything that is above 50th percentile is selected.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;distance_metrics = function(stock1, stock2){
  
  dist = c(NA, NA, NA)
  
  tryCatch({
  
  ctry_pair_composition_sub = subset(ctry_pair_composition, ctry_pair_composition$ticker == stock1 | ctry_pair_composition$ticker == stock2)
  manhattan = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;manhattan&amp;quot;))
  euclidean = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;euclidean&amp;quot;))
  correlation = cor(as.numeric(ctry_pair_composition_sub[1, -1]), as.numeric(ctry_pair_composition_sub[2, -1]))
  
  dist = c(manhattan, euclidean, correlation)
  }, error=function(e){})
  
  return(dist)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And ta-dah! This is the final selected country pairs that I will be using for my mean reversion strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/final_selection.png&#34; alt=&#34;/post/img/final_selection.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-improvement&#34;&gt;Further improvement&lt;/h3&gt;

&lt;p&gt;Note: I could have applied co-integration test. Will do it pretty soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research to Production Pipeline for Mean Reversion</title>
      <link>/post/research-to-production-pipeline-for-mean-reversion/</link>
      <pubDate>Tue, 25 Dec 2018 18:07:19 +0800</pubDate>
      
      <guid>/post/research-to-production-pipeline-for-mean-reversion/</guid>
      <description>

&lt;h2 id=&#34;research-to-production-pipeline-for-mean-reversion&#34;&gt;Research to Production Pipeline for Mean Reversion&lt;/h2&gt;

&lt;p&gt;Here is a high level overview of something that I&amp;rsquo;m working on.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been grappling with the finite state automata Event Driven Computing transitions and I kinda sorted it out for production use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/research_to_production.png&#34; alt=&#34;/post/img/research_to_production.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prototype Pair Trading Strategy for Silver ETFs</title>
      <link>/post/prototype-of-pair-trading-strategy-for-silver-etfs/</link>
      <pubDate>Tue, 18 Dec 2018 13:03:44 +0800</pubDate>
      
      <guid>/post/prototype-of-pair-trading-strategy-for-silver-etfs/</guid>
      <description>&lt;p&gt;In these 2 weeks, I&amp;rsquo;ll deploy my pair trading algo strategy into my server.&lt;/p&gt;

&lt;p&gt;I modified the code below from a renowned quant trader, Ernest Chan. The basic idea is to find z-scores through moving average &amp;amp; moving SD of spread. If it&amp;rsquo;s more than absolute of z-score, I will either short or long the spread depending on the polarity.&lt;/p&gt;

&lt;p&gt;In the backtesting below (using a pair of silver ETFs as an example), I assumed a hypothetical amount of 10,000 dollars per trade.&lt;/p&gt;

&lt;p&gt;Results are pretty good with a healthy sharpe ratio of 2.7 in the training set and 1.6 in the testing set of data. Annualized return is approximately 26% (translates to 2600 dollars) for the test set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/equity_curve.png&#34; alt=&#34;/post/img/equity_curve.png&#34;&gt;
&lt;img src=&#34;/post/img/summary_stats.png&#34; alt=&#34;/post/img/summary_stats.png&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rm(list=ls()) # clear workspace

library(&#39;zoo&#39;)
library(&amp;quot;tidyr&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
require(&amp;quot;quantmod&amp;quot;)
require(&amp;quot;urca&amp;quot;)
require(&amp;quot;PerformanceAnalytics&amp;quot;)
source(&#39;R/util/calculateReturns.R&#39;)
source(&#39;R/util/calculateMaxDD.R&#39;)
source(&#39;R/util/backshift.R&#39;)
source(&#39;R/util/extract_stock_prices.R&#39;)

#List of silver etfs, SIVR, USV, SLV, DBS
stock1 = &amp;quot;SIVR&amp;quot;
stock2 = &amp;quot;USV&amp;quot;

start_date = &amp;quot;2014-12-30&amp;quot;
end_date = &amp;quot;2018-12-30&amp;quot;

prop_train = 0.65
enter_z_score = 2     #Can use nlmb to vary
exit_z_score = 1     #Can use nlmb to vary

trade_amount = 10000
finance_rates = 2.5/100

data1 = df_crawl_time_series(stock1, start_date, end_date)
data1 = subset(data1, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;))
data1$Date = as.Date(data1$Date)

data2 = df_crawl_time_series(stock2, start_date, end_date)
data2 = subset(data2, select = c(&amp;quot;Date&amp;quot;, &amp;quot;Open&amp;quot;, &amp;quot;Close&amp;quot;))
data2$Date = as.Date(data2$Date)

data1 = xts(data1[, -1], order.by = data1[, 1])
data2 = xts(data2[, -1], order.by = data2[, 1])

data = merge(data1, data2)
data = as.data.frame(data)
data = subset(data, !is.na(data$Close) &amp;amp; !is.na(data$Close.1))

#  define indices for training and test sets
trainset &amp;lt;- 1:as.integer(nrow(data) * prop_train)
testset &amp;lt;- (length(trainset)+1):nrow(data)

#Cointegration test--&amp;gt;See if test of r&amp;lt;=1 &amp;gt; threshold. If more cointegrating
jotest=ca.jo(data.frame(data$Close[trainset], data$Close.1[trainset]), type=&amp;quot;trace&amp;quot;, K=2, ecdet=&amp;quot;none&amp;quot;, spec=&amp;quot;longrun&amp;quot;)
summary(jotest)

is_coint = jotest@teststat[1] &amp;gt; jotest@cval[1,3]
if(is_coint){
  print(&amp;quot;This pair&#39;s training set is cointegrating&amp;quot;)
}else{
  print(&amp;quot;This pair&#39;s training set is not cointegrating&amp;quot;)  
}

#Hedge ratio
result &amp;lt;- lm(data$Close[trainset] ~ 0 + data$Close.1[trainset])
hedgeRatio &amp;lt;- coef(result) # 1.631

#Spread
data$spread &amp;lt;- data$Close - hedgeRatio * data$Close.1

##########################Calculate half life#############################
# Calculate half life of mean reversion (residuals)
# Calculate yt-1 and (yt-1-yt)
# pull residuals to a vector
spread_train = data$spread[trainset]
y.lag &amp;lt;- c(spread_train[2:length(spread_train)], 0) # Set vector to lag -1 day
y.lag &amp;lt;- y.lag[1:length(y.lag)-1] # As shifted vector by -1, remove anomalous element at end of vector
spread_train &amp;lt;- spread_train[1:length(spread_train)-1] # Make vector same length as vector y.lag
y.diff &amp;lt;- spread_train - y.lag # Subtract todays close from yesterdays close
y.diff &amp;lt;- y.diff [1:length(y.diff)-1] # Make vector same length as vector y.lag
prev.y.mean &amp;lt;- y.lag - mean(y.lag, na.rm = T) # Subtract yesterdays close from the mean of lagged differences
prev.y.mean &amp;lt;- prev.y.mean [1:length(prev.y.mean )-1] # Make vector same length as vector y.lag
final.df &amp;lt;- data.frame(y.diff,prev.y.mean) # Create final data frame

# Linear Regression With Intercept
result &amp;lt;- lm(y.diff ~ prev.y.mean, data = final.df)
half_life &amp;lt;- -log(2)/coef(result)[2]   #Looking at this to 

if(half_life &amp;lt; 3){
  half_life = 14
}

######################MA of Spread#################################
#Change this to half life for lookback--&amp;gt;https://flare9xblog.com/2017/11/02/pairs-trading-testing-for-conintergration-adf-johansen-test-half-life-of-mean-reversion/
#Try EMA too
data$spread = zoo::na.locf(data$spread)
data$spreadMean &amp;lt;- SMA(data$spread, round(half_life))
data$spreadStd &amp;lt;- runSD(data$spread, n = round(half_life), sample = TRUE, cumulative = FALSE)

# data$spreadMean &amp;lt;- mean(data$spread[trainset], na.rm = T)
# data$spreadStd &amp;lt;- sd(data$spread[trainset], na.rm = T)

data$zscore = (data$spread - data$spreadMean)/data$spreadStd

data$longs &amp;lt;- data$zscore &amp;lt;= -enter_z_score # buy spread when its value drops below 2 standard deviations.
data$shorts &amp;lt;- data$zscore &amp;gt;= enter_z_score # short spread when its value rises above 2 standard deviations.

#  exit any spread position when its value is within 1 standard deviation of its mean.
data$longExits   &amp;lt;- data$zscore &amp;gt;= -exit_z_score 
data$shortExits &amp;lt;- data$zscore &amp;lt;= exit_z_score 

#Signal
data$posL1 = NA
data$posL2 = NA
data$posS1 = NA
data$posS2 = NA

# initialize to 0
data$posL1[1] &amp;lt;- 0; data$posL2[1] &amp;lt;- 0
data$posS1[1] &amp;lt;- 0; data$posS2[1] &amp;lt;- 0

data$posL1[data$longs] &amp;lt;- 1
data$posL2[data$longs] &amp;lt;- -1

data$posS1[data$shorts] &amp;lt;- -1
data$posS2[data$shorts] &amp;lt;- 1

data$posL1[data$longExits] &amp;lt;- 0
data$posL2[data$longExits] &amp;lt;- 0
data$posS1[data$shortExits] &amp;lt;- 0
data$posS2[data$shortExits] &amp;lt;- 0

#positions
data$posL1 &amp;lt;- zoo::na.locf(data$posL1); data$posL2 &amp;lt;- zoo::na.locf(data$posL2)
data$posS1 &amp;lt;- zoo::na.locf(data$posS1); data$posS2 &amp;lt;- zoo::na.locf(data$posS2)
data$position1 &amp;lt;- data$posL1 + data$posS1
# data$position1 = -data$position1    #Don&#39;t know why. It should be flipped!!!

data$position2 &amp;lt;- data$posL2 + data$posS2
# data$position2 = -data$position2    #Don&#39;t know why. It should be flipped!!!

#Returns
data$dailyret1 &amp;lt;- ROC(data$Close) #  last row is [385,] -0.0122636689 -0.0140365802
data$dailyret2 &amp;lt;- ROC(data$Close.1) #  last row is [385,] -0.0122636689 -0.0140365802

#Backshifting here. But signal is for following day returns!. So can still use latest Z-score
data$date = as.Date(row.names(data))
data = xts(data[,-which(names(data) == &amp;quot;date&amp;quot;)], order.by = data[, which(names(data) == &amp;quot;date&amp;quot;)])

data$pnl = lag(data$position1, 1) * data$dailyret1  + lag(data$position2, 1) * data$dailyret2

#Sharpe ratio
sharpeRatioTrainset &amp;lt;- sqrt(252)*mean(data$pnl[trainset], na.rm = TRUE)/sd(data$pnl[trainset], na.rm = TRUE)
sharpeRatioTrainset

sharpeRatioTestset &amp;lt;- sqrt(252)*mean(data$pnl[testset], na.rm = TRUE)/sd(data$pnl[testset], na.rm = TRUE)
sharpeRatioTestset 

#Performance analytics
charts.PerformanceSummary(data$pnl[testset])
table.Drawdowns(data$pnl[testset])
table.DownsideRisk(data$pnl[testset])
table.AnnualizedReturns(data$pnl[testset])

#Number of days not in the market
sum(data$pnl == 0, na.rm = T)/length(data$pnl)

#Putting a trade indicator
data$trade_indicator = lag(ifelse(data$position2 != 0 &amp;amp; !is.na(data$position2), 1, 0))

#Putting a unique id
count = 0
data$trade_id = NA

for(i in 2:nrow(data)){ 
  if(as.numeric(data$trade_indicator[i-1]) == 0 &amp;amp; as.numeric(data$trade_indicator[i]) != 0){
    count = count + 1
    data$trade_id[i] = count
  }else if(as.numeric(data$trade_indicator[i-1]) != 0 &amp;amp; as.numeric(data$trade_indicator[i]) != 0){
    data$trade_id[i] = count
  }
}

#Simple trade statistics
data$test = 0; data$test[testset] = 1 
data$pnl_add1 = data$pnl + 1
data_trade = as.data.frame(data)
data_trade_stats = data_trade %&amp;gt;%
  group_by(trade_id, test) %&amp;gt;%
  summarize(trade_duration = n(),
            cum_pnl = prod(pnl_add1, na.rm = T))

data_trade_stats$cum_pnl = data_trade_stats$cum_pnl - 1
data_trade_stats$profit_per_trade = data_trade_stats$cum_pnl * trade_amount

#Financing charges --&amp;gt;Depends on length of days
data_trade_stats$finance_fees =  trade_amount * finance_rates * (data_trade_stats$trade_duration/365)

#Commission fees
data_trade_stats$comm_fess = 4  #2 for 1 pair

#Net profit
data_trade_stats$profit_per_trade_less_comms = data_trade_stats$profit_per_trade - data_trade_stats$finance_fees - data_trade_stats$comm_fess

#Average loss
data_trade_stats = data_trade_stats[-which(is.na(data_trade_stats)), ]

data_trade_stats %&amp;gt;%
  group_by(test) %&amp;gt;%
  summarize(sum_profits = sum(profit_per_trade_less_comms), 
            mean_profits = mean(profit_per_trade_less_comms),
            na.rm = T)

sum(data_trade_stats$profit_per_trade_less_comms &amp;lt; 0)/ nrow(data_trade_stats)

summary(data_trade_stats$profit_per_trade_less_comms)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Summary of My Computational Photography Module From Georgia Tech Computer Science Masters</title>
      <link>/post/summary-of-my-computational-photography-from-georgia-tech-computer-science-masters/</link>
      <pubDate>Sat, 08 Dec 2018 01:11:52 +0800</pubDate>
      
      <guid>/post/summary-of-my-computational-photography-from-georgia-tech-computer-science-masters/</guid>
      <description>&lt;p&gt;For what&amp;rsquo;s worth, here is a summary of what I went through for my Georgia Tech Computer Science Msc Computational Photography module.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s really painful but rewarding!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/CP_1.png&#34; alt=&#34;/post/img/CP_1.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_2.png&#34; alt=&#34;/post/img/CP_2.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_3.png&#34; alt=&#34;/post/img/CP_3.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_4.png&#34; alt=&#34;/post/img/CP_4.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_5.png&#34; alt=&#34;/post/img/CP_5.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_6.png&#34; alt=&#34;/post/img/CP_6.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_7.png&#34; alt=&#34;/post/img/CP_7.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_8.png&#34; alt=&#34;/post/img/CP_8.png&#34;&gt;
&lt;img src=&#34;/post/img/CP_9.png&#34; alt=&#34;/post/img/CP_9.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Colorization</title>
      <link>/post/colorization/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/colorization/</guid>
      <description>

&lt;h2 id=&#34;colorization&#34;&gt;Colorization&lt;/h2&gt;

&lt;p&gt;The following is a high level project pipeline of my Computational Photography Colorization report. The project scope involves minimizing a quadratic cost function. An artist would only need to make a few colour scribble on a grey photograph and the algorithm will automatically populate the entire photograph with the associated colours.&lt;/p&gt;

&lt;p&gt;1.Input: I first read in the image using imread function.&lt;/p&gt;

&lt;p&gt;2.Find the difference: Next I compute the difference between the marked and grey scale image. This would feed into step 5.&lt;/p&gt;

&lt;p&gt;3.Transform to YIQ space: Then I convert the grey image and the marked version from RGB space to YIQ space 2 &amp;amp; 3 . I wrote a function, rgbToyiq in color_space.py to convert rgb dimension to that of YIQ.&lt;/p&gt;

&lt;p&gt;4.Compute weight matrix:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The next step, also the most complicated one is to compute the weight matrix.&lt;/li&gt;
&lt;li&gt;I first initialize 3 matrices of size height X width X size of window (9): row indices (i, j count), colIndices and values (weights) to hold key information during the loop&lt;/li&gt;
&lt;li&gt;The algo will loop through each pixel. And it will compute the weights (using marked) according to formula below in a window of size 9 i.e. 9 pixels in a window (including the pixel).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5.Solve Ax = B: Once the weights are obtained, I proceed to obtain a least square solution.&lt;/p&gt;

&lt;p&gt;6.Lastly, I transform the YIQ output back to RGB space.&lt;/p&gt;

&lt;p&gt;Here are the photographs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/baby.bmp&#34; alt=&#34;/post/img/baby.bmp&#34;&gt;
&lt;img src=&#34;/post/img/baby_marked.bmp&#34; alt=&#34;/post/img/baby_marked.bmp&#34;&gt;
&lt;img src=&#34;/post/img/baby_colorized.bmp&#34; alt=&#34;/post/img/baby_colorized.bmp&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Architecture and Process Flow for My Algorithmic Trading</title>
      <link>/post/architecture-and-process-flow-for-my-algorithmic-trading/</link>
      <pubDate>Sun, 04 Nov 2018 10:19:13 +0800</pubDate>
      
      <guid>/post/architecture-and-process-flow-for-my-algorithmic-trading/</guid>
      <description>

&lt;h2 id=&#34;project-that-i-will-be-working-in-2018-2019&#34;&gt;Project that I will be working in 2018-2019&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/mvp_algo_trading.png&#34; alt=&#34;/post/img/mvp_algo_trading.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seam Carving</title>
      <link>/post/seam-carving/</link>
      <pubDate>Thu, 25 Oct 2018 13:23:23 +0800</pubDate>
      
      <guid>/post/seam-carving/</guid>
      <description>

&lt;h2 id=&#34;snippet-of-my-seam-carving-report-from-my-msc-computer-science-georgia-tech-s-computational-photography-module&#34;&gt;Snippet of my Seam Carving Report from my Msc Computer Science Georgia Tech&amp;rsquo;s Computational Photography module&lt;/h2&gt;

&lt;p&gt;Besides removing of streams, we can also add streams. We identify k streams for removal and duplicate by averaging the left and right neighbours. The computation of these averages is done by convolving the following matrix with the images’ colour channels.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kernel = np.array([[0, 0, 0],
         [0.5, 0, 0.5],
         [0, 0, 0]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the implementation of my scaling_up algorithm, I first remove k streams (depending on ratio set by user) and recorded the coordinates and cumulative energy values of the original picture in each removal.&lt;/p&gt;

&lt;p&gt;Then I reverse the whole process by adding the stream back together with the averaged values of neighbours&lt;/p&gt;

&lt;p&gt;I implemented this scaling_up algorithn for the dolphin pictures.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8(a) is the original picture&lt;/li&gt;
&lt;li&gt;8&amp;copy; Enlarged picture with added streams: python main.py fig8 u c 1.5 y&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;8(d) Enalrged picture without added streams: python main.py fig8 u c 1.5 n&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;8(f) Enlarged picture with scaling up algorithm implemented twice: python main.py fig8_processed u c 1.5 n&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 8(a), 8&amp;copy;, 8(d), (f)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/seam_carving.img.png&#34; alt=&#34;/post/img/seam_carving.img.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R package: Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects</title>
      <link>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</link>
      <pubDate>Thu, 25 Oct 2018 11:39:24 +0800</pubDate>
      
      <guid>/post/decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects/</guid>
      <description>

&lt;h2 id=&#34;decomposing-a-position-into-exchange-rate-and-non-exchange-rate-effects&#34;&gt;Decomposing a Position Into Exchange Rate and Non Exchange Rate Effects&lt;/h2&gt;

&lt;p&gt;If you are someone with a stake in foreign positions, this package I wrote here may be a useful tool to help you understand the impact of foreign currency on your positions. For instance,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you are an investor, you may use it to analyze impact of exchange rate on your investment positions.&lt;/li&gt;
&lt;li&gt;If you are in the treasury department, you may wish to analyze the impact of exchange rates on your bonds.&lt;/li&gt;
&lt;li&gt;If you are in the finance department, you could analyze the exchange rate impact on your foreign revenue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To start using this package, you may first install the devtools package and execute the following command. install_github(&amp;ldquo;jironghuang/RemoveExchangeRateEffects&amp;rdquo;). The R documentation is as follows,&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/exch_documentation1.png&#34; alt=&#34;/post/img/exch_documentation1.png&#34;&gt;
&lt;img src=&#34;/post/img/exch_documentation2.png&#34; alt=&#34;/post/img/exch_documentation2.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;You may follow the 2 examples to better understand how this package works.&lt;/p&gt;

&lt;h3 id=&#34;example-1&#34;&gt;Example 1&lt;/h3&gt;

&lt;p&gt;In summary, what the example does below is to decompose 1 instrument position in SGD (column value) - from the perspective of someone staying in Singapore - into local static value (i.e if I keep the exchange rate constant at the start of the period) and the residual exchange rate impact.&lt;/p&gt;

&lt;p&gt;If you look at the value at the end of the period (Oct 2018), you would notice that the value in SGD fell from 331 to 261. From the perspective of a Singaporean local - through this package -  we can understand that the appreciation in USD negate the fall in value by 4 SGD.&lt;/p&gt;

&lt;p&gt;If you are an economist, you could have considered the exchange rate elasticities. But let&amp;rsquo;s ignore that for now.&lt;/p&gt;

&lt;h3 id=&#34;quick-example-1-in-r-codes-decomposing-a-single-instrument&#34;&gt;Quick example 1 in R codes (decomposing a single instrument)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;jironghuang/RemoveExchangeRateEffects&amp;quot;)
library(RemoveExchangeRateEffects)

sp_exch_rate_pair = &amp;quot;USDSGD=X&amp;quot;  #exchange rate pair. e.g &amp;quot;USDSGD=X&amp;quot;. &amp;quot;&amp;lt;Foreign_currency&amp;gt;&amp;lt;local_currency&amp;gt;=X&amp;quot;

ap_start_date &amp;lt;- as.Date(&amp;quot;2017-10-01&amp;quot;)  #starting date of portfolio e.g. 2017-10-01
ap_end_date &amp;lt;- as.Date(&amp;quot;2020-10-01&amp;quot;) #ending date of portfolio e.g. 2020-10-01. If you include a date beyond current date, the function will use the current date instead
np_mthly_yearly = &amp;quot;monthly&amp;quot;  #alternatively this could be &amp;quot;yearly&amp;quot;&amp;quot;

data(eg_dat) #example dataset that I included in this package
dp_dates_investment_value = instrument
o_exchRate_effect &amp;lt;- exchange_rate_decomposition(sp_exch_rate_pair, ap_start_date, ap_end_date, np_mthly_yearly, dp_dates_investment_value)
o_exchRate_effect$get_portfolio()

    value_in_sgd exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53   1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85   1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35   1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31   1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06   1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13   1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90   1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73   1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95   1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14   1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66   1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77   1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95   1.38061  189.0107           257.0734        3.8766087

o_exchRate_effect$get_diff_portfolio_value()
[1] 3.8766087
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;example-2&#34;&gt;Example 2&lt;/h3&gt;

&lt;p&gt;The second example builds upon the first. I&amp;rsquo;ve expanded the previous function to decompose multiple instruments at once.&lt;/p&gt;

&lt;p&gt;get_full_decomposition() returns a list of data frames with the decompositions.&lt;/p&gt;

&lt;p&gt;But before you implement the function above, you would have to add the information via the mutator functions as shown in the example below (the functions with the prefix set)&lt;/p&gt;

&lt;h3 id=&#34;quick-example-2-in-r-codes-decomposing-multiple-instruments-at-1-time&#34;&gt;Quick example 2 in R codes (Decomposing multiple instruments at 1 time)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;data(eg_dat)

er = c(&amp;quot;USDSGD=X&amp;quot;, &amp;quot;GBPSGD=X&amp;quot;)
start_date = c(&amp;quot;2017-10-01&amp;quot;, &amp;quot;2017-10-01&amp;quot;)
end_date = c(&amp;quot;2020-10-01&amp;quot;, &amp;quot;2020-10-01&amp;quot;)
freq = c(&amp;quot;monthly&amp;quot;, &amp;quot;monthly&amp;quot;)
dat = list(eg_dat, eg_dat)


o_exchRate_effect &amp;lt;- multiple_exchange_rate_decomposition(2)
o_exchRate_effect$set_sa_exch_rate_pair(er) #adding an array of exchange rate pairs
o_exchRate_effect$set_sa_start_date(start_date) #adding an array of starting dates
o_exchRate_effect$set_sa_end_date(end_date) #adding an array of ending dates
o_exchRate_effect$set_sa_mthly_yearly(freq) #adding an array of &amp;quot;monthly&amp;quot; or &amp;quot;yearly&amp;quot; option
o_exchRate_effect$set_dl_dates_investment_value(dat) #adding list of data frames
o_exchRate_effect$get_full_decomposition()  #carry out decomposition and obtain a list of data frames

[[1]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.36010  243.7541           331.5300        0.0000000
Nov 2017 308.85       1.34670  229.3384           311.9231       -3.0731344
Dec 2017 311.35       1.33780  232.7328           316.5399       -5.1899425
Jan 2018 354.31       1.31168  270.1192           367.3892      -13.0791734
Feb 2018 343.06       1.32433  259.0442           352.3260       -9.2660108
Mar 2018 266.13       1.31090  203.0132           276.1183       -9.9882495
Apr 2018 293.90       1.32577  221.6825           301.5104       -7.6103599
May 2018 284.73       1.33850  212.7232           289.3248       -4.5948212
Jun 2018 342.95       1.36830  250.6395           340.8948        2.0552438
Jul 2018 298.14       1.36140  218.9952           297.8553        0.2846937
Aug 2018 301.66       1.36700  220.6730           300.1374        1.5226438
Sep 2018 264.77       1.36732  193.6416           263.3719        1.3980921
Oct 2018 260.95       1.38061  189.0107           257.0734        3.8766087

[[2]]
          value exchange_rate fgn_value local_static_value exch_rate_impact
Oct 2017 331.53       1.79703  184.4877           331.5300        0.0000000
Nov 2017 308.85       1.80690  170.9281           307.1629        1.6870605
Dec 2017 311.35       1.79812  173.1531           311.1613        0.1887369
Jan 2018 354.31       1.85680  190.8175           342.9048       11.4051640
Feb 2018 343.06       1.84162  186.2816           334.7537        8.3062984
Mar 2018 266.13       1.83911  144.7059           260.0408        6.0892228
Apr 2018 293.90       1.82588  160.9635           289.2562        4.6437963
May 2018 284.73       1.77878  160.0704           287.6513       -2.9212846
Jun 2018 342.95       1.78907  191.6918           344.4759       -1.5258666
Jul 2018 298.14       1.78638  166.8962           299.9175       -1.7774444
Aug 2018 301.66       1.77860  169.6053           304.7858       -3.1258259
Sep 2018 264.77       1.78285  148.5094           266.8759       -2.1058633
Oct 2018 260.95       1.76900  147.5127           265.0848       -4.1347817

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
