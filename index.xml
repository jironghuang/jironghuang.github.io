<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jirong&#39;s sandbox on Jirong&#39;s sandbox</title>
    <link>/</link>
    <description>Recent content in Jirong&#39;s sandbox on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Loading excel data with correct variable types</title>
      <link>/post/load_data_with_correct_types/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/load_data_with_correct_types/</guid>
      <description>


&lt;div id=&#34;loading-data-with-data-types&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading data with data types&lt;/h3&gt;
&lt;p&gt;When reading static files into R or Python, most of the times we are lazy as we load the data with no regard to the data types.&lt;/p&gt;
&lt;p&gt;But in mission critical ETL jobs or Data analytics workflow, data types are quintessential and there’s a fine line between life and death. Ok, I’m exaggerating here.&lt;/p&gt;
&lt;p&gt;What I’ve written below is a swiss army knife function to read an excel file: 1st tab is data and 2nd tab is the variable types (e.g. database variable types mapped to R variable types)&lt;/p&gt;
&lt;p&gt;Note: If I read or write to database, I would have to modify my function below. Oh well.&lt;/p&gt;
&lt;p&gt;The steps in the function are pretty simple,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First I read in the data type&lt;/li&gt;
&lt;li&gt;Second I read in the column name of the dataset&lt;/li&gt;
&lt;li&gt;Third I left join the column names to its data type from database (or .sav or .dta or .sas files)&lt;/li&gt;
&lt;li&gt;Fourth I left join the (DB variable types) to (R data types) translation into the column names&lt;/li&gt;
&lt;li&gt;Lastly, I read in the dataset through read_excel functions with col_types as the parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here you go! Hope this is useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Reading excel data together with data types
#&amp;#39; @title Reading data with data types
#&amp;#39; @param spName_file Path of the data file
#&amp;#39; @param spTab_name_data Tab name of data
#&amp;#39; @param spTab_name_dataType Tab name of data type
#&amp;#39; @param sp_dataType_col_name Column name of dataType&amp;#39;s (variables column)
#&amp;#39; @param sp_dataType_datatype_name Column name of dataType&amp;#39;s (data type column)
#&amp;#39; @param dp_r_hana_type data frame of R to hana variable types conversion
#&amp;#39; @param sp_r_hana_type_DBType Column name of r_hana_type (DB_TYPE column)
#&amp;#39; @param sp_r_hana_type_RType Column name of r_hana_type (R_TYPE column)
#&amp;#39; @return A data frame with corresponding data types
#&amp;#39; @export
df_read_data_with_types = function(spName_file, spTab_name_data,
                                   spTab_name_dataType,
                                   sp_dataType_col_name, sp_dataType_datatype_name,
                                   dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType){

  #Read in data types
  data_type = df_read_tab(spName_file, spTab_name_dataType)

  #Read in just first row of dataset
  col_name = read_excel(spName_file, spTab_name_data, n_max = 1)
  col_name = data.frame(col_name = names(col_name)); col_name$col_name = as.character(col_name$col_name)
  col_name$column_order = 1:nrow(col_name)

  #Left join data type to name
  col_name = col_name %&amp;gt;%
    dplyr::left_join(data_type, by = c(&amp;quot;col_name&amp;quot; = sp_dataType_col_name))

  #Split this from above because of errors. weird
  col_name = base::merge(col_name, dp_r_hana_type,
                   by.x = sp_dataType_datatype_name, by.y = sp_r_hana_type_DBType,
                   all.x = T)

  col_name = arrange(col_name, column_order)

  #Read in full dataset with assignment of classes (look at readxl.tidyverse.org)
  data = readxl::read_excel(spName_file, spTab_name_data, col_types = col_name[, sp_r_hana_type_RType])

  return(data)
}

# spName_file = &amp;#39;./data/input/TB_OVSS_FNB_FACT.xlsx&amp;#39;
# spTab_name_data = &amp;#39;data&amp;#39;
# spTab_name_dataType = &amp;#39;data_type&amp;#39;
# sp_dataType_col_name = &amp;#39;COLUMN_NAME&amp;#39; 
# sp_dataType_datatype_name = &amp;#39;DATA_TYPE_NAME&amp;#39;
# dp_r_hana_type = r_hana_type 
# sp_r_hana_type_DBType = &amp;#39;DB_TYPE&amp;#39; 
# sp_r_hana_type_RType = &amp;#39;R_TYPE&amp;#39;
#
# a = df_read_data_with_types (spName_file, spTab_name_data,
#                          spTab_name_dataType,
#                          sp_dataType_col_name, sp_dataType_datatype_name,
#                          dp_r_hana_type, sp_r_hana_type_DBType, sp_r_hana_type_RType)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Function to describe clusters derived from unsupervised learning</title>
      <link>/post/cluster_descriptive_stats/</link>
      <pubDate>Fri, 24 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/cluster_descriptive_stats/</guid>
      <description>

&lt;h2 id=&#34;describing-unsupervised-learning-clusters&#34;&gt;Describing unsupervised learning clusters&lt;/h2&gt;

&lt;p&gt;As a data scientist / analyst, besides doing cool modelling stuff, we&amp;rsquo;re often asked to churn out descriptive statistics. Yes, we know. It&amp;rsquo;s part of the process.&lt;/p&gt;

&lt;p&gt;I chanced upon this really nifty concept at work to describe the clusters derived from unsupervised learnig. Here&amp;rsquo;s how it goes,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Say it&amp;rsquo;s a nominal or ordinal variable. First, I find the proportion of the feature across the X clusters&lt;/li&gt;
&lt;li&gt;Second, I rank this proportion through percentiles across these X values&lt;/li&gt;
&lt;li&gt;The cluster with the highest percentile will earn its right to be represented by the feature&lt;/li&gt;
&lt;li&gt;And if it&amp;rsquo;s a scale variable, you may find the mean of the feature for each cluster and repeat the steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is a nifty function to carry out the above steps. You can compile it into a package and may start using it in your data science work!&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  This function takes in a data-frame
#&#39; @title Finding feature importance associated with each cluster
#&#39; @param dp_data data frame for clustering
#&#39; @param sp_resp_key_name primary key in dataset
#&#39; @param sp_feature Feature name
#&#39; @param sp_cluster_name column name for cluster 
#&#39; @param sp_scale_categorical scale or categorical
#&#39; @param sp_IndivYr_aggregate IndivYr or aggregate
#&#39; @param sp_weight_name Column names for weights
#&#39; @param np_feature_imp_threshold threshold for percentile ranking
#&#39; @param sp_all_filter filter or no filter
#&#39; @return list of proportion, mean or percentile for 6 cluster importance
#&#39; @export
#&#39;

l_feature_importance_cluster = function(dp_data, sp_resp_key_name,
                                        sp_feature, sp_cluster_name, sp_scale_categorical,
                                        sp_IndivYr_aggregate = NULL, sp_weight_name,
                                        np_feature_imp_threshold = 1, sp_all_filter = &amp;quot;all&amp;quot;){

  tryCatch({
  if((sp_scale_categorical == &amp;quot;categorical&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
    #Tabulate

    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_weight_name, sp_cluster_name)) %&amp;gt;%                #select columns
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%   #Count by cluster and feature
        dplyr::summarize(counts = n(),
                         wt_counts = sum(!! rlang:: sym(sp_weight_name))) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%                               #Find proportion of feature in cluster group
        dplyr::mutate(counts_cluster = sum(counts, na.rm = T),
                      prop_feature_within_cluster = counts/counts_cluster,
                      counts_cluster_wt = sum(wt_counts, na.rm = T),
                      prop_feature_within_cluster_wt = wt_counts/counts_cluster_wt) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_feature)) %&amp;gt;%                                    #Find percentile of proportions
        dplyr::mutate(percentile_feature = percent_rank(prop_feature_within_cluster),
                      percentile_feature_wt = percent_rank(prop_feature_within_cluster_wt),
                      counts_feature = sum(counts, na.rm = T),
                      prop_cluster_within_features = counts/counts_feature) %&amp;gt;%            #Find proportion of cluster in feature
        dplyr::group_by(!! rlang:: sym(sp_cluster_name), !! rlang:: sym(sp_feature)) %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::rename(feature_value = sp_feature)

    #Filter to keep only the &#39;meaningful&#39; features   

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
        dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)   
    }

  } else if((sp_scale_categorical == &amp;quot;scale&amp;quot;) &amp;amp; base::is.null(sp_IndivYr_aggregate)){
  
    dTabulated_data = dp_data %&amp;gt;%
        dplyr::select(c(sp_resp_key_name, sp_feature, sp_cluster_name, sp_weight_name)) %&amp;gt;%
        dplyr::group_by(!! rlang:: sym(sp_cluster_name)) %&amp;gt;%
        dplyr::summarize(avg_by_cluster = mean(!! rlang:: sym(sp_feature), na.rm = T),
                         avg_by_cluster_wt = weighted.mean(!! rlang:: sym(sp_feature), !! rlang:: sym(sp_weight_name), na.rm = T))  %&amp;gt;%
        dplyr::mutate(feature_name = sp_feature) %&amp;gt;%
        dplyr::mutate(percentile_feature = percent_rank(avg_by_cluster),
                      percentile_feature_wt = percent_rank(avg_by_cluster_wt))

     #Filter to keep only the &#39;meaningful&#39; features

    if(sp_all_filter == &amp;quot;filter&amp;quot;){
      dTabulated_data = dRaw_data %&amp;gt;%
                            dplyr::filter(percentile_feature &amp;gt;= np_feature_imp_threshold)    
    } 
  }
  }, error = function(e){
    print(paste0(&amp;quot;Error with &amp;quot;, sp_feature))
  })
  
  return(dTabulated_data)

}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Playing with Google Place API</title>
      <link>/post/google_place_api/</link>
      <pubDate>Tue, 14 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/google_place_api/</guid>
      <description>

&lt;h2 id=&#34;google-place-api&#34;&gt;Google Place API&lt;/h2&gt;

&lt;p&gt;I was playing around with the API to obtain lat-long for my geo analytics work.&lt;/p&gt;

&lt;p&gt;I entered my credit card info but it seems that I&amp;rsquo;m not charged even with 9000+ API calls. Unsure if it&amp;rsquo;s because I&amp;rsquo;ve a 400+ dollars free cloud credit?&lt;/p&gt;

&lt;p&gt;Anyway, what I did here was to make API calls and storing the data into my local database.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested, you may visit this stackoverflow link (&lt;a href=&#34;https://stackoverflow.com/questions/52565472/get-map-not-passing-the-api-key-http-status-was-403-forbidden/52617929#52617929&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/52565472/get-map-not-passing-the-api-key-http-status-was-403-forbidden/52617929#52617929&lt;/a&gt;) to understand how to set up the credentials.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Load util function
source(&amp;quot;connect_db.R&amp;quot;)
vf_connect_db(&amp;quot;google_place.db&amp;quot;)

#load packages
sapply(c(&amp;quot;ggmap&amp;quot;, &amp;quot;RSQLite&amp;quot;), require, character.only = T)

#Google key information
register_google(key = &amp;quot;&amp;lt;intentionally left blank&amp;gt;&amp;quot;) 

#Load dataframe into the memory
query = paste0(&#39;select TripAdv_Key, address from &amp;quot;trip_advisor_full_dataset_without_lat_long&amp;quot;&#39;)
origAddress = dbGetQuery(con, query)

# Loop through the addresses to get the latitude and longitude of each address and add it to the
# origAddress data frame in new columns lat and lon
for(i in 1:nrow(origAddress))
{
  print(i)
  tryCatch({
    
    #Geocoding based on address
    result &amp;lt;- geocode(origAddress$address[i], output = &amp;quot;latlona&amp;quot;, source = &amp;quot;google&amp;quot;)
    
    #Extracting results values and inserting into database
    TripAdv_Key = origAddress$TripAdv_Key[i] 
    lat = as.numeric(result[2])
    long = as.numeric(result[1])
    add = as.character(result[3])
    values = paste0(&amp;quot;(&amp;quot;,TripAdv_Key, &amp;quot;,&amp;quot;, lat, &amp;quot;,&amp;quot;, long, &amp;quot;,&#39;&amp;quot;, add,&amp;quot;&#39;)&amp;quot;)
    query = paste0(&#39;INSERT INTO lat_long_info VALUES&#39;, values)
    dbSendQuery(con, query)
    
    #Print lat long
    print(c(lat, long))
  }, error=function(e){})
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Using exponential distribution to estimate frequency of occurence</title>
      <link>/post/exp_distrib/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/exp_distrib/</guid>
      <description>


&lt;div id=&#34;simulating-product-failures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating product failures&lt;/h2&gt;
&lt;p&gt;I’m inspired by this post here (&lt;a href=&#34;http://www.programmingr.com/examples/neat-tricks/sample-r-function/rexp/&#34; class=&#34;uri&#34;&gt;http://www.programmingr.com/examples/neat-tricks/sample-r-function/rexp/&lt;/a&gt;). And decided to expand on the example.&lt;/p&gt;
&lt;p&gt;Say you are an owner of a computer store and you would like to estimate the frequency of warranty repairs - and the ensuing costs.&lt;/p&gt;
&lt;p&gt;Here’s the scenario with the accompanying assumptions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each computer is expected to last an average of 7 years&lt;/li&gt;
&lt;li&gt;You only sell 1000 computers at the start of each year&lt;/li&gt;
&lt;li&gt;You sell computer from 2019 to 2025&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, I simulate an exponential distribution of 1000 points for 7 years; and place a time index of 2019 to 2025&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1) 
library(tidyr)

sim_repair_time = mapply(rexp, rep(1000, 7), rep(1/7, 7))
sim_repair_time = data.frame(sim_repair_time)
names(sim_repair_time) = paste0(&amp;quot;Y&amp;quot;, 2019:2025)
sim_repair_time$comp_index = 1:nrow(sim_repair_time)
head(sim_repair_time)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Y2019     Y2020      Y2021     Y2022      Y2023     Y2024     Y2025
## 1  5.2862728 7.0456022  4.4584814  2.987471  2.4983623  1.155475  3.526856
## 2  8.2714995 0.3831851 21.3583423 16.976357  0.6371918 19.870135  8.339298
## 3  1.0199471 6.1577002 24.7270286  7.184722 13.6686444  2.692336  1.531733
## 4  0.9785668 2.0554758  4.0556961 11.680879  3.4047724  8.547126  4.098845
## 5  3.0524804 1.3899782  0.8014122  7.394160 10.8930823  1.088804  8.679067
## 6 20.2647798 1.1392965  8.5595344  3.423599  5.0281127  8.178582 15.854250
##   comp_index
## 1          1
## 2          2
## 3          3
## 4          4
## 5          5
## 6          6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I - in dplyr lingo - gather the dataset (convert to long form)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_repair_time = sim_repair_time %&amp;gt;% gather(key = year, 
                                     value = spoilt_years_later, 
                                    -comp_index)
sim_repair_time$year = gsub(&amp;quot;Y&amp;quot;, &amp;quot;&amp;quot;, sim_repair_time$year)

head(sim_repair_time, 50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    comp_index year spoilt_years_later
## 1           1 2019          5.2862728
## 2           2 2019          8.2714995
## 3           3 2019          1.0199471
## 4           4 2019          0.9785668
## 5           5 2019          3.0524804
## 6           6 2019         20.2647798
## 7           7 2019          8.6069344
## 8           8 2019          3.7777799
## 9           9 2019          6.6959725
## 10         10 2019          1.0293219
## 11         11 2019          9.7351459
## 12         12 2019          5.3342090
## 13         13 2019          8.6632249
## 14         14 2019         30.9675395
## 15         15 2019          7.3818022
## 16         16 2019          7.2467076
## 17         17 2019         13.1322462
## 18         18 2019          4.5832265
## 19         19 2019          2.3585343
## 20         20 2019          4.1193581
## 21         21 2019         16.5516068
## 22         22 2019          4.4932481
## 23         23 2019          2.0588427
## 24         24 2019          3.9610587
## 25         25 2019          0.7425084
## 26         26 2019          0.4160741
## 27         27 2019          4.0509872
## 28         28 2019         27.7125300
## 29         29 2019          8.2131847
## 30         30 2019          6.9776907
## 31         31 2019         10.0469974
## 32         32 2019          0.2608797
## 33         33 2019          2.2680711
## 34         34 2019          9.2432755
## 35         35 2019          1.4245725
## 36         36 2019          7.1590811
## 37         37 2019          2.1121865
## 38         38 2019          5.0765001
## 39         39 2019          5.2607988
## 40         40 2019          1.6451922
## 41         41 2019          7.5591680
## 42         42 2019          7.1977283
## 43         43 2019          9.0458315
## 44         44 2019          8.7717375
## 45         45 2019          3.8824898
## 46         46 2019          2.1089810
## 47         47 2019          9.0518726
## 48         48 2019          6.9618905
## 49         49 2019          3.5992201
## 50         50 2019         14.0548268&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, I add the time taken for each computer to break down to the year for which the computer is bought.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#sim_repair_time$spoilt_years_later = round(sim_repair_time$spoilt_years_later, 0)
sim_repair_time$year_spoilt = sim_repair_time$spoilt_years_later + as.numeric(sim_repair_time$year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the distribution of years taken that a computer will break down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sim_repair_time$spoilt_years_later)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exp_distrib_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is the distribution of the years that the computer will break down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sim_repair_time$year_spoilt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exp_distrib_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explaining-exponential-distribution-from-first-principle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explaining exponential distribution from first principle&lt;/h2&gt;
&lt;p&gt;If you are keen from the first principle perspective,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) =  {\lambda}e^{-\lambda x} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To understand pdf function here. It’s pretty simple. If you run the simulator 1000 times with mean = 7 (lambda = 1/7), and you plot the distribution, it’s mostly likely to be front-loaded.&lt;/p&gt;
&lt;p&gt;If you fit a series of values x - N to the above function, it will be pretty similar to simulated series of values.&lt;/p&gt;
&lt;p&gt;And if you do a mean of the simulated data, it will return close to the mean of 7&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = rexp(1000, 1/7)
hist(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exp_distrib_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.775649&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this simple example here is useful!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on Reinforcement Learning - Q Learning</title>
      <link>/post/q_learning/</link>
      <pubDate>Mon, 08 Apr 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/q_learning/</guid>
      <description>

&lt;h2 id=&#34;q-learning&#34;&gt;Q learning&lt;/h2&gt;

&lt;p&gt;I just completed a Reinforcement Learning assignment - in particular on Q-learning. According to Wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Q-learning&#34;&gt;here&lt;/a&gt;, it&amp;rsquo;s a model-free Rl algorithm. The goal for the algo is to learn a policy, which tells an agent what action to take under different circumstances.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s my confession. What I&amp;rsquo;m doing in this post is to summarise what I&amp;rsquo;ve just learnt so that I may come back to this at any point in future. Hence it may or may not make sense to you,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RL is rather different from a conventional ML process. It contains an in-built iterative process to refresh the parameters&lt;/li&gt;
&lt;li&gt;User can train the algorithm till the &amp;lsquo;total reward&amp;rsquo; (in ML lingo that could be error-type measures) converges. I will coin each training process as a simulation&lt;/li&gt;
&lt;li&gt;To set up the learning process, I first initalize a Q learning table of 0s with dimension of number of states * actions (In python lingo, self.q = np.zeros((num_states, num_actions), dtype = np.float64))&lt;/li&gt;
&lt;li&gt;In every step of each simulation, the algo will pick a random float of 0 to 1. If it&amp;rsquo;s lesser than the threshold, it will pick a random action. Else, it will pick the action with the best outcome. According to the literature, it seems that exploration plays a role in improving the results&lt;/li&gt;
&lt;li&gt;From second step onward, the algo will update the Q-table as follows: self.q[self.s, self.a] = (1 - self.alpha) * self.q[self.s, self.a] + self.alpha * (r + self.gamma * np.max(self.q[s_prime,]))&lt;/li&gt;
&lt;li&gt;What it meant in the above formula is that Q-learning computes a weighted score for a particular state and action based on present and future discounted score from best action.&lt;/li&gt;
&lt;li&gt;The updated score will be used in subsequent simulations, and not current one. i.e in future iteration, if the option is non-random, it will pick the highest score option.&lt;/li&gt;
&lt;li&gt;When the current simulation end, the algorithm will return to the starting point and retrain the algorithm with the refreshed Q-table&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dyna-q&#34;&gt;Dyna-Q&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The additional bootstrap component algorithm is deemed to be cheaper because it doesn&amp;rsquo;t require additional interaction with the external environment to refresh the Q-table.&lt;/li&gt;
&lt;li&gt;It will instead pick random states and actions&lt;/li&gt;
&lt;li&gt;And select a new state based on a probability mass function in each loop (each state is assigned a discrete chance. Say there&amp;rsquo;re 100 states where 1 of the states has 2% chance and another has 1% chance. The latter might still be selected albeit with a lower chance)&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s different here is that it will refresh the reward information too&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;insights-from-this-exercise&#34;&gt;Insights from this exercise&lt;/h3&gt;

&lt;p&gt;The strength of this algorithm lies in the fact that it doesn&amp;rsquo;t require a ton of data. I&amp;rsquo;m excited to apply this algorithm if there&amp;rsquo;s a chance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;re the returns (XIRR) for my CPFIS Portfolio</title>
      <link>/project/xirr_cpf/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/xirr_cpf/</guid>
      <description>


&lt;div id=&#34;whatre-the-returns-xirr-for-my-cpfis-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’re the returns (XIRR) for my CPFIS Portfolio?&lt;/h2&gt;
&lt;p&gt;I expounded my philosophy and calculated the returns of my CPFIS portfolio via XIRR (Extended Internal Rate of Returns) &lt;a href=&#34;https://jironghuang.github.io/post/xirr_cpfis/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The binary lens of evaluating the XIRR is as follows,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If XIRR &amp;gt; 2.5%, my investment decision paid off&lt;/li&gt;
&lt;li&gt;Else if XIRR &amp;lt; 2.5%, my invesment decision did not pay off (not to say it’s a bad decision. Pls do not confuse decision with outcome)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an ex-economist/ data geek who doesn’t shy away from having skin in the game, I will be honest in sharing my returns and its stream of cashflow here,&lt;/p&gt;
&lt;iframe src=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQtSJfzakpUWRkryIoXaqJm7szd-g6R1SHr-aAXAlHNOFEDXYGhCBNC9UeYEYv8cYf8krgsS6LPpED9/pubhtml?gid=267342954&amp;amp;single=true&amp;amp;widget=true&amp;amp;headers=false&#34; width=&#34;1000&#34; height=&#34;780&#34; style=&#34;border: none;&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;re the returns (XIRR) for my CPFIS Portfolio</title>
      <link>/post/xirr_cpfis/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/xirr_cpfis/</guid>
      <description>


&lt;div id=&#34;whatre-the-returns-xirr-for-my-cpfis-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’re the returns (XIRR) for my CPFIS Portfolio?&lt;/h2&gt;
&lt;p&gt;Every employee in Singapore is bounded by the same set of CPF rules.&lt;/p&gt;
&lt;p&gt;As an ex-economist/ data geek who doesn’t shy away from having skin in the game. I asked myself this question back in 2015 when I was still a starry-eyed young man 2 years into the workforce - how do I set out to optimize my returns in my CPF OA with these given set of constraints,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You cannot withdraw your CPF OA till you are 55 years old&lt;/li&gt;
&lt;li&gt;You can only invest CPF OA beyond 20k&lt;/li&gt;
&lt;li&gt;And of this delta, you can only invest 35% in non ETFs instruments OR 100% of it in ETFs&lt;/li&gt;
&lt;li&gt;CPF OA returns of 2.5% per annum&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last point is something that I wish to highlight. If I choose to invest, I would have to overcome the 2.5% hurdle rate from CPF OA.&lt;/p&gt;
&lt;p&gt;Subscribing to Vanguard’s philosophy, a pioneeer &amp;amp; leader in the space of index funds - (&lt;a href=&#34;https://personal.vanguard.com/pdf/s315.pdf&#34; class=&#34;uri&#34;&gt;https://personal.vanguard.com/pdf/s315.pdf&lt;/a&gt;) at least for my CPFIS portion, they advocate that lump sum investing proves to be superior than spacing out your investments (dollar cost averaging).&lt;/p&gt;
&lt;p&gt;Since then - whenever I have 5-6k in my CPF OA beyond the 20k, I promptly allocated it to the market. In some periods, I invested with smaller amounts when POEMS brokerage offered some promotions on commission fees.&lt;/p&gt;
&lt;div id=&#34;how-did-this-strategy-fare-thus-far&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How did this strategy fare thus far?&lt;/h3&gt;
&lt;p&gt;XIRR- a metric used in assessing rate of returns with a given set of cashflows - came up to be around 5.7%! Note: This figure here accounts for the dividends received over the years.&lt;/p&gt;
&lt;p&gt;Hurray! It’s more than twice the 2.5% hurdle rate in CPF OA. And even the 4% rate from SA. One could voluntarily transfer OA to SA, but you will lose the flexibility of using the OA for serving mortage in future&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My thoughts&lt;/h3&gt;
&lt;p&gt;This is still an ongoing experiment in my ‘lab’. Apparently it seems to be working well! Going forward, I will continue this strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;if-you-are-keen-in-the-technicalities-of-computing-xirr-applied-the-function-developed-by-someone-else-here-httpsgithub.comsunilveeravallixirr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;If you are keen in the technicalities of computing XIRR (applied the function developed by someone else here –&amp;gt;#&lt;a href=&#34;https://github.com/SunilVeeravalli/xirr&#34; class=&#34;uri&#34;&gt;https://github.com/SunilVeeravalli/xirr&lt;/a&gt;)…&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(source(&amp;quot;./CPFOA/F_xirr.R&amp;quot;))
suppressMessages(source(&amp;quot;./CPFOA/another_irr_eg.R&amp;quot;))
suppressPackageStartupMessages(library(tvm))

#Reading my dataset
cpf_contrib &amp;lt;- read_csv(file = &amp;quot;./CPFOA/jr_cpfis_contrib.csv&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Date = col_character(),
##   num_stocks_cpf_port = col_double(),
##   sti_px = col_double(),
##   cpf_oa_port_val = col_double(),
##   Dividends_per_share = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Formattting date
cpf_contrib$Date = as.Date(cpf_contrib$Date, format = &amp;quot;%m/%d/%y&amp;quot;)
names(cpf_contrib)[which(names(cpf_contrib) == &amp;quot;Date&amp;quot;)] = &amp;quot;dates&amp;quot; 

#including ss
cpf_contrib$net_stocks_purchase = lead(cpf_contrib$num_stocks_cpf_port - lag(cpf_contrib$num_stocks_cpf_port))

#Contributions
cpf_contrib$stock_amt_purchase = cpf_contrib$net_stocks_purchase * cpf_contrib$sti_px

#Dividends amount
cpf_contrib$dividends = cpf_contrib$num_stocks_cpf_port * cpf_contrib$Dividends_per_share 
cpf_contrib$dividends = ifelse(is.na(cpf_contrib$dividends), 0, cpf_contrib$dividends)

#Net cashflow
cpf_contrib = cpf_contrib %&amp;gt;%
                mutate(net_cash_flow = dividends - stock_amt_purchase)

#Setting final cashflow  
cpf_contrib$net_cash_flow[nrow(cpf_contrib)] = cpf_contrib$cpf_oa_port_val[nrow(cpf_contrib)]

#Computing the xirr
xirr(cpf_contrib[, c(&amp;quot;dates&amp;quot;,&amp;quot;net_cash_flow&amp;quot;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;XIRR is 5.775%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(xirr2(cpf_contrib$net_cash_flow, cpf_contrib$dates))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05777714&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;final-dataset-on-dividends-purchases-cashflows-portfolio-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final dataset on dividends, purchases, cashflows, portfolio values&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))

kable(cpf_contrib, caption = &amp;quot;Final data-frame of dividends, purchase, cashflows, portfolio values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Final data-frame of dividends, purchase, cashflows, portfolio values
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
dates
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
num_stocks_cpf_port
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sti_px
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
cpf_oa_port_val
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Dividends_per_share
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
net_stocks_purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
stock_amt_purchase
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dividends
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
net_cash_flow
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-05-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.350
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6030.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-6030.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-06-01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.240
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3240.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3240.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-07-15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8372
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2970.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
137.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2832.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-08-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.850
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10564
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5700.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5700.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-09-13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15486
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-10-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.920
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-11-11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15892
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2950.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2950.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2015-12-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.630
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18768
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-01-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.690
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16728
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1883.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1883.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-02-03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19275
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.107
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1144.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
802.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-341.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-03-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.870
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21567
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1148.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1148.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-04-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.840
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22742
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2556.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2556.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-05-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24932
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1728.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1728.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-06-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.880
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1152.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1152.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-07-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1144.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1144.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-08-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.910
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29362
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2910.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
890.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2019.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-09-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32712
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2002.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2002.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-10-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-11-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35178
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2016-12-09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-01-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37023
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4960.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4960.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-02-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1914.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
736.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1177.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-03-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2560.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2560.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-04-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48195
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-05-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.270
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49113
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-06-22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49266
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2656.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2656.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-07-21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52647
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
772.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
772.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-08-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53452
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1300.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1300.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-09-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.420
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53625
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3762.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3762.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-10-25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.480
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-11-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.450
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2017-12-23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.580
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-01-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.520
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64416
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-02-22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61952
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
932.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
932.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-03-23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.637
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60368
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-04-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.500
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-05-17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.321
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63008
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-06-24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.388
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-07-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.251
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58432
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-08-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.299
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1700
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5608.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1056.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4552.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-09-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60795
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-10-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60216
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1300
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4102.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4102.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-11-10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64272
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2018-12-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.247
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
63860
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-01-11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.218
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
67362
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-02-19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
66332
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.056
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6382.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1153.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5228.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2019-03-12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22600
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.210
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72388
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
72388.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hosting a Flask App on Heroku</title>
      <link>/post/hosting-a-flask-app-on-heroku/</link>
      <pubDate>Thu, 28 Feb 2019 23:34:32 +0800</pubDate>
      
      <guid>/post/hosting-a-flask-app-on-heroku/</guid>
      <description>&lt;p&gt;Following the steps here &amp;ndash;&amp;gt; &lt;a href=&#34;https://realpython.com/flask-by-example-part-1-project-setup/&#34; target=&#34;_blank&#34;&gt;https://realpython.com/flask-by-example-part-1-project-setup/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I managed to deploy my python flask app in Heroku.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask
app = Flask(__name__)


@app.route(&#39;/&#39;)
def hello():
    return &amp;quot;Hello World!&amp;quot;


@app.route(&#39;/&amp;lt;name&amp;gt;&#39;)
def hello_name(name):
    return &amp;quot;Hello {}!&amp;quot;.format(name)

if __name__ == &#39;__main__&#39;:
    app.run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may visit the following link &amp;ndash;&amp;gt;&lt;a href=&#34;https://jirong-stage.herokuapp.com/&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/&lt;/a&gt; &amp;amp; add a suffix to it.&lt;/p&gt;

&lt;p&gt;Example &lt;a href=&#34;https://jirong-stage.herokuapp.com/jirong&#34; target=&#34;_blank&#34;&gt;https://jirong-stage.herokuapp.com/jirong&lt;/a&gt; &amp;amp; this will return Hello jirong!&lt;/p&gt;

&lt;p&gt;Possibilites are immense! I can easily create APIs or host dashboard here. Pretty exciting to me:)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling With Replacement Through First Principles</title>
      <link>/post/sampling-with-replacement-through-first-principles/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sampling-with-replacement-through-first-principles/</guid>
      <description>

&lt;h1 id=&#34;sampling-with-replacement&#34;&gt;Sampling with replacement&lt;/h1&gt;

&lt;p&gt;Hello! It&amp;rsquo;s me once again attempting to explain things from first principles - a term popularized by Elon Musk.&lt;/p&gt;

&lt;p&gt;I will use some psudeo code - on sampling with replacement for weights - to aid my explanation.&lt;/p&gt;

&lt;p&gt;Earlier in the week, I attempted to write a simple function from scratch but I gave up after realising that it will take me more than 15 mins! Difficulties lies in the multiple switch statements in defining the intervals. Haven&amp;rsquo;t figured that part out yet.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;re definitely packages out there that does sort of stuff but this forces me to understand the underlying theory from scratch.&lt;/p&gt;

&lt;p&gt;So here is the idea, I&amp;rsquo;ve a dataset with 4 individuals, tagged to respective weights that corresponds to the population. And I wish to do a bootstrap i.e. sampling with replacement to get a sample size of N = 100&lt;/p&gt;

&lt;p&gt;See Wikipedia page on advantages of Bootstrapping &amp;ndash;&amp;gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Bootstrapping_(statistics&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I will do. I will first line up the individuals and find the Probability Mass Function (PMF) for each individual accounting for its weight. Second, I will add up the PMF to obtain the Cumulative Density Function (cumulative proportion)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df
id weight PMF     CDF
1  2      20%    [0, 20]
2  3      30%    (20, 50]
3  3      30%    (50, 80]
4  2      20%    (80, 100]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, I will do a loop of 100 times. At the start of each loop I will obtain a float of between 0 to 1. If the value lies between a certain range, I will add that individual to the dataset. Given that num is random, it will lie between the various ranges without order, accouting for length of the interval.&lt;/p&gt;

&lt;p&gt;Note that sampling with replacement means there&amp;rsquo;s a chance that an individual may be represented more than once in the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (i in 1:100){

  num = randint(0, 1)
  
  if(num &amp;lt; 0.2){
    add_to_new_dataset(id = 1)
  }else if (num between 0.2 to 0.5){
    add_to_new_dataset(id = 2)
  }else if (num between 0.5 to 0.8){
    add_to_new_dataset(id = 3)
  }else{
    add_to_new_dataset(id = 3)
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course! In R, you should avoid an explicit loop at all costs. The solution is to embed it in a function and use a lapply function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bootstrap_weights = function(i){

num = randint(0, 1)

  if(num &amp;lt; 0.2){
    id = 1
  }else if (num between 0.2 to 0.5){
    id = 2 
  }else if (num between 0.5 to 0.8){
    id = 3
  }else{
    id = 4
  }
  
  data_row = data[id, ]
  return (data_row)
}

bootstrapped_data = rbind.fill(lapply(1: 100, bootstrap_weights))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I hope this is useful!&lt;/p&gt;

&lt;h2 id=&#34;latest-developments&#34;&gt;Latest developments&lt;/h2&gt;

&lt;p&gt;Courtesy of this post here &amp;ndash;&amp;gt; &lt;a href=&#34;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/24766104/checking-if-value-in-vector-is-in-range-of-values-in-different-length-vector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the simple solution to link a value to an interval,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getValue &amp;lt;- function(x, data) {
  tmp &amp;lt;- data %&amp;gt;%
    filter(CDF1 &amp;lt;= x, x &amp;lt;= CDF2)
  return(tmp$id)
}

# Using rand function to get a list of numbers
rand_numbers &amp;lt;- c(0.1, 0.173, 0.235)
sapply(rand_numbers, getValue, data = df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a decision tree algorithm from scratch</title>
      <link>/post/building-adecision-tree-from-scratch/</link>
      <pubDate>Fri, 15 Feb 2019 13:09:44 +0800</pubDate>
      
      <guid>/post/building-adecision-tree-from-scratch/</guid>
      <description>

&lt;h2 id=&#34;building-a-decision-tree-from-scratch&#34;&gt;Building a decision tree from scratch&lt;/h2&gt;

&lt;p&gt;Sometimes to truly understand and internalise an algorithm, it&amp;rsquo;s always useful to build from scratch. Rather than relying on a module or library written by someone else.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fortunate to be given the chance to do it in 1 of my assignments for decision trees.&lt;/p&gt;

&lt;p&gt;From this exercise, I had to rely on my knowledge on recursion, binary trees (in-order traversal) and object oriented programming.&lt;/p&gt;

&lt;p&gt;Below is a snippet of method in a class. The algorithm works as follows,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First you define a leaf size i.e. the maximum number of data you allow to be left in the leaf node.&lt;/li&gt;
&lt;li&gt;If number of data in leaf node is still more than the allowable size, check if all data is the same. If it&amp;rsquo;s the same, return just 1 value&lt;/li&gt;
&lt;li&gt;Next I find the feature based on best correlation (gini coefficient and information gain works too) with the dependent variable values. Note that as you traverse down the tree, this dataset gets smaller&lt;/li&gt;
&lt;li&gt;With the best feature found in each split, I proceed to contruct the left tree and right tree together with a root node&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is that if you are left with a node that&amp;rsquo;s smaller or equals to allowable leaf size, it will stop traversing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/Building_decision_tree.png&#34; alt=&#34;/post/img/Building_decision_tree.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Martingale Strategy - Double Down</title>
      <link>/post/martingale-strategy/</link>
      <pubDate>Sat, 26 Jan 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/martingale-strategy/</guid>
      <description>

&lt;h2 id=&#34;martingale-strategy&#34;&gt;Martingale Strategy&lt;/h2&gt;

&lt;p&gt;In this post, I will simulate a martingale strategy in Roulette&amp;rsquo;s context to highlight the potential risks associated with this strategy.&lt;/p&gt;

&lt;p&gt;Double down! That&amp;rsquo;s essentially the essence of it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a simple explanation of the strategy,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The croupier spins the ball. If it&amp;rsquo;s red you win the amount you bet, black you lose the same amount&lt;/li&gt;
&lt;li&gt;If you win, you continue to bet the same amount (same as your 1st bet amount)&lt;/li&gt;
&lt;li&gt;If you lose, you double your bet amount&lt;/li&gt;
&lt;li&gt;And if your accumulated winnings hits a certain amount, you stop and leave the casino&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So how would the strategy fare? To explain, I will use Monte Carlo with a Bernoulli distribution for each roulette spin (X ~ B(1, 0.48)).&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-10-runs-times&#34;&gt;Simulate strategy 10 runs/ times&lt;/h3&gt;

&lt;p&gt;Here, I simulate the strategy of 10 times. Think of it in this way, there&amp;rsquo;re 10 alternate universes which you exist and you play the same game 10 times. Or you can just simply treat this as going back to the casino on 10 separate days.&lt;/p&gt;

&lt;p&gt;In the chart below, you will notice that for some runs; because of a sequence of bad luck, the losses quick spiralled!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/10_simulations.png&#34; alt=&#34;/post/img/10_simulations.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;simulate-strategy-1000-runs-times-and-compute-the-mean&#34;&gt;Simulate strategy 1000 runs/ times and compute the mean&lt;/h3&gt;

&lt;p&gt;Next, for a more robust interpretation, I went on to simulate this strategy 1000 times and computed the mean and standard deviation. You will notice that the strategy eventually converges to a desired terminal value. In this case, it&amp;rsquo;s $80. So essentially, out of 1000 simulations, all of them reaches my profit target!&lt;/p&gt;

&lt;p&gt;However the the risk is enormous. Near the average 120th run, the standard deviation sky-rocketed to 120,000. I&amp;rsquo;m unsure if anyone could stomach this loss at any one point of time. The journey matters!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/1000_simulations.png&#34; alt=&#34;/post/img/1000_simulations.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/simulation_sd.png&#34; alt=&#34;/post/img/simulation_sd.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;insights-from-this-strategy&#34;&gt;Insights from this strategy&lt;/h3&gt;

&lt;p&gt;Martingale strategy - to put it simply - is a win small, lose potentially huge strategy.&lt;/p&gt;

&lt;p&gt;In this strategy, you will win 100% of the time.&lt;/p&gt;

&lt;p&gt;But the question is, do you have the money (or infinite bankroll) to tide through tough times?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Create a Python Environment in Ubuntu or any Debian-based system</title>
      <link>/post/how-to-create-a-python-environment-in-ubuntu/</link>
      <pubDate>Wed, 09 Jan 2019 23:39:22 +0800</pubDate>
      
      <guid>/post/how-to-create-a-python-environment-in-ubuntu/</guid>
      <description>&lt;p&gt;Often, certain projects or classes involving python require a set of modules/packages for the code to work.&lt;/p&gt;

&lt;p&gt;1 solution is to create a Python Environment dedicated to that project.&lt;/p&gt;

&lt;p&gt;First set up a folder, and include a .yml file with the specific modules and environment that you wish to install. Here is an example (env.yml),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: env
channels: !!python/tuple
- !!python/unicode &#39;defaults&#39;
dependencies:
- nb_conda=2.2.0=py27_0
- python=2.7.13=0
- cycler=0.10.0
- functools32=3.2.3.2
- matplotlib=2.0.2
- numpy=1.13.1
- pandas=0.20.3
- py=1.4.34
- pyparsing=2.2.0
- pytest=3.2.1
- python-dateutil=2.6.1
- pytz=2017.2
- scipy=0.19.1
- six=1.10.0
- subprocess32=3.2.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have anaconda installed, navigate to the folder with .yml; right click and select open in terminal. Then, type the following into bash&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda env create -f env.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once installed, type the following into bash to bring up the environment,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source activate env
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you wish to install a specific program in this environment - say spyder - you can install it directly. Example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install spyder

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To open spyder program, simply type spyder into terminal. And there you go!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R</title>
      <link>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</link>
      <pubDate>Tue, 01 Jan 2019 00:15:53 +0800</pubDate>
      
      <guid>/post/translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r/</guid>
      <description>

&lt;h2 id=&#34;translating-ernest-chan-kalman-filter-strategy-matlab-and-python-code-into-r&#34;&gt;Translating Ernest Chan Kalman Filter Strategy Matlab and Python Code Into R&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m really intrigued by Ernest Chan&amp;rsquo;s approach in Quant Trading.&lt;/p&gt;

&lt;p&gt;Often in the retail trading space, what &amp;lsquo;gurus&amp;rsquo; preach often sounds really dubious. But Ernest Chan is different. He&amp;rsquo;s sincere, down-to-earth and earnest (meant to be a pun here).&lt;/p&gt;

&lt;p&gt;In my first month of deploying algo trading strategies, I focus mainly on mean-reversion strategies - paricularly amongst pairs. What I learnt - with real capital - is that the hedge ratio is dynamic and will vary over time. In the early days, I fixed it through linear regression. But boy this doesn&amp;rsquo;t work! It&amp;rsquo;s not really market neutral because of the imbalance in values between pairs across time.&lt;/p&gt;

&lt;p&gt;Then I chanced upon Kalman filter - something I learnt during my AI module in my Computer Science Degree days. I&amp;rsquo;ll spare the Math here. It&amp;rsquo;s a variant of the markov model, that uses a series of measurements over time (in this case, one of the pairs price), containing noise and produces estimates of unknown (here it&amp;rsquo;s the hedge ratio and intercept). Hedge ratio is updated in each time step.&lt;/p&gt;

&lt;p&gt;I saw the Python code online for EWA-EWC pair strategy that returns a sharpe ratio of 2.4. I tried to search for a R version but to no avail!&lt;/p&gt;

&lt;p&gt;Hence I decided to spend a day translating the python code into R code (for deployment purposes. Currently my algo trading stack is built around R). Thankfully I&amp;rsquo;m not translating the Matlab version because I do not have prior experience in that. And it would definitely take me more than a day for the translation.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve since,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Wrapped the code with a function&lt;/li&gt;
&lt;li&gt;Loop through a 40 choose 2 combinations of country pairs&lt;/li&gt;
&lt;li&gt;Triangulated with distance metrics like Correlation, Euclidean Distance and Manhattan Distance&lt;/li&gt;
&lt;li&gt;Filtered out long half life (i.e. # of days before reverting to the mean)&lt;/li&gt;
&lt;li&gt;Filtered by sharpe ratios, drawdown and average profits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so far, this market-neutral strategy is really promising!&lt;/p&gt;

&lt;h3 id=&#34;translated-r-code-for-ewa-ewc-strategy&#34;&gt;Translated R code for EWA - EWC strategy&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#Note: Try to put everything in a data-frame
lapply(c(&amp;quot;zoo&amp;quot;, &amp;quot;tidyr&amp;quot;, &amp;quot;plyr&amp;quot;, &amp;quot;dplyr&amp;quot;,
         &amp;quot;gtools&amp;quot;,&amp;quot;googlesheets&amp;quot;, &amp;quot;quantmod&amp;quot;, 
         &amp;quot;urca&amp;quot;, &amp;quot;PerformanceAnalytics&amp;quot;, &amp;quot;parallel&amp;quot;), require, character.only = T)

source(&#39;util/calculateReturns.R&#39;)
source(&#39;util/calculateMaxDD.R&#39;)
source(&#39;util/backshift.R&#39;)
source(&#39;util/extract_stock_prices.R&#39;)
source(&#39;util/cointegration_pair.R&#39;)


#Reading in the data
df = read.csv(&#39;kalman_filter/inputData_EWA_EWC.csv&#39;)
df = subset(df, select = c(&amp;quot;Date&amp;quot;, &amp;quot;EWC&amp;quot;, &amp;quot;EWA&amp;quot;))

# Augment x with ones to  accomodate possible offset in the regression between y vs x.
# df$EWA_ones = 1

# delta=1 gives fastest change in beta, delta=0.000....1 allows no change (like traditional linear regression).
delta = 0.0001 

#yhat=np.full(y.shape[0], np.nan) # measurement prediction
df$yhat = NA

#Initialize matrix
df$e = df$yhat # e = yhat.copy(), residuals
df$Q = df$yhat # Q = yhat.copy(), measurement variance

# For clarity, we denote R(t|t) by P(t). Initialize R, P and beta.
R = matrix(dat = rep(0, 4), nrow = 2, ncol = 2) #R = np.zeros((2,2))
P = R   #P = R.copy()

#Store beta in df and separately for computation
beta = matrix(dat = rep(NA, nrow(df) * 2), nrow = 2, ncol = nrow(df))
df$beta1 = NA; df$beta2 = NA  #beta = np.full((2, x.shape[0]), np.nan)

Vw = delta/(1-delta) * diag(2) #Vw=delta/(1-delta)*np.eye(2)
Ve = 0.001

# Initialize beta(:, 1) to zero
beta[, 1] = 0 #beta[:, 0]=0
df$beta1 = beta[1, 1]; df$beta2 = beta[2, 1]

#for t in range(len(y)):
for (t in 1:nrow(df)){
 
  if(t &amp;gt; 1){
    #Update matrix
    beta[, t] = beta[, t-1]
    R = P + Vw
    
    #Update df
    df$beta1[t] = beta[1, t]
    df$beta2[t] = beta[2, t]
  }
  
  # yhat[t, ] = as.matrix(x[t, ]) %*% as.matrix(beta[, t])    #yhat[t]=np.dot(x[t, :], beta[:, t])
  df$yhat[t] = as.matrix(data.frame(df$EWA[t], 1)) %*% as.matrix(beta[, t]) 
  
  # Q[t, ] = (as.matrix(x[t, ]) %*% R) %*% t(as.matrix(x[t, ])) + Ve  #Q[t] = np.dot(np.dot(x[t, :], R), x[t, :].T)+Ve
  df$Q[t] = (as.matrix(data.frame(df$EWA[t], 1)) %*% R) %*% t(as.matrix(data.frame(df$EWA[t], 1))) + Ve
  
  # e[t, ] = y[t, ] - yhat[t, ] #e[t]=y[t]-yhat[t] # measurement prediction error
  df$e[t] = df$EWC[t] - df$yhat[t]
  
  K = R %*% t(as.matrix(data.frame(df$EWA[t], 1))) / df$Q[t]  #K = np.dot(R, x[t, :].T)/Q[t] #  Kalman gain
  beta[, t] = beta[, t] + K %*% as.matrix(df$e[t]) #beta[:, t]=beta[:, t]+np.dot(K, e[t]) #  State update. Equation 3.11
  
  #Update df
  df$beta1[t] = beta[1, t]
  df$beta2[t] = beta[2, t]
  
  # State covariance update. Euqation 3.12
  P = R - ((K %*% as.matrix(data.frame(df$EWA[t], 1))) %*% R) #P = R-np.dot(np.dot(K, x[t, :]), R) 
}

#Generated signals
df$Q_root = df$Q ^ 0.5
df$longs &amp;lt;- df$e &amp;lt;= -df$Q ^ 0.5 # buy spread when its value drops below 2 standard deviations.
df$shorts &amp;lt;- df$e &amp;gt;= df$Q ^ 0.5  # short spread when its value rises above 2 standard deviations.  Short EWC

df$longExits  &amp;lt;- df$e &amp;gt; 0 
df$shortExits &amp;lt;- df$e &amp;lt; 0

# initialize to 0
df$numUnitsLong = NA
df$numUnitsShort = NA
df$numUnitsLong[0]=0.
df$numUnitsShort[0]=0.

df$numUnitsLong[df$longs]=1.
df$numUnitsLong[df$longsExit]=0
df$numUnitsLong = ifelse(is.na(df$numUnitsLong), 0, df$numUnitsLong)

df$numUnitsShort[df$shorts]=-1.
df$numUnitsShort[df$shortsExit]=0
df$numUnitsShort = ifelse(is.na(df$numUnitsShort), 0, df$numUnitsShort)

df$numUnits = df$numUnitsLong + df$numUnitsShort 

df$position1 = 0; df$position2 = 0 

df$position1 = ifelse(df$numUnits == -1, -1, df$position1)   #short EWC, Long EWA --&amp;gt; df$e[t] = df$EWC[t] - df$yhat[t]
df$position2 = ifelse(df$numUnits == -1, 1, df$position2)  #short EWC, Long EWA 

df$position1 = ifelse(df$numUnits == 1, 1, df$position1)   #long EWC, short EWA 
df$position2 = ifelse(df$numUnits == 1, -1, df$position2)  #long EWC, short EWA 

# df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(-df$beta1, 1)) * data.frame(df$EWA, df$EWC)   #Adjusted price
df$positions = data.frame(df$numUnits, df$numUnits) * (data.frame(1, -df$beta1)) * data.frame(df$EWC, df$EWA)   #Adjusted price

#Returns
df$dailyret1 &amp;lt;- c(NA, (df$EWC[2: nrow(df)] - df$EWC[1: (nrow(df) - 1)])/df$EWC[1: (nrow(df) - 1)])
df$dailyret2 &amp;lt;- c(NA, (df$EWA[2: nrow(df)] - df$EWA[1: (nrow(df) - 1)])/df$EWA[1: (nrow(df) - 1)])

#Daily returns
# lag(df$position1, 1)
# lag(df$position2, 1) * df$beta1
df$pnl = lag(df$positions$df.numUnits, 1) * df$dailyret1  + lag(df$positions$df.numUnits.1, 1) * df$dailyret2

df$ret = (df$pnl)/lag((df$positions$df.numUnits + df$positions$df.numUnits.1), 1)
df$ret = ifelse(is.na(df$ret), 0, df$ret)
df$ret[2] = 0

#Sharpe ratio
sqrt(252)*mean(df$pnl, na.rm = TRUE)/sd(df$pnl, na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I Find Country Pairs for Mean Reversion Strategy</title>
      <link>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</link>
      <pubDate>Wed, 26 Dec 2018 12:30:03 +0800</pubDate>
      
      <guid>/post/how-i-find-country-pairs-for-mean-reversion-strategy/</guid>
      <description>

&lt;h2 id=&#34;how-i-find-country-pairs-for-mean-reversion-strategy&#34;&gt;How I Find Country Pairs for Mean Reversion Strategy&lt;/h2&gt;

&lt;p&gt;As mentioned in my previous post &lt;a href=&#34;https://jirong-huang.netlify.com/post/research-to-production-pipeline-for-mean-reversion/&#34;&gt;here&lt;/a&gt;, the first step for a mean reversion strategy is to conduct some background quantitative research.&lt;/p&gt;

&lt;h3 id=&#34;step-1&#34;&gt;Step 1&lt;/h3&gt;

&lt;p&gt;First, I use a pair trading function to loop across 800+ country pairs (created from combination function),&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pair_trading = function(stock1, stock2, trade_amount, finance_rates, start_date, end_date, 
                        prop_train, enter_z_score, exit_z_score){

## More codes here
   
## Return this
key_info = list(
  ticker = c(stock1, stock2),
  start_date = start_date,
  trade_table = data_trade,
  sharpe = c(sharpeRatioTrainset, sharpeRatioTestset),
  half_life = half_life,
  profits = data_trade_stats,
  max_drawdown = c(table.DownsideRisk(data$pnl[trainset])[1]$pnl[7], table.DownsideRisk(data$pnl[testset])[1]$pnl[7]),
  returns = cbind(table.AnnualizedReturns(data$pnl[trainset]), table.AnnualizedReturns(data$pnl[testset])),
  hedgeRatio_mean_sd = c(as.numeric(hedgeRatio), as.numeric(data_trade$spreadMean[nrow(data_trade)]), as.numeric(data_trade$spreadStd[nrow(data_trade)])),   #critical --&amp;gt;to be used in real-time trading
  close_z_score = as.numeric(data_trade$zscore[nrow(data_trade)]),
  hist_spread = data_trade$spread[(nrow(data_trade) - round(half_life) + 2):nrow(data_trade)],
  prop_days_mkt = c(prop_days_mkt_train, prop_days_mkt_test),
  close_price = c(data_trade$Close[nrow(data_trade)], data_trade$Close.1[nrow(data_trade)]),
  win_rate = c(perc_win_train, perc_win_test)
  # ,
  # chart_train = charts.PerformanceSummary(data$pnl[trainset]),
  # chart_test = charts.PerformanceSummary(data$pnl[testset])
)

return(key_info)
                        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2&#34;&gt;Step 2&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Next, I select pairs with sharpe ratio &amp;gt;1 in both training and testing periods.&lt;/li&gt;
&lt;li&gt;And also select pairs with shorter half-life i.e. shorter duration before it reverts to its mean path - more than 5 and lesser than 25&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;step-3&#34;&gt;Step 3&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then I went on to IShares website to get the respective tickers&amp;rsquo; industries&amp;rsquo; composition.
&lt;img src=&#34;/post/img/country_composition.png&#34; alt=&#34;/post/img/country_composition.png&#34;&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With this new piece of information, I went on to compute the manhattan distance, euclidean distance and correlation between these country pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next I applied percentile ranks to these distance measures and find an average percentile rank&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anything that is above 50th percentile is selected.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;distance_metrics = function(stock1, stock2){
  
  dist = c(NA, NA, NA)
  
  tryCatch({
  
  ctry_pair_composition_sub = subset(ctry_pair_composition, ctry_pair_composition$ticker == stock1 | ctry_pair_composition$ticker == stock2)
  manhattan = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;manhattan&amp;quot;))
  euclidean = as.numeric(distance(ctry_pair_composition_sub[, -1], method = &amp;quot;euclidean&amp;quot;))
  correlation = cor(as.numeric(ctry_pair_composition_sub[1, -1]), as.numeric(ctry_pair_composition_sub[2, -1]))
  
  dist = c(manhattan, euclidean, correlation)
  }, error=function(e){})
  
  return(dist)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And ta-dah! This is the final selected country pairs that I will be using for my mean reversion strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/final_selection.png&#34; alt=&#34;/post/img/final_selection.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-improvement&#34;&gt;Further improvement&lt;/h3&gt;

&lt;p&gt;Note: I could have applied co-integration test. Will do it pretty soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research to Production Pipeline for Mean Reversion</title>
      <link>/post/research-to-production-pipeline-for-mean-reversion/</link>
      <pubDate>Tue, 25 Dec 2018 18:07:19 +0800</pubDate>
      
      <guid>/post/research-to-production-pipeline-for-mean-reversion/</guid>
      <description>

&lt;h2 id=&#34;research-to-production-pipeline-for-mean-reversion&#34;&gt;Research to Production Pipeline for Mean Reversion&lt;/h2&gt;

&lt;p&gt;Here is a high level overview of something that I&amp;rsquo;m working on.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been grappling with the finite state automata Event Driven Computing transitions and I kinda sorted it out for production use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/img/research_to_production.png&#34; alt=&#34;/post/img/research_to_production.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
