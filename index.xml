<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jirong&#39;s sandbox on Jirong&#39;s sandbox</title>
    <link>/</link>
    <description>Recent content in Jirong&#39;s sandbox on Jirong&#39;s sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Latest lessons learnt from crawling</title>
      <link>/post/crawling-insights/</link>
      <pubDate>Sun, 08 Dec 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/crawling-insights/</guid>
      <description>Lessons learnt I just realised that there&amp;rsquo;s a quick way to understand the xpaths&amp;rsquo; patterns.
In the past, usually what I did is to manually eyeball to infer the patterns from the page source or inspect page.
Silly me!
1 quick way to understand the pattern is through the following,
 Right click on an element in a web page that you are interested in and click on &amp;lsquo;inspect&amp;rsquo; Right click on the node and click &amp;lsquo;copy&amp;rsquo; Copy full xpath  And paste to a notepad.</description>
    </item>
    
    <item>
      <title>Free lunch does exist in Singapore - SRS analysis</title>
      <link>/post/srs_analysis/</link>
      <pubDate>Wed, 27 Nov 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/srs_analysis/</guid>
      <description>SRS analysis I haven&amp;rsquo;t paid much attention to SRS contributions as a way to reduce taxable income. But lately, I realised you could boost investment portfolio returns through this avenue at literally 0 cost.
I find this blog post written by a local finance blogger to be really helpful in understanding the SRS contributions and withdrawal mechanics.
For my own understanding, I also did some quick analysis using google sheet (see &amp;lsquo;Analysis in google sheet&amp;rsquo; section) to evaluate if this works, and to my surprise, I found that free lunch does exist in Singapore!</description>
    </item>
    
    <item>
      <title>Back to Basics - Birth of an idea in the jungle</title>
      <link>/post/birth-of-an-idea-in-the-jungle/</link>
      <pubDate>Sun, 06 Oct 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/birth-of-an-idea-in-the-jungle/</guid>
      <description>Birth of an idea of all places: In the Jungle Having to juggle both work and masters in computer science at the same time, it&amp;rsquo;s really hard to afford any more time to my side projects.
But back in August, I had a break away from both school and work by going back for reservist. As it&amp;rsquo;s really bored in there where I spent most of the time in a small building (with no aircon!</description>
    </item>
    
    <item>
      <title>Setting up a database for my Jarvis</title>
      <link>/post/jarvis_database/</link>
      <pubDate>Sun, 29 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/jarvis_database/</guid>
      <description>Setting up a database for my Jarvis As I run more sophiscated trading strategies, I require a proper database for training parameters and records.
Previously, I was using a mix of SQLite, RDA and CSV files - but going forward I will be using Mysql (workbench) to house my data.
Below is an example of database tables for my market neutral strategies. I will be using these tables for the following,</description>
    </item>
    
    <item>
      <title>Market Neutral Strategy - DAX Index and EWG ETF</title>
      <link>/post/market_neutral_ewg/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/market_neutral_ewg/</guid>
      <description>DAX index and Germany ETF I will keep it short in this post since I espoused on this strategy a couple of times.
I discovered another market neutral opportunity this month.
And this is based on ratio between Germany DAX index and MSCI based Germany ETF (EWG)
Based on backtest, sharpe ratio is close to 1.16.
The composition between these 2 indexes are largely similar and any significant deviation shouldn’t persist for long.</description>
    </item>
    
    <item>
      <title>My Investment Jarvis (In Ray Dalio&#39;s words, My Investment Principles)</title>
      <link>/post/jarvis/</link>
      <pubDate>Fri, 20 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/jarvis/</guid>
      <description>Jarvis Humans are imperfect.
Humans are prone to biases.
Humans are dumb.
Humans have egos.
Humans rely on intuitions which are way way overrated.
And all these are blockers to sustainable positive performances in the area of investment portfolio management.
But not all is lost&amp;hellip; I&amp;rsquo;ve found a way to aid me in my invesment decision making processes.
And that&amp;rsquo;s Jarvis! My expert advisor to advise me what to do in different scenarios.</description>
    </item>
    
    <item>
      <title>Market Neutral Strategy - FTSE Index and EWU ETF</title>
      <link>/post/market_neutral_ewu/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/market_neutral_ewu/</guid>
      <description>UK index and UK ETF I discovered another market neutral opportunity this month.
And this is based on ratio between FTSE 100 index and MSCI based UK ETF (EWU)
Based on backtest, sharpe ratio is close to 0.9.
The composition between these 2 indexes are largely similar and any significant deviation shouldn’t persist for long.
The optimal lookback period for the MA component in bollinger band is approximately 40 days.</description>
    </item>
    
    <item>
      <title>Asset allocation notification</title>
      <link>/post/asset_allocation_notification/</link>
      <pubDate>Thu, 12 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/asset_allocation_notification/</guid>
      <description>Asset allocation notification I&amp;rsquo;m in the midst of automating/ guiding my life with algorithms (largely inspired by Ray Dalio) - and 1 of the guidelines that I set is on asset allocation,
 Emerging market and Developed Market should be of the same proportion Bonds + Cash proportion should be equivalent to my age. This can deviate in times of crisis when I want to be more opportunistic.  If it deviates from the portfolio policy statement, it will send me a pushover notification to my phone:)</description>
    </item>
    
    <item>
      <title>ETF watchlist email notification Through Python</title>
      <link>/post/email_notification_python/</link>
      <pubDate>Tue, 10 Sep 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/email_notification_python/</guid>
      <description>Email notification I finally bit the bullet and updated my previously hideous email notification!
You may find the updated email notification template here - alongside with the code.
Feel free to ping me if you are keen to be on the email list too.
~ Jirong
import smtplib, ssl import datetime import pandas as pd from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart #Format text data = pd.read_csv(&#39;/home/jirong/Desktop/github/ETF_watchlist/Output/yahoo_crawled_data.csv&#39;) data[&#39;Change_fr_52_week_high&#39;] = round(100 * data[&#39;Change_fr_52_week_high&#39;], 1) data = data[[&#39;Name&#39;, &#39;Price&#39;, &#39;Change_fr_52_week_high&#39;]].</description>
    </item>
    
    <item>
      <title>Market Neutral Strategy - SnP500 (SPY) to Berkshire Hathaway Ratio</title>
      <link>/post/market_neutral/</link>
      <pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/market_neutral/</guid>
      <description>Market neutral strategy As the negative news pile up (trade wars, slump in economy growths, etc), I sought for market neutral stategies that could perform well in any market environment.
An idea that struck me recently is to exploit the pair between Berkshire and SnP 500 ETF.
The SnP500 ETF/ Berkshire ratio has been falling over the years - insinuating that Berkshire still outperforms the index in the last couple of years.</description>
    </item>
    
    <item>
      <title>Convert NAs to Obscure Number in Data Frame to Aid in Recoding/ Feature Engineering</title>
      <link>/post/convert_na_num/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/convert_na_num/</guid>
      <description>Converting NAs to obscure numbers to prevent the data from messing up the recoding. 1 issue that I encounter while I data-munge is that NAs in data seem to mess up my recoding. Here’s a neat swiss army knife utility function I developed recently.
suppressMessages(library(dplyr)) # Converting NA to obscure number to prevent awkward recoding situations that require &amp;amp; !is.na(&amp;lt;variable&amp;gt;) # Doesn&amp;#39;t work for factors #&amp;#39; @title Convert NA to obscure number #&amp;#39; @param dp_dataframe Dataframe in consideration #&amp;#39; @param np_obscure_num Numeric - Obscure number #&amp;#39; @param bp_na_to_num Boolean if TRUE, convert NA to num.</description>
    </item>
    
    <item>
      <title>Loading excel data with correct variable types</title>
      <link>/post/load_data_with_correct_types/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/load_data_with_correct_types/</guid>
      <description>Loading data with data types When reading static files into R or Python, most of the times we are lazy as we load the data with no regard to the data types.
But in mission critical ETL jobs or Data analytics workflow, data types are quintessential and there’s a fine line between life and death. Ok, I’m exaggerating here.
What I’ve written below is a swiss army knife function to read an excel file: 1st tab is data and 2nd tab is the variable types (e.</description>
    </item>
    
    <item>
      <title>Function to describe clusters derived from unsupervised learning</title>
      <link>/post/cluster_descriptive_stats/</link>
      <pubDate>Fri, 24 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/cluster_descriptive_stats/</guid>
      <description>Describing unsupervised learning clusters As a data scientist / analyst, besides doing cool modelling stuff, we&amp;rsquo;re often asked to churn out descriptive statistics. Yes, we know. It&amp;rsquo;s part of the process.
I chanced upon this really nifty concept at work to describe the clusters derived from unsupervised learnig. Here&amp;rsquo;s how it goes,
 Say it&amp;rsquo;s a nominal or ordinal variable. First, I find the proportion of the feature across the X clusters Second, I rank this proportion through percentiles across these X values The cluster with the highest percentile will earn its right to be represented by the feature And if it&amp;rsquo;s a scale variable, you may find the mean of the feature for each cluster and repeat the steps.</description>
    </item>
    
    <item>
      <title>Playing with Google Place API</title>
      <link>/post/google_place_api/</link>
      <pubDate>Tue, 14 May 2019 11:46:49 +0800</pubDate>
      
      <guid>/post/google_place_api/</guid>
      <description>Google Place API I was playing around with the API to obtain lat-long for my geo analytics work.
I entered my credit card info but it seems that I&amp;rsquo;m not charged even with 9000+ API calls. Unsure if it&amp;rsquo;s because I&amp;rsquo;ve a 400+ dollars free cloud credit?
Anyway, what I did here was to make API calls and storing the data into my local database.
If you&amp;rsquo;re interested, you may visit this stackoverflow link (https://stackoverflow.</description>
    </item>
    
    <item>
      <title>Using exponential distribution to estimate frequency of occurence</title>
      <link>/post/exp_distrib/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/exp_distrib/</guid>
      <description>Simulating product failures I’m inspired by this post here (http://www.programmingr.com/examples/neat-tricks/sample-r-function/rexp/). And decided to expand on the example.
Say you are an owner of a computer store and you would like to estimate the frequency of warranty repairs - and the ensuing costs.
Here’s the scenario with the accompanying assumptions
 Each computer is expected to last an average of 7 years You only sell 1000 computers at the start of each year You sell computer from 2019 to 2025  First, I simulate an exponential distribution of 1000 points for 7 years; and place a time index of 2019 to 2025</description>
    </item>
    
  </channel>
</rss>
